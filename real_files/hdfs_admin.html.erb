---
title: HDFS Administration
---
<!DOCTYPE html><html xmlns:xy="http://xyleme.com/xylink">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>HDFS Administration</title>
<link href="http://docs.pivotal.io/stylesheets/master.css" rel="stylesheet" type="text/css" media="screen,print">
<link href="../../xyleme.css" rel="stylesheet" type="text/css" media="screen, print">
</head>
<body><div class="viewport"><div class="wrap">
<div class="container">
<header></header><main class="content content-layout" id="js-content" role="main"><a id="top"></a><h1 class="bold horton-blue">HDFS Administration</h1>
<h2 class="small-title thin">HDP 2.2.3</h2>
<div class="lessons">
    
      <h2 class="horton-green bold">HDFS Administration</h2>
      
        <h3 class="horton-blue bold" id="ref-ced5a316-ec71-4032-925c-e63aeb385025">ACLs on HDFS</h3>
        
          <p>This guide describes how to use Access Control Lists (ACLs) on the Hadoop Distributed File System
            (HDFS). ACLs extend the HDFS permission model to support more granular file access based on arbitrary
            combinations of users and groups.



            TESTTTTTTTTTT
          </p>
        
        
          <h4 class="bold">Configuring ACLs on HDFS</h4>
          
            <ul class="bullet-list">
              Only one property needs to be specified in the hdfs-site.xml file in order to enable ACLs on
                HDFS:
              
              
                <li>
                  <p>
                    <strong>dfs.namenode.acls.enabled</strong>

                    Set this property to "true" to enable support for ACLs. ACLs are disabled by default. When ACLs are
                    disabled, the NameNode rejects all attempts to set an ACL.

                    Example:
                  </p>
                </li>
              
            </ul>
            <pre><code>&lt;property&gt;
              &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;
              &lt;value&gt;true&lt;/value&gt;
              &lt;/property&gt;</code></pre>
          
        
        
          <h4 class="bold">Using CLI Commands to Create and List ACLs</h4>
          
            <p>Two new sub-commands are added to FsShell: setfacl and getfacl. These commands are modeled after
              the same Linux shell commands, but fewer flags are implemented. Support for additional flags may be added
              later if required.
            </p>
            <ul class="bullet-list">
              
                <li>
                  <p>
                    <strong>setfacl</strong>

                    Sets ACLs for files and directories.

                    Usage:

                    <code>-setfacl [-bkR] {-m|-x} &lt;acl_spec&gt; &lt;path&gt;</code>

                    <code>-setfacl --set &lt;acl_spec&gt; &lt;path&gt;

                    </code>Options:
                  </p>
                  <div class="xyleme-table"><table border="1">
                    
                      
                      
                      <thead></thead>
                      <tbody>
                        <tr>
                          <th rowspan="1">
                            <p>
                              <strong>Option</strong>
                            </p>
                          </th>
                          <th rowspan="1">
                            <p>
                              <strong>Description</strong>
                            </p>
                          </th>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>-b</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>Remove all entries, but retain the base ACL entries. The entries for User, Group,
                              and Others are retained for compatibility with Permission Bits.
                            </p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>-k</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>Remove the default ACL.</p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>-R</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>Apply operations to all files and directories recursively.</p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>-m</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>Modify the ACL. New entries are added to the ACL, and existing entries are
                              retained.
                            </p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>-x</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>Remove the specified ACL entries. All other ACL entries are retained.</p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>--set</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>Fully replace the ACL and discard all existing entries. The
                              <code>acl_spec</code>
                              must include entries for User, Group, and Others for compatibility with Permission Bits.
                            </p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>&lt;acl_spec&gt;</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>A comma-separated list of ACL entries.</p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>&lt;path&gt;</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>The path to the file or directory to modify.</p>
                          </td>
                        </tr>
                      </tbody>
                      
                    
                  </table></div>
                  <p>Examples:</p>
                  <pre><code>hdfs dfs -setfacl -m user:hadoop:rw- /file

                    hdfs dfs -setfacl -x user:hadoop /file

                    hdfs dfs -setfacl -b /file

                    hdfs dfs -setfacl -k /dir

                    hdfs dfs -setfacl --set user::rw-,user:hadoop:rw-,group::r--,other::r-- /file

                    hdfs dfs -setfacl -R -m user:hadoop:r-x /dir

                    hdfs dfs -setfacl -m default:user:hadoop:r-x /dir
                  </code></pre>
                  <p>
                    Exit Code:

                    Returns 0 on success and non-zero on error.
                  </p>
                </li>
                <li>
                  <p>
                    <strong>getfacl</strong>
                    <code>

                    </code>Displays the ACLs of files and directories. If a directory has a default ACL, getfacl
                    also displays the default ACL.

                    Usage:

                    <code>-getfacl [-R] &lt;path&gt;

                    </code>Options:
                  </p>
                  <div class="xyleme-table"><table border="1">
                    
                      
                      
                      <thead></thead>
                      <tbody>
                        <tr>
                          <th rowspan="1">
                            <p>
                              <strong>Option</strong>
                            </p>
                          </th>
                          <th rowspan="1">
                            <p>
                              <strong>Description</strong>
                            </p>
                          </th>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>-R</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>List the ACLs of all files and directories recursively.</p>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="1">
                            <p>
                              <code>&lt;path&gt;</code>
                            </p>
                          </td>
                          <td rowspan="1">
                            <p>The path to the file or directory to list.</p>
                          </td>
                        </tr>
                      </tbody>
                      
                    
                  </table></div>
                  <p>
                    Examples:
                  </p>
                  <pre><code>hdfs dfs -getfacl /file

                    hdfs dfs -getfacl -R /dir
                  </code></pre>
                  <p>
                    Exit Code:

                    Returns 0 on success and non-zero on error.
                  </p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">ACLs Examples</h4>
          
            <h4 class="bold">Introduction: ACLs Versus Permission Bits</h4>
            
              <p>Before the implementation of Access Control Lists (ACLs), the HDFS permission model was
                equivalent to traditional UNIX Permission Bits. In this model, permissions for each file or directory
                are managed by a set of three distinct user classes: Owner, Group, and Others. There are three
                permissions for each user class: Read, Write, and Execute. Thus, for any file system object, its
                permissions can be encoded in 3*3=9 bits. When a user attempts to access a file system object, HDFS
                enforces permissions according to the most specific user class applicable to that user. If the user is
                the owner, HDFS checks the Owner class permissions. If the user is not the owner, but is a member of the
                file system object’s group, HDFS checks the Group class permissions. Otherwise, HDFS checks the Others
                class permissions.
              </p>
              <p>
                This model can sufficiently address a large number of security requirements. For example, consider a
                sales department that would like a single user -- Bruce, the department manager -- to control all
                modifications to sales data. Other members of the sales department need to view the data, but must not
                be allowed to modify it. Everyone else in the company (outside of the sales department) must not be
                allowed to view the data. This requirement can be implemented by running chmod 640 on the file, with the
                following outcome:
              </p>
              <pre><code>-rw-r-----1 brucesales22K Nov 18 10:55 sales-data</code></pre>
              <p>Only Bruce can modify the file, only members of the sales group can read the file, and no one
                else can access the file in any way.
              </p>
              <p>
                Suppose that new requirements arise. The sales department has grown, and it is no longer feasible for
                Bruce to control all modifications to the file. The new requirement is that Bruce, Diana, and Clark are
                allowed to make modifications. Unfortunately, there is no way for Permission Bits to address this
                requirement, because there can be only one owner and one group, and the group is already used to
                implement the read-only requirement for the sales team. A typical workaround is to set the file owner to
                a synthetic user account, such as "salesmgr," and allow Bruce, Diana, and Clark to use the "salesmgr"
                account via sudo or similar impersonation mechanisms. The drawback with this workaround is that it
                forces complexity onto end-users, requiring them to use different accounts for different actions.
              </p>
              <p>
                Now suppose that in addition to the sales staff, all executives in the company need to be able to read
                the sales data. This is another requirement that cannot be expressed with Permission Bits, because there
                is only one group, and it is already used by sales. A typical workaround is to set the file’s group to a
                new synthetic group, such as "salesandexecs," and add all users of "sales" and all users of "execs" to
                that group. The drawback with this workaround is that it requires administrators to create and manage
                additional users and groups.
              </p>
              <p>
                Based on the preceding examples, you can see that it can be awkward to use Permission Bits to address
                permission requirements that differ from the natural organizational hierarchy of users and groups. The
                advantage of using ACLs is that it enables you to address these requirements more naturally, in that for
                any file system object, multiple users and multiple groups can have different sets of permissions.
              </p>
            
          
          
            <h4 class="bold">Example 1: Granting Access to Another Named Group</h4>
            
              <p>To address one of the issues raised in the preceding section, we will set an ACL that grants
                Read access to sales data to members of the "execs" group.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Set the ACL:</p>
                    <pre><code>&gt; hdfs dfs -setfacl -m group:execs:r-- /sales-data</code></pre>
                  </li>
                
              </ul>
              <ul class="bullet-list">
                
                  <li>
                    <p>Run getfacl to check the results:</p>
                    <pre><code>&gt; hdfs dfs -getfacl /sales-data
                      # file: /sales-data
                      # owner: bruce
                      # group: sales
                      user::rw-
                      group::r--
                      group:execs:r--
                      mask::r--
                      other::---
                    </code></pre>
                  </li>
                
              </ul>
              <ul class="bullet-list">
                
                  <li>
                    <p>If we run the ls command, we see that the listed permissions have been appended with a "+"
                      symbol to indicate the presence of an ACL. The "+" symbol is appended to the permissions of any
                      file or directory that has an ACL.
                    </p>
                    <pre><code>&gt; hdfs dfs -ls /sales-data
                      Found 1 items
                      -rw-r-----+ 3 bruce sales 0 2014-03-04 16:31 /sales-data
                    </code></pre>
                  </li>
                
              </ul>
              <p>The new ACL entry is added to the existing permissions defined by the Permission Bits. As the
                file owner, Bruce has full control. Members of either the "sales" group or the "execs" group have Read
                access. All others do not have access.
              </p>
            
          
          
            <h4 class="bold">Example 2: Using a Default ACL for Automatic Application to New Children</h4>
            
              <p>In addition to an ACL enforced during permission checks, there is also the separate concept of a
                default ACL. A default ACL can only be applied to a directory -- not to a file. Default ACLs have no
                direct effect on permission checks for existing child files and directories, but instead define the ACL
                that new child files and directories will receive when they are created.

                Suppose we have a "monthly-sales-data" directory that is further subdivided into separate directories
                for each month. We will set a default ACL to guarantee that members of the "execs" group automatically
                get access to new subdirectories as they get created each month.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Set a default ACL on the parent directory:</p>
                    <pre><code>&gt; hdfs dfs -setfacl -m default:group:execs:r-x /monthly-sales-data</code></pre>
                  </li>
                
              </ul>
              <ul class="bullet-list">
                
                  <li>
                    <p>Make subdirectories:</p>
                    <pre><code>&gt; hdfs dfs -mkdir /monthly-sales-data/JAN
                      &gt; hdfs dfs -mkdir /monthly-sales-data/FEB
                    </code></pre>
                  </li>
                
              </ul>
              <ul class="bullet-list">
                
                  <li>
                    <p>Verify that HDFS has automatically applied the default ACL to the subdirectories :
                    </p>
                    <pre><code>&gt; hdfs dfs -getfacl -R /monthly-sales-data
                      # file: /monthly-sales-data
                      # owner: bruce
                      # group: sales
                      user::rwx
                      group::r-x
                      other::---
                      default:user::rwx
                      default:group::r-x
                      default:group:execs:r-x
                      default:mask::r-x
                      default:other::---

                      # file: /monthly-sales-data/FEB
                      # owner: bruce
                      # group: sales
                      user::rwx
                      group::r-x
                      group:execs:r-x
                      mask::r-x
                      other::---
                      default:user::rwx
                      default:group::r-x
                      default:group:execs:r-x
                      default:mask::r-x
                      default:other::---

                      # file: /monthly-sales-data/JAN
                      # owner: bruce
                      # group: sales
                      user::rwx
                      group::r-x
                      group:execs:r-x
                      mask::r-x
                      other::---
                      default:user::rwx
                      default:group::r-x
                      default:group:execs:r-x
                      default:mask::r-x
                      default:other::---
                    </code></pre>
                  </li>
                
              </ul>
              <p>The default ACL is copied from the parent directory to a child file or directory when it is
                created. Subsequent changes to the default ACL of the parent directory do not alter the ACLs of existing
                children.
              </p>
            
          
          
            <h4 class="bold">Example 3: Blocking Access to a Sub-Tree for a Specific User</h4>
            
              <p>Suppose there is a need to immediately block access to an entire sub-tree for a specific user.
                Applying a named user ACL entry to the root of that sub-tree is the fastest way to accomplish this
                without accidentally revoking permissions for other users.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Add an ACL entry to block all access to "monthly-sales-data" by user Diana:</p>
                    <pre><code>&gt; hdfs dfs -setfacl -m user:diana:--- /monthly-sales-data</code></pre>
                  </li>
                
              </ul>
              <ul class="bullet-list">
                
                  <li>
                    <p>Run
                      <code>getfacl</code>
                      to check the results:
                    </p>
                    <pre><code>&gt; hdfs dfs -getfacl /monthly-sales-data
                      # file: /monthly-sales-data
                      # owner: bruce
                      # group: sales
                      user::rwx
                      user:diana:---
                      group::r-x
                      mask::r-x
                      other::---
                      default:user::rwx
                      default:group::r-x
                      default:group:execs:r-x
                      default:mask::r-x
                      default:other::---
                    </code></pre>
                  </li>
                
              </ul>
              <p>The new ACL entry is added to the existing permissions defined by the Permission Bits. Bruce has
                full control as the file owner. Members of either the "sales" group or the "execs" group have Read
                access. All others do not have access.
              </p>
              <ul class="number-list">
                It is important to keep in mind the order of evaluation for ACL entries when a user
                  attempts to access a file system object:
                
                
                  <li>
                    <p>If the user is the file owner, the Owner Permission Bits are enforced.</p>
                  </li>
                  <li>
                    <p>Else, if the user has a named user ACL entry, those permissions are enforced.</p>
                  </li>
                  <li>
                    <p>Else, if the user is a member of the file’s group or any named group in an ACL entry, then
                      the union of permissions for all matching entries are enforced. (The user may be a member of
                      multiple groups.)
                    </p>
                  </li>
                  <li>
                    <p>If none of the above are applicable, the Other Permission Bits are enforced.</p>
                  </li>
                
              </ul>
              <p>In this example, the named user ACL entry accomplished our goal, because the user is not the
                file owner, and the named user entry takes precedence over all other entries.
              </p>
            
          
        
        
          <h4 class="bold">ACLS on HDFS Features</h4>
          
            <h4 class="bold">POSIX ACL Implementation</h4>
            
              <p>ACLs on HDFS have been implemented with the POSIX ACL model. If you have ever used POSIX ACLs on
                a Linux file system, the HDFS ACLs work the same way.
              </p>
            
          
          
            <h4 class="bold">Compatibility and Enforcement</h4>
            
              <p>HDFS can associate an optional ACL with any file or directory. All HDFS operations that enforce
                permissions expressed with Permission Bits must also enforce any ACL that is defined for the file or
                directory. Any existing logic that bypasses Permission Bits enforcement also bypasses ACLs. This
                includes the HDFS super-user and setting dfs.permissions to "false" in the configuration.
              </p>
            
          
          
            <h4 class="bold">Access Through Multiple User-Facing Endpoints</h4>
            
              <p>HDFS supports operations for setting and getting the ACL associated with a file or directory.
                These operations are accessible through multiple user-facing endpoints. These endpoints include the
                FsShell CLI, programmatic manipulation through the FileSystem and FileContext classes, WebHDFS, and NFS.
              </p>
            
          
          
            <h4 class="bold">User Feedback: CLI Indicator for ACLs</h4>
            
              <p>The ‘+’ symbol is appended to the listed permissions (the output of the ls -l command) of any
                file or directory with an associated ACL.
              </p>
            
          
          
            <h4 class="bold">Backward-Compatibility</h4>
            
              <p>The implementation of ACLs is backward-compatible with existing usage of Permission Bits.
                Changes applied via Permission Bits (i.e., chmod) are also visible as changes in the ACL. Likewise,
                changes applied to ACL entries for the base user classes (Owner, Group, and Others) are also visible as
                changes in the Permission Bits. In other words, Permission Bit and ACL operations manipulate a shared
                model, and the Permission Bit operations can be considered a subset of the ACL operations.
              </p>
            
          
          
            <h4 class="bold">Low Overhead</h4>
            
              <p>The addition of ACLs will not cause a detrimental impact to the consumption of system resources
                in deployments that choose not to use ACLs. This includes CPU, memory, disk, and network bandwidth.
              </p>
              <p>
                Using ACLs does impact NameNode performance. It is therefore recommended that you use Permission Bits,
                if adequate, before using ACLs.
              </p>
            
          
          
            <h4 class="bold">ACL Entry Limits</h4>
            
              <p>The number of entries in a single ACL is capped at a maximum of 32. Attempts to add ACL entries
                over the maximum will fail with a user-facing error. This is done for two reasons: to simplify
                management, and to limit resource consumption. ACLs with a very high number of entries tend to become
                difficult to understand, and may indicate that the requirements are better addressed by defining
                additional groups or users. ACLs with a very high number of entries also require more memory and
                storage, and take longer to evaluate on each permission check. The number 32 is consistent with the
                maximum number of ACL entries enforced by the "ext" family of file systems.
              </p>
            
          
          
            <h4 class="bold">Symlinks</h4>
            
              <p>Symlinks do not have ACLs of their own. The ACL of a symlink is always seen as the default
                permissions (777 in Permission Bits). Operations that modify the ACL of a symlink instead modify the ACL
                of the symlink’s target.
              </p>
            
          
          
            <h4 class="bold">Snapshots</h4>
            
              <p>Within a snapshot, all ACLs are frozen at the moment that the snapshot was created. ACL changes
                in the parent of the snapshot are not applied to the snapshot.
              </p>
            
          
          
            <h4 class="bold">Tooling</h4>
            
              <p>Tooling that propagates Permission Bits will not propagate ACLs. This includes the
                <code>cp -p</code>
                shell command and<code>distcp -p</code>.
              </p>
            
          
        
        
          <h4 class="bold">Use Cases for ACLs on HDFS</h4>
          
            <p>ACLs on HDFS supports the following use cases:</p>
          
          
            <h4 class="bold">Multiple Users</h4>
            
              <p>In this use case, multiple users require Read access to a file. None of the users are the owner
                of the file. The users are not members of a common group, so it is impossible to use group Permission
                Bits.
              </p>
              <p>
                This use case can be addressed by setting an access ACL containing multiple named user entries:
              </p>
              <pre><code>ACLs on HDFS supports the following use cases:</code></pre>
            
          
          
            <h4 class="bold">Multiple Groups</h4>
            
              <p>In this use case, multiple groups require Read and Write access to a file. There is no group
                containing the union of all of the groups' members, so it is impossible to use group Permission Bits.
              </p>
              <p>
                This use case can be addressed by setting an access ACL containing multiple named group entries:
              </p>
              <pre><code>group:sales:rw-
                group:execs:rw-
              </code></pre>
            
          
          
            <h4 class="bold">Hive Partitioned Tables</h4>
            
              <p>In this use case, Hive contains a partitioned table of sales data. The partition key is
                "country". Hive persists partitioned tables using a separate subdirectory for each distinct value of the
                partition key, so the file system structure in HDFS looks like this:
              </p>
              <pre><code>user
                `-- hive
                `-- warehouse
                `-- sales
                |-- country=CN
                |-- country=GB
                `-- country=US
              </code></pre>
              <p>A "salesadmin" group is the owning group for all of these files. Members of this group have Read
                and Write access to all files. Separate country-specific groups can run Hive queries that only read data
                for a specific country, e.g., "sales_CN", "sales_GB", and "sales_US". These groups do not have Write
                access.
              </p>
              <p>
                This use case can be addressed by setting an access ACL on each subdirectory containing an owning group
                entry and a named group entry:
              </p>
              <pre><code>country=CN
                group::rwx
                group:sales_CN:r-x

                country=GB
                group::rwx
                group:sales_GB:r-x

                country=US
                group::rwx
                group:sales_US:r-x
              </code></pre>
              <p>Note that the functionality of the owning group ACL entry (the group entry with no name) is
                equivalent to setting Permission Bits.
              </p>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Important.png" width="50"></div>
                <div class="simple-block">
                  <p>Storage-based authorization in Hive does not currently consider the ACL permissions in HDFS.
                    Rather, it verifies access using the traditional POSIX permissions model
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Default ACLs</h4>
            
              <p>In this use case, a file system administrator or sub-tree owner would like to define an access
                policy that will be applied to the entire sub-tree. This access policy must apply not only to the
                current set of files and directories, but also to any new files and directories that are added later.
              </p>
              <p>
                This use case can be addressed by setting a default ACL on the directory. The default ACL can contain
                any arbitrary combination of entries. For example:
              </p>
              <pre><code>default:user::rwx
                default:user:bruce:rw-
                default:user:diana:r--
                default:user:clark:rw-
                default:group::r--
                default:group:sales::rw-
                default:group:execs::rw-
                default:others::---
              </code></pre>
              <p>It is important to note that the default ACL gets copied from the directory to newly created
                child files and directories at time of creation of the child file or directory. If you change the
                default ACL on a directory, that will have no effect on the ACL of the files and subdirectories that
                already exist within the directory. Default ACLs are never considered during permission enforcement.
                They are only used to define the ACL that new files and subdirectories will receive automatically when
                they are created.
              </p>
            
          
          
            <h4 class="bold">Minimal ACL/Permissions Only</h4>
            
              <p>HDFS ACLs support deployments that may want to use only Permission Bits and not ACLs with named
                user and group entries. Permission Bits are equivalent to a minimal ACL containing only 3 entries. For
                example:
              </p>
              <pre><code>user::rw-
                group::r--
                others::---
              </code></pre>
            
          
          
            <h4 class="bold">Block Access to a Sub-Tree for a Specific User</h4>
            
              <p>In this use case, a deeply nested file system sub-tree was created as world-readable, followed
                by a subsequent requirement to block access for a specific user to all files in that sub-tree.
              </p>
              <p>
                This use case can be addressed by setting an ACL on the root of the sub-tree with a named user entry
                that strips all access from the user.
              </p>
              <p>
                For this file system structure:
              </p>
              <pre><code>dir1
                `-- dir2
                `-- dir3
                |-- file1
                |-- file2
                `-- file3
              </code></pre>
              <p>dir1`-- dir2`-- dir3|-- file1|-- file2`-- file3</p>
              <p>Setting the following ACL on "dir2" blocks access for Bruce to "dir3," "file1," "file2," and
                "file3":
              </p>
              <pre><code>user:bruce:---</code></pre>
              <p>More specifically, the removal of execute permissions on "dir2" means that Bruce cannot access
                "dir2", and therefore cannot see any of its children. This also means that access is blocked
                automatically for any newly-added files under "dir2". If a "file4" is created under "dir3", Bruce will
                not be able to access it.
              </p>
            
          
          
            <h4 class="bold">ACLs with Sticky Bit</h4>
            
              <p>In this use case, multiple named users or named groups require full access to a general- purpose
                shared directory, such as "/tmp". However, Write and Execute permissions on the directory also give
                users the ability to delete or rename any files in the directory, even files created by other users.
                Users must be restricted so they are only allowed to delete or rename files that they created.
              </p>
              <p>
                This use case can be addressed by combining an ACL with the sticky bit. The sticky bit is existing
                functionality that currently works with Permission Bits. It will continue to work as expected in
                combination with ACLs.
              </p>
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-3bd0808b-b357-4d88-8e94-a93c49b3094b">Archival Storage</h3>
        
          <p>This guide describes how to use storage policies to assign files and directories to archival storage
            types.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>Archival storage lets you store data on physical media with high storage density and low
              processing resources.
            </p>
            <ul class="number-list">
              Implementing archival storage involves the following steps:
              
                <li>
                  <p>Shut down the DataNode.</p>
                </li>
                <li>
                  <p>Assign the ARCHIVE storage type to DataNodes designed for archival storage.</p>
                </li>
                <li>
                  <p>Set HOT, WARM, or COLD storage policies on HDFS files and directories.</p>
                </li>
                <li>
                  <p>Restart the DataNode.</p>
                </li>
              
            </ul>
            <p>If you update a storage policy setting on a file or directory, you must use the HDFS mover data
              migration tool to actually move blocks as specified by the new storage policy.
            </p>
          
        
        
          <h4 class="bold">HDFS Storage Types</h4>
          
            <ul class="bullet-list">
              HDFS storage types can be used to assign data to different types of physical storage media.
                The following storage types are available:
              
              
                <li>
                  <p>DISK -- Disk drive storage (default storage type)</p>
                </li>
                <li>
                  <p>ARCHIVE -- Archival storage (high storage density, low processing resources)</p>
                </li>
                <li>
                  <p>SSD -- Solid State Drive</p>
                </li>
                <li>
                  <p>RAM_DISK -- DataNode Memory</p>
                </li>
              
            </ul>
            <p>If no storage type is assigned, DISK is used as the default storage type.</p>
          
        
        
          <h4 class="bold">Storage Policies: Hot, Warm, and Cold</h4>
          
            <ul class="bullet-list">
              You can store data on DISK or ARCHIVE storage types using the following preconfigured
                storage policies:
              
              
                <li>
                  <p><strong>HOT</strong>– Used for both storage and compute. Data that is being used for processing
                    will stay in this policy. When a block is HOT, all replicas are stored on DISK. There is no fallback
                    storage for creation, and ARCHIVE is used for replication fallback storage.
                  </p>
                </li>
                <li>
                  <p>
                    <strong>WARM</strong>
                    - Partially HOT and partially COLD. When a block is WARM, the first replica is stored on DISK, and
                    the remaining replicas are stored on ARCHIVE. The fallback storage for both creation and replication
                    is DISK, or ARCHIVE if DISK is unavailable.
                  </p>
                </li>
                <li>
                  <p>
                    <strong>COLD</strong>
                    - Used only for storage, with limited compute. Data that is no longer being used, or data that needs
                    to be archived, is moved from HOT storage to COLD storage. When a block is COLD, all replicas are
                    stored on ARCHIVE, and there is no fallback storage for creation or replication
                  </p>
                </li>
              
            </ul>
            <p>The following table summarizes these replication policies:</p>
            <div class="xyleme-table"><table border="1">
              
                
                
                
                
                
                <thead>
                  <tr>
                    <th rowspan="1">
                      <p>
                        <strong>Policy ID</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Policy Name</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Replica Block Placement (for n replicas)</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Fallback storage for creation</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Fallback storage for replication</strong>
                      </p>
                    </th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td rowspan="1">
                      <p>12</p>
                    </td>
                    <td rowspan="1">
                      <p>HOT (default)</p>
                    </td>
                    <td rowspan="1">
                      <p>DISK:
                        <i>n</i>
                      </p>
                    </td>
                    <td rowspan="1">
                      <p>&lt;none&gt;</p>
                    </td>
                    <td rowspan="1">
                      <p>ARCHIVE</p>
                    </td>
                  </tr>
                  <tr>
                    <td rowspan="1">
                      <p>8</p>
                    </td>
                    <td rowspan="1">
                      <p>WARM</p>
                    </td>
                    <td rowspan="1">
                      <p>DISK: 1, ARCHIVE:<i>n</i>-1
                      </p>
                    </td>
                    <td rowspan="1">
                      <p>DISK, ARCHIVE</p>
                    </td>
                    <td rowspan="1">
                      <p>DISK, ARCHIVE</p>
                    </td>
                  </tr>
                  <tr>
                    <td rowspan="1">
                      <p>4</p>
                    </td>
                    <td rowspan="1">
                      <p>COLD</p>
                    </td>
                    <td rowspan="1">
                      <p>ARCHIVE:
                        <i>n</i>
                      </p>
                    </td>
                    <td rowspan="1">
                      <p>&lt;none&gt;</p>
                    </td>
                    <td rowspan="1">
                      <p>&lt;none&gt;</p>
                    </td>
                  </tr>
                </tbody>
                
              
            </table></div>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>Currently, storage policies cannot be edited.</p>
              </div>
            </aside>
          
        
        
          <h4 class="bold">Configuring Archival Storage</h4>
          
            <p>Use the following steps to configure archival storage:</p>
          
          
            <span class="title"><strong>1. Shut Down the DataNode</strong></span>
            
              <p>Shut down the DataNode using the applicable command in the "Controlling HDP Services Manually"
                section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
          
            <span class="title"><strong>2. Assign the ARCHIVE Storage Type to the DataNode</strong></span>
            
              <p>You can use the
                <code>dfs.name.dir property</code>
                in the
                <code>/etc/hadoop/conf/hdfs-site.xml</code>
                file to assign the ARCHIVE storage type to a DataNode.
              </p>
              <p>The dfs.name.dir property determines where on the local filesystem a DataNode should store its
                blocks.
              </p>
              <p>To specify a DataNode as DISK storage, use a local file system path as you normally would.
                Because DISK is the default storage type, nothing further is required. For example:
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;dfs.data.dir&lt;/name&gt;
                &lt;value&gt;file:///grid/1/tmp/data_trunk&lt;/value&gt;
                &lt;/property&gt;</code></pre>
              <p>To specify a DataNode as ARCHIVE storage, insert [ARCHIVE] at the beginning of the local file
                system path. For example:
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;dfs.data.dir&lt;/name&gt;
                &lt;value&gt;[ARCHIVE]file:///grid/1/tmp/data_trunk&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
          
            <span class="title"><strong>3. Set Storage Policies</strong></span>
            
              <span class="title"><strong>Set Storage Policy</strong></span>
              
                <p>Sets a storage policy on a file or a directory.</p>
                <p>Command:</p>
                <pre><code>hdfs dfsadmin -setStoragePolicy &lt;path&gt; &lt;policyName&gt;</code></pre>
                <p>Arguments:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>&lt;path&gt;</p>
                        </th>
                        <th rowspan="1">
                          <p>The path to a directory or a file.</p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>&lt;policyName&gt;</p>
                        </td>
                        <td rowspan="1">
                          <p>The name of the storage policy.</p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
                <p>Example:</p>
                <pre><code>hdfs dfsadmin -setStoragePolicy /cold1 COLD</code></pre>
              
            
            
              <span class="title"><strong>Get Storage Policy</strong></span>
              
                <p>Gets the storage policy of a file or a directory.</p>
                <p>Command:</p>
                <pre><code>hdfs dfsadmin -getStoragePolicy &lt;path&gt;</code></pre>
                <p>Arguments:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>&lt;path&gt;</p>
                        </th>
                        <th rowspan="1">
                          <p>The path to a directory or a file.</p>
                        </th>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
                <p>Example:</p>
                <pre><code>hdfs dfsadmin -getStoragePolicy /cold1</code></pre>
              
            
          
          
            <span class="title"><strong>4. Start the DataNode</strong></span>
            
              <p>Start the DataNode using the applicable commands in the "Controlling HDP Services Manually"
                section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
          
            <span class="title"><strong>5. Using Mover to Apply Storage Policies</strong></span>
            
              <p>When you update a storage policy setting on a file or directory, the new policy is not
                automatically enforced. You must use the HDFS
                <code>mover</code>
                data migration tool to actually move blocks as specified by the new storage policy.
              </p>
              <p>The
                <code>mover</code>
                data migration tool scans the specified files in HDFS and checks to see if the block placement satisfies
                the storage policy. For the blocks that violate the storage policy, it moves the replicas to a different
                storage type in order to fulfill the storage policy requirements.
              </p>
              <p>Command:</p>
              <pre><code>hdfs mover [-p &lt;files/dirs&gt; | -f &lt;local file name&gt;]</code></pre>
              <p>Arguments:</p>
              <div class="xyleme-table"><table border="1">
                
                  
                  
                  <thead></thead>
                  <tbody>
                    <tr>
                      <th rowspan="1">
                        <p>-p &lt;files/dirs&gt;</p>
                      </th>
                      <th rowspan="1">
                        <p>Specify a space-separated list of HDFS files/directories to migrate.</p>
                      </th>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>-f &lt;local file&gt;</p>
                      </td>
                      <td rowspan="1">
                        <p>Specify a local file containing a list of HDFS files/directories to migrate.
                        </p>
                      </td>
                    </tr>
                  </tbody>
                  
                
              </table></div>
              <ul class="bullet-list">
                
                  <li>
                    <p>Note that when both -p and -f options are omitted, the default path is the root
                      directory.
                    </p>
                  </li>
                
              </ul>
              <p>Example:</p>
              <pre><code>hdfs mover /cold1/testfile</code></pre>
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-7d7d0171-3a35-4553-96d9-f271bcb21c3b">Centralized Cache Management in HDFS</h3>
        
          <p>This guide provides instructions on setting up and using centralized cache management in HDFS.
            Centralized cache management enables you to specify paths to directories or files that will be cached by
            HDFS, thereby improving performance for applications that repeatedly access the same data.
          </p>
        
        
          <h4 class="bold">Overview</h4>
          
            <p>Centralized cache management in HDFS is an explicit caching mechanism that enables you to specify
              paths to directories or files that will be cached by HDFS. The NameNode will communicate with DataNodes
              that have the desired blocks available on disk, and instruct the DataNodes to cache the blocks in off-heap
              caches.
            </p>
            <ul class="bullet-list">
              Centralized cache management in HDFS offers many significant advantages:
              
                <li>
                  <p>Explicit pinning prevents frequently used data from being evicted from memory. This is
                    particularly important when the size of the working set exceeds the size of main memory, which is
                    common for many HDFS workloads.
                  </p>
                </li>
                <li>
                  <p>Because DataNode caches are managed by the NameNode, applications can query the set of
                    cached block locations when making task placement decisions. Co-locating a task with a cached block
                    replica improves read performance.
                  </p>
                </li>
                <li>
                  <p>When a block has been cached by a DataNode, clients can use a new, more efficient, zero-copy
                    read API. Since checksum verification of cached data is done once by the DataNode, clients can incur
                    essentially zero overhead when using this new API.
                  </p>
                </li>
                <li>
                  <p>Centralized caching can improve overall cluster memory utilization. When relying on the
                    Operating System (OS) buffer cache on each DataNode, repeated reads of a block will result in all
                    &lt;n&gt; replicas of the block being pulled into the buffer cache. With centralized cache
                    management, you can explicitly pin only &lt;m&gt; of the &lt;n&gt; replicas, thereby saving &lt;n-m&gt;
                    memory.
                  </p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Caching Use Cases</h4>
          
            <p>Centralized cache management is useful for:</p>
            <ul class="bullet-list">
              
                <li>
                  <p><strong>Files that are accessed repeatedly --</strong>For example, a small fact table in Hive
                    that is often used for joins is a good candidate for caching. Conversely, caching the input of a
                    once- yearly reporting query is probably less useful, since the historical data might only be read
                    once.
                  </p>
                </li>
                <li>
                  <p><strong>Mixed workloads with performance SLAs --</strong>Caching the working set of a high
                    priority workload ensures that it does not compete with low priority workloads for disk I/O.
                  </p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Caching Architecture</h4>
          
            <p>The following figure illustrates the centralized cached management architecture.</p>
            <div class="figure">
              
                
                  
                  <img src="01-RawContent/zzzzz-LegacyContent/bk_system-admin-guide-20140829-image_8.jpeg" width="50">
                
              
            </div>
            <p>
              In this architecture, the NameNode is responsible for coordinating all of the DataNode off-heap caches in
              the cluster. The NameNode periodically receives a cache report from each DataNode. The cache report
              describes all of the blocks cached on the DataNode. The NameNode manages DataNode caches by piggy-backing
              cache and uncache commands on the DataNode heartbeat.
            </p>
            <p>
              The NameNode queries its set of Cache Directives to determine which paths should be cached. Cache
              Directives are persistently stored in the fsimage and edit logs, and can be added, removed, and modified
              via Java and command-line APIs. The NameNode also stores a set of Cache Pools, which are administrative
              entities used to group Cache Directives together for resource management, and to enforce permissions.
            </p>
            <p>
              The NameNode periodically re-scans the namespace and active Cache Directives to determine which blocks
              need to be cached or uncached, and assigns caching work to DataNodes. Re-scans can also be triggered by
              user actions such as adding or removing a Cache Directive or removing a Cache Pool.
            </p>
            <p>
              Cache blocks that are under construction, corrupt, or otherwise incomplete are not cached. If a Cache
              Directive covers a symlink, the symlink target is not cached.
            </p>
            <p>
              Currently, caching can only be applied to directories and files.
            </p>
          
        
        
          <h4 class="bold">Caching Terminology</h4>
          
            <h4 class="bold">Cache Directive</h4>
            
              <p>A Cache Directive defines the path that will be cached. Paths can point either directories or
                files. Directories are cached non-recursively, meaning only files in the first-level listing of the
                directory will be cached.
              </p>
              <p>
                Cache Directives also specify additional parameters, such as the cache replication factor and expiration
                time. The replication factor specifies the number of block replicas to cache. If multiple Cache
                Directives refer to the same file, the maximum cache replication factor is applied.
              </p>
              <p>
                The expiration time is specified on the command line as a time-to-live (TTL), which represents a
                relative expiration time in the future. After a Cache Directive expires, it is no longer taken into
                consideration by the NameNode when making caching decisions.
              </p>
            
          
          
            <h4 class="bold">Cache Pool</h4>
            
              <p>A Cache Pool is an administrative entity used to manage groups of Cache Directives. Cache Pools
                have UNIX-like permissions that restrict which users and groups have access to the pool. Write
                permissions allow users to add and remove Cache Directives to the pool. Read permissions allow users to
                list the Cache Directives in a pool, as well as additional metadata. Execute permissions are unused.
              </p>
              <p>
                Cache Pools are also used for resource management. Cache Pools can enforce a maximum memory limit, which
                restricts the aggregate number of bytes that can be cached by directives in the pool. Normally, the sum
                of the pool limits will approximately equal the amount of aggregate memory reserved for HDFS caching on
                the cluster. Cache Pools also track a number of statistics to help cluster users track what is currently
                cached, and to determine what else should be cached.
              </p>
              <p>
                Cache Pools can also enforce a maximum time-to-live. This restricts the maximum expiration time of
                directives being added to the pool.
              </p>
            
          
        
        
          <h4 class="bold">Configuring Centralized Caching</h4>
          
            <h4 class="bold">Native Libraries</h4>
            
              <p>In order to lock block files into memory, the DataNode relies on native JNI code found in
                <code>libhadoop.so</code>. Be sure to enable JNI if you are using HDFS centralized cache
                management.
              </p>
            
          
          
            <h4 class="bold">Configuration Properties</h4>
            
              <p>Configuration properties for centralized caching are specified in the
                <code>hdfs-site.xml</code>
                file.
              </p>
            
            
              <h4 class="bold">Required Properties</h4>
              
                <ul class="bullet-list">
                  Currently, only one property is required:
                  
                    <li>
                      <p><code>dfs.datanode.max.locked.memory

                      </code>This property determines the maximum amount of memory (in bytes) that a DataNode will
                        use for caching. The "locked-in-memory size" ulimit (<code>ulimit -l</code>) of the
                        DataNode user also needs to be increased to exceed this parameter (for more details, see the
                        following section on ). When setting this value, remember that you will need space in memory for
                        other things as well, such as the DataNode and application JVM heaps, and the operating system
                        page cache.

                        Example:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.datanode.max.locked.memory&lt;/name&gt;
                        &lt;value&gt;268435456&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                  
                </ul>
              
            
            
              <h4 class="bold">Optional Properties</h4>
              
                <p>The following properties are not required, but can be specified for tuning.</p>
                <ul class="bullet-list">
                  
                    <li>
                      <p>
                        <code>dfs.namenode.path.based.cache.refresh.interval.ms</code>

                        The NameNode will use this value as the number of milliseconds between subsequent cache path
                        re-scans. By default, this parameter is set to 300000, which is five minutes.

                        Example:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.namenode.path.based.cache.refresh.interval.ms&lt;/name&gt;
                        &lt;value&gt;300000&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                  
                </ul>
                <ul class="bullet-list">
                  
                    <li>
                      <p>
                        <code>dfs.time.between.resending.caching.directives.ms</code>

                        The NameNode will use this value as the number of milliseconds between resending caching
                        directives.

                        Example:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.time.between.resending.caching.directives.ms&lt;/name&gt;
                        &lt;value&gt;300000&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                  
                </ul>
                <ul class="bullet-list">
                  
                    <li>
                      <p>
                        <code>dfs.datanode.fsdatasetcache.max.threads.per.volume</code>

                        The DataNode will use this value as the maximum number of threads per volume to use for caching
                        new data. By default, this parameter is set to 4.

                        Example:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.datanode.fsdatasetcache.max.threads.per.volume&lt;/name&gt;
                        &lt;value&gt;4&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                  
                </ul>
                <ul class="bullet-list">
                  
                    <li>
                      <p>
                        <code>dfs.cachereport.intervalMsec</code>

                        The DataNode will use this value as the number of milliseconds between sending a full report of
                        its cache state to the NameNode. By default, this parameter is set to 10000, which is 10
                        seconds.

                        Example:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.cachereport.intervalMsec&lt;/name&gt;
                        &lt;value&gt;10000&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                  
                </ul>
                <ul class="bullet-list">
                  
                    <li>
                      <p>dfs.namenode.path.based.cache.block.map.allocation.percent

                        The percentage of the Java heap that will be allocated to the cached blocks map. The cached
                        blocks map is a hash map that uses chained hashing. Smaller maps may be accessed more slowly if
                        the number of cached blocks is large; larger maps will consume more memory. The default value is
                        0.25 percent.

                        Example:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.namenode.path.based.cache.block.map.allocation.percent&lt;/name&gt;
                        &lt;value&gt;0.25&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                  
                </ul>
              
            
          
          
            <h4 class="bold">OS Limits</h4>
            
              <p>If you get the error "Cannot start datanode because the configured max locked memory size... is
                more than the datanode's available RLIMIT_MEMLOCK ulimit," that means that the operating system is
                imposing a lower limit on the amount of memory that you can lock than what you have configured. To fix
                this, you must adjust the
                <code>ulimit -l</code>
                value that the DataNode runs with. This value is usually configured in<code>
                  /etc/security/limits.conf</code>, but this may vary depending on what operating system and
                distribution you are using.
              </p>
              <p>
                You will know that you have correctly configured this value when you can run
                <code>ulimit - l</code>
                from the shell and get back either a higher value than what you have configured with , or the string
                "unlimited", which indicates that there is no limit. Note that it is typical for
                <code>ulimit -l</code>
                to output the memory lock limit in kilobytes (KB), but
                <code>dfs.datanode.max.locked.memory</code>
                must be specified in bytes.
              </p>
              <p>
                For example, if the value of
                <code>dfs.datanode.max.locked.memory</code>
                is set to 128000 bytes:
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;dfs.datanode.max.locked.memory&lt;/name&gt;
                &lt;value&gt;128000&lt;/value&gt;
                &lt;/property&gt;</code></pre>
              <p>You would set the
                <code>memlock</code>
                (max locked-in-memory address space) to a slightly higher value. For example, to set memlock to 130 KB
                (130,000 bytes) for the hdfs user, you would add the following line to<code>
                  /etc/security/limits.conf</code>.
              </p>
              <pre><code>hdfs - memlock 130</code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>The information in this section does not apply to deployments on Windows. Windows has no
                    direct equivalent of<code>ulimit -l</code>.
                  </p>
                </div>
              </aside>
            
          
        
        
          <h4 class="bold">Using Cache Pools and Directives</h4>
          
            <p>You can use the Command-Line Interface (CLI) to create, modify, and list Cache Pools and Cache
              Directives via the
              <code>hdfs cacheadmin</code>
              subcommand.
            </p>
            <p>
              Cache Directives are identified by a unique, non-repeating, 64-bit integer ID. IDs will not be reused even
              if a Cache Directive is removed.
            </p>
            <p>
              Cache Pools are identified by a unique string name.
            </p>
            <p>
              You must first create a Cache Pool, and then add Cache Directives to the Cache Pool.
            </p>
          
          
            <h4 class="bold">Cache Pool Commands</h4>
            
              <h4 class="bold">addPool</h4>
              
                <p>
                  Adds a new Cache Pool.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -addPool &lt;name&gt; [-owner &lt;owner&gt;] [-group &lt;group&gt;]
                  [-mode &lt;mode&gt;] [-limit &lt;limit&gt;] [-maxTtl &lt;maxTtl&gt;]
                </code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;name&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The name of the new pool.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;owner&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The user name of the owner of the pool. Defaults to the current user.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;group&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The group that the pool is assigned to. Defaults to the primary group name of the
                            current user.
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;mode&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The UNIX-style permissions assigned to the pool. Permissions are specified in octal,
                            e.g., 0755. Pool permissions are set to 0755 by default.  
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;limit&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The maximum number of bytes that can be cached by directives in the pool, in
                            aggregate. By default, no limit is set.
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;maxTtl&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The maximum allowed time-to-live for directives being added to the pool. This can be
                            specified in seconds, minutes, hours, and days, e.g., 120s, 30m, 4h, 2d. Valid units are
                            [smhd]. By default, no maximum is set. A value of "never" specifies that there is no limit.
                          </p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">modifyPool</h4>
              
                <p>Modifies the metadata of an existing Cache Pool.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -modifyPool &lt;name&gt; [-owner &lt;owner&gt;] [-group &lt;group&gt;]
                  [-mode &lt;mode&gt;] [-limit &lt;limit&gt;] [-maxTtl &lt;maxTtl&gt;]
                </code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;name&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The name of the pool to modify.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;owner&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The user name of the owner of the pool.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;group&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The group that the pool is assigned to.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;mode&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The UNIX-style permissions assigned to the pool. Permissions are specified in octal,
                            e.g., 0755.  
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;limit&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The maximum number of bytes that can be cached by directives in the pool, in
                            aggregate.
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;maxTtl&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The maximum allowed time-to-live for directives being added to the pool. This can be
                            specified in seconds, minutes, hours, and days, e.g., 120s, 30m, 4h, 2d. Valid units are
                            [smhd]. By default, no maximum is set. A value of "never" specifies that there is no limit.
                          </p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">removePool</h4>
              
                <p>Remove a Cache Pool. This also un-caches paths associated with the pool.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -removePool &lt;name&gt;</code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;name&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The name of the Cache Pool to remove.</p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">listPools</h4>
              
                <p>
                  Displays information about one or more Cache Pools, e.g., name, owner, group, permissions, etc.
                </p>
                <p>
                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -listPools [-stats] [&lt;name&gt;]</code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>-stats</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>Display additional Cache Pool statistics.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;name&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>If specified, list only the named Cache Pool.</p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">help</h4>
              
                <p>Displays detailed information about a command.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -help &lt;command-name&gt;</code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;command-name&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>Displays detailed information for the specified command name. If no command name is
                            specified, detailed help is displayed for all commands.
                          </p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
          
          
            <h4 class="bold">Cache Directive Commands</h4>
            
              <h4 class="bold">addDirective</h4>
              
                <p>Adds a new Cache Directive.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -addDirective -path &lt;path&gt; -pool &lt;pool-name&gt; [-force]
                  [-replication &lt;replication&gt;] [-ttl &lt;time-to-live&gt;]
                </code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;path&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The path to the cache directory or file.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;pool-name&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The Cache Pool to which the Cache Directive will be added. You must have Write
                            permission for the Cache Pool in order to add new directives.
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;-force&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>Skips checking of Cache Pool resource limits.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;-replication&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The UNIX-style permissions assigned to the pool. Permissions are specified in octal,
                            e.g., 0755. Pool permissions are set to 0755 by default. 
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;limit&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The cache replication factor to use. Defaults to 1.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;time-to-live&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>How long the directive is valid. This can be specified in minutes, hours, and days,
                            e.g., 30m, 4h, 2d. Valid units are [smhd]. A value of "never" indicates a directive that
                            never expires. If unspecified, the directive never expires.
                          </p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">removeDirective</h4>
              
                <p>Removes a Cache Directive.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -removeDirective &lt;id&gt;</code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;id&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The ID of the Cache Directive to remove. You must have Write permission for the pool
                            that the directive belongs to in order to remove it. You can use the
                            <code>-listDirectives</code>
                            command to display a list of Cache Directive IDs.
                          </p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">removeDirectives</h4>
              
                <p>Removes all of the Cache Directives in a specified path.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -removeDirectives &lt;path&gt; </code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;path&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>The path of the Cache Directives to remove. You must have Write permission for the
                            pool that the directives belong to in order to remove them. You can use the
                            <code>-listDirectives</code>
                            command to display a list of Cache Directives.
                          </p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
            
              <h4 class="bold">listDirectives</h4>
              
                <p>Returns a list of Cache Directives.

                  Usage:
                </p>
                <pre><code>hdfs cacheadmin -listDirectives [-stats] [-path &lt;path&gt;] [-pool &lt;pool&gt;]</code></pre>
                <p>Options:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>
                            <strong>Option</strong>
                          </p>
                        </th>
                        <th rowspan="1">
                          <p>
                            <strong>Description</strong>
                          </p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;path&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>List only the Cache Directives in this path. Note that if there is a Cache Directive
                            in the
                            <code>&lt;path&gt;</code>
                            that belongs to a Cache Pool for which you do not have Read access, it will not be listed.
                          </p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;pool&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>Only list the Cache Directives in the specified Cache Pool.</p>
                        </td>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>
                            <code>&lt;-stats&gt;</code>
                          </p>
                        </td>
                        <td rowspan="1">
                          <p>List path-based Cache Directive statistics.</p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
              
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-5003922c-f66e-4674-a91d-21dcb83cae12">Configuring HDFS Compression</h3>
        
          <p>This section describes how to configure HDFS compression on Linux.

            Linux supports<code>GzipCodec</code>,<code>DefaultCodec</code>,<code>
              BZip2Codec</code>,<code>LzoCodec</code>, and<code>SnappyCodec</code>.
            Typically,
            <code>GzipCodec</code>
            is used for HDFS compression. Use the following instructions to use<code>GZipCodec</code>.
          </p>
          <ul class="bullet-list">
            
              <li>
                <p>
                  <strong>Option I:</strong>
                  To use
                  <code>GzipCodec</code>
                  with a one-time only job:
                </p>
                <pre><code>hadoop jar hadoop-examples-1.1.0-SNAPSHOT.jar sort sbr"-Dmapred.compress.map.output=true"
                  sbr"-Dmapred.map.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec"sbr
                  "-Dmapred.output.compress=true"
                  sbr"-Dmapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec"sbr -outKey
                  org.apache.hadoop.io.Textsbr -outValue org.apache.hadoop.io.Text input output
                </code></pre>
              </li>
              <li>
                <p>
                  <strong>Option II:</strong>
                  To enable
                  <code>GzipCodec</code>
                  as the default compression:
                </p>
                <ul class="Numeric">
                  
                    <li>
                      <p>Edit the
                        <code>core-site.xml</code>
                        file on the NameNode host machine:
                      </p>
                      <pre><code>  &lt;property&gt;
                        &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
                        &lt;description&gt;A list of the compression codec classes that can be used
                        for compression/decompression.&lt;/description&gt;
                        &lt;/property&gt;</code></pre>
                    </li>
                    <li>
                      <p>Edit the
                        <code>mapred-site.xml</code>
                        file on the JobTracker host machine:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;mapred.compress.map.output&lt;/name&gt;
                        &lt;value&gt;true&lt;/value&gt;
                        &lt;/property&gt;

                        &lt;property&gt;
                        &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;
                        &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec&lt;/value&gt;
                        &lt;/property&gt;

                        &lt;property&gt;
                        &lt;name&gt;mapred.output.compression.type&lt;/name&gt;
                        &lt;value&gt;BLOCK&lt;/value&gt;
                        &lt;/property&gt; </code></pre>
                    </li>
                    <li>
                      <p>[Optional] - Enable the following two configuration parameters to enable job output
                        compression. Edit the
                        <code>mapred-site.xml</code>
                        file on the Resource Manager host machine:
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;mapred.output.compress&lt;/name&gt;
                        &lt;value&gt;true&lt;/value&gt;
                        &lt;/property&gt;

                        &lt;property&gt;
                        &lt;name&gt;mapred.output.compression.codec&lt;/name&gt;
                        &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec&lt;/value&gt;
                        &lt;/property&gt; </code></pre>
                    </li>
                    <li>
                      <p>Restart the cluster using the instructions provided in the "Controlling HDP Services
                        Manually" section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                          HDP Reference Guide</a>.
                      </p>
                    </li>
                  
                </ul>
              </li>
            
          </ul>
        
      
      
        <h3 class="horton-blue bold" id="ref-80037e5d-99a2-401b-a059-3090872178e1">Configuring Rack Awareness on HDP</h3>
        
          <p>Use the following instructions to configure rack awareness on a HDP cluster:</p>
        
        
          <h4 class="bold">1. Create a Rack Topology Script</h4>
          
            <p>Topology scripts are used by Hadoop to determine the rack location of nodes. This information is
              used by Hadoop to replicate block data to redundant racks.
            </p>
            <ul class="number-list">
              
                <li>
                  <p>Create a topology script and data file. The topology script MUST be executable.

                    <strong>Sample Topology Script

                    </strong>File name:
                    <code>rack-topology.sh</code>
                  </p>
                  <pre><code>#!/bin/bash

                    # Adjust/Add the property "net.topology.script.file.name"
                    # to core-site.xml with the "absolute" path the this
                    # file. ENSURE the file is "executable".

                    # Supply appropriate rack prefix
                    RACK_PREFIX=default

                    # To test, supply a hostname as script input:
                    if [ $# -gt 0 ]; then

                    CTL_FILE=${CTL_FILE:-"rack_topology.data"}

                    HADOOP_CONF=${HADOOP_CONF:-"/etc/hadoop/conf"}

                    if [ ! -f ${HADOOP_CONF}/${CTL_FILE} ]; then
                    echo -n "/$RACK_PREFIX/rack "
                    exit 0
                    fi

                    while [ $# -gt 0 ] ; do
                    nodeArg=$1
                    exec&lt; ${HADOOP_CONF}/${CTL_FILE}
                    result=""
                    while read line ; do
                    ar=( $line )
                    if [ "${ar[0]}" = "$nodeArg" ] ; then
                    result="${ar[1]}"
                    fi
                    done
                    shift
                    if [ -z "$result" ] ; then
                    echo -n "/$RACK_PREFIX/rack "
                    else
                    echo -n "/$RACK_PREFIX/rack_$result "
                    fi
                    done

                    else
                    echo -n "/$RACK_PREFIX/rack "
                    fi
                  </code></pre>
                  <p>
                    <strong>Sample Topology Data File</strong>

                    File name:
                    <code>rack_topology.data</code>
                  </p>
                  <pre><code># This file should be:
                    # - Placed in the /etc/hadoop/conf directory
                    # - On the Namenode (and backups IE: HA, Failover, etc)
                    # - On the Job Tracker OR Resource Manager (and any Failover JT's/RM's)
                    # This file should be placed in the /etc/hadoop/conf directory.

                    # Add Hostnames to this file. Format &lt;host ip&gt; &lt;rack_location&gt;
                    192.168.2.10 01
                    192.168.2.11 02
                    192.168.2.12 03
                  </code></pre>
                </li>
                <li>
                  <p>Copy both of these files to the
                    <code>/etc/hadoop/conf</code>
                    directory on all cluster nodes.
                  </p>
                </li>
                <li>
                  <p>Run the rack-topology.sh script to ensure that it returns the correct rack information for
                    each host.
                  </p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">2. Add the Topology Script Property to core-site.xml</h4>
          
            <ul class="number-list">
              
                <li>
                  <p>Stop HDFS using the applicable commands in the "Controlling HDP Services Manually" section
                    of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                      HDP Reference Guide</a>.
                  </p>
                </li>
                <li>
                  <p>Add the following property to<code>core-site.xml</code>:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;name&gt;net.topology.script.file.name&lt;/name&gt;
                    &lt;value&gt;/etc/hadoop/conf/rack-topology.sh&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
              
            </ul>
            <p>By default the topology script will process up to 100 requests per invocation. You can also
              specify a different number of requests with the
              <code>net.topology.script.number.args</code>
              property. For example:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;net.topology.script.number.args&lt;/name&gt;
              &lt;value&gt;75&lt;/value&gt;
              &lt;/property&gt;</code></pre>
          
        
        
          <h4 class="bold">3. Restart HDFS and MapReduce</h4>
          
            <p>Restart HDFS and MapReduce using the applicable commands in the "Controlling HDP Services
              Manually" section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">HDP
                Reference Guide</a>.
            </p>
          
        
        
          <h4 class="bold">4. Verify Rack Awareness</h4>
          
            <p>After the services have started, you can use the following methods to verify that rack awareness
              has been activated:
            </p>
            <ul class="bullet-list">
              
                <li>
                  <p>Look in the NameNode logs located in
                    <code>/var/log/hadoop/hdfs/</code>
                    (for example:<code>hadoop-hdfs-namenode-sandbox.log</code>). You should see an entry
                    like this:
                  </p>
                  <pre><code>014-01-13 15:58:08,495 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /rack01/&lt;ipaddress&gt;</code></pre>
                </li>
                <li>
                  <p>The Hadoop
                    <code>fsck</code>
                    command should return something like the following (if there are two racks):
                  </p>
                  <pre><code>Status: HEALTHY
                    Total size: 123456789 B
                    Total dirs: 0
                    Total files: 1
                    Total blocks (validated): 1 (avg. block size 123456789 B)
                    Minimally replicated blocks: 1 (100.0 %)
                    Over-replicated blocks: 0 (0.0 %)
                    Under-replicated blocks: 0 (0.0 %)
                    Mis-replicated blocks: 0 (0.0 %)
                    Default replication factor: 3
                    Average block replication: 3.0
                    Corrupt blocks: 0
                    Missing replicas: 0 (0.0 %)
                    Number of data-nodes: 40
                    Number of racks: 2
                    FSCK ended at Mon Jan 13 17:10:51 UTC 2014 in 1 milliseconds
                  </code></pre>
                </li>
                <li>
                  <p>The Hadoop
                    <code>dfsadmin -report</code>
                    command will return a report that includes the rack name next to each machine. The report should
                    look something like the following (partial) example:
                  </p>
                  <pre><code>[bsmith@hadoop01 ~]$ sudo -u hdfs hadoop dfsadmin -report
                    Configured Capacity: 19010409390080 (17.29 TB)
                    Present Capacity: 18228294160384 (16.58 TB)
                    DFS Remaining: 5514620928000 (5.02 TB)
                    DFS Used: 12713673232384 (11.56 TB) DFS Used%: 69.75%
                    Under replicated blocks: 181
                    Blocks with corrupt replicas: 0
                    Missing blocks: 0

                    -------------------------------------------------
                    Datanodes available: 5 (5 total, 0 dead)

                    Name: 192.168.90.231:50010 (h2d1.hdp.local)
                    Hostname: h2d1.hdp.local
                    Rack: /default/rack_02
                    Decommission Status : Normal
                    Configured Capacity: 15696052224 (14.62 GB)
                    DFS Used: 314380288 (299.82 MB)
                    Non DFS Used: 3238612992 (3.02 GB)
                    DFS Remaining: 12143058944 (11.31 GB)
                    DFS Used%: 2.00%
                    DFS Remaining%: 77.36%
                    Configured Cache Capacity: 0 (0 B)
                    Cache Used: 0 (0 B)
                    Cache Remaining: 0 (0 B)
                    Cache Used%: 100.00%
                    Cache Remaining%: 0.00%
                    Last contact: Thu Jun 12 11:39:51 EDT 2014
                  </code></pre>
                </li>
              
            </ul>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-d296fac2-3f90-4c5f-846b-3353ac192933">Hadoop Archives</h3>
        
          <p>The Hadoop Distributed File System (HDFS) is designed to store and process large data sets, but HDFS
            can be less efficient when storing a large number of small files. When there are many small files stored in
            HDFS, these small files occupy a large portion of the namespace. As a result, disk space is under-utilized
            because of the namespace limitation.

            Hadoop Archives (HAR) can be used to address the namespace limitations associated with storing many small
            files. A Hadoop Archive packs small files into HDFS blocks more efficiently, thereby reducing NameNode
            memory usage while still allowing transparent access to files. Hadoop Archives are also compatible with
            MapReduce, allowing transparent access to the original files by MapReduce jobs.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>The Hadoop Distributed File System (HDFS) is designed to store and process large (terabytes) data
              sets. For example, a large production cluster may have 14 PB of disk space and store 60 million files.

              However, storing a large number of small files in HDFS is inefficient. A file is generally considered to
              be "small" when its size is substantially less than the HDFS block size, which is 256 MB by default in
              HDP. Files and blocks are name objects in HDFS, meaning that they occupy namespace (space on the
              NameNode). The namespace capacity of the system is therefore limited by the physical memory of the
              NameNode.

              When there are many small files stored in the system, these small files occupy a large portion of the
              namespace. As a consequence, the disk space is underutilized because of the namespace limitation. In one
              real-world example, a production cluster had 57 million files less than 256 MB in size, with each of these
              files taking up one block on the NameNode. These small files used up 95% of the namespace but occupied
              only 30% of the cluster disk space.

              Hadoop Archives (HAR) can be used to address the namespace limitations associated with storing many small
              files. HAR packs a number of small files into large files so that the original files can be accessed
              transparently (without expanding the files).

              HAR increases the scalability of the system by reducing the namespace usage and decreasing the operation
              load in the NameNode. This improvement is orthogonal to memory optimization in the NameNode and
              distributing namespace management across multiple NameNodes.

              Hadoop Archive is also compatible with MapReduce — it allows parallel access to the original files by
              MapReduce jobs.
            </p>
          
        
        
          <h4 class="bold">Hadoop Archive Components</h4>
          
            <h4 class="bold">HAR Format Data Model</h4>
            
              <p>The Hadoop Archive data format has the following layout:</p>
              <pre><code>foo.har/_masterindex //stores hashes and offsets
                foo.har/_index //stores file statuses
                foo.har/part-[1..n] //stores actual file data
              </code></pre>
              <p>The file data is stored in multi-part files, which are indexed in order to retain the original
                separation of data. Moreover, the part files can be accessed in parallel by MapReduce programs. The
                index files also record the original directory tree structures and file status.
              </p>
            
          
          
            <h4 class="bold">HAR File System</h4>
            
              <p>Most archival systems, such as tar, are tools for archiving and de-archiving. Generally, they do
                not fit into the actual file system layer and hence are not transparent to the application writer in
                that the user must de-archive (expand) the archive before use.

                The Hadoop Archive is integrated with the Hadoop file system interface. The
                <code>HarFileSystem</code>
                implements the
                <code>FileSystem</code>
                interface and provides access via the
                <code>har://</code>
                scheme. This exposes the archived files and directory tree structures transparently to users. Files in a
                HAR can be accessed directly without expanding it.

                For example, if we have the following command to copy a HDFS file to a local directory:
              </p>
              <pre><code>hdfs dfs –get hdfs://namenode/foo/file-1 localdir</code></pre>
              <p>Suppose a Hadoop Archive
                <code>bar.har</code>
                is created from the
                <code>foo</code>
                directory. With the HAR, the command to copy the original file becomes:
              </p>
              <pre><code>hdfs dfs –get har://namenode/bar.har/foo/file-1 localdir</code></pre>
              <p>Users only need to change the URI paths. Alternatively, users may choose to create a symbolic
                link (from
                <code>hdfs://namenode/foo</code>
                to
                <code>har://namenode/bar.har/foo</code>
                in the example above), and then even the URIs do not need to be changed. In either case,
                <code>HarFileSystem</code>
                will be invoked automatically to provide access to the files in the HAR. Because of this transparent
                layer, HAR is compatible with the Hadoop APIs, MapReduce, the FS shell command-line interface, and
                higher-level applications such as Pig, Zebra, Streaming, Pipes, and DistCp.
              </p>
            
          
          
            <h4 class="bold">Hadoop Archiving Tool</h4>
            
              <p>Hadoop Archives can be created using the Hadoop archiving tool. The archiving tool uses
                MapReduce to efficiently create Hadoop Archives in parallel. The tool can be invoked using the command:
              </p>
              <pre><code>hadoop archive -archiveName name -p &lt;parent&gt; &lt;src&gt;* &lt;dest&gt;</code></pre>
              <p>A list of files is generated by traversing the source directories recursively, and then the list
                is split into map task inputs. Each map task creates a part file (about 2 GB, configurable) from a
                subset of the source files and outputs the metadata. Finally, a reduce task collects metadata and
                generates the index files.
              </p>
            
          
        
        
          <h4 class="bold">Creating a Hadoop Archive</h4>
          
            <p>The Hadoop archiving tool can be invoked using the following command format:</p>
            <pre><code>hadoop archive -archiveName name -p &lt;parent&gt; &lt;src&gt;* &lt;dest&gt; </code></pre>
            <p>Where -archiveName is the name of the archive you would like to create. The archive name should be
              given a .har extension. The
              <code>&lt;parent&gt;</code>
              argument is used to specify the relative path to the location where the files will be archived in the HAR.

              <strong>Example</strong>
            </p>
            <pre><code>hadoop archive -archiveName foo.har -p /user/hadoop dir1 dir2 /user/zoo</code></pre>
            <p>This example creates an archive using
              <code>/user/hadoop</code>
              as the relative archive directory. The directories
              <code>/user/hadoop/dir1</code>
              and
              <code>/user/hadoop/dir2</code>
              will be archived in the
              <code>/user/zoo/foo.har</code>
              archive.

              Archiving does not delete the source files. If you would like to delete the input files after creating an
              archive (to reduce namespace), you must manually delete the source files.

              Although the hadoop archive command can be run from the host file system, the archive file is created in
              the HDFS file system -- from directories that exist in HDFS. If you reference a directory on the host file
              system rather than in HDFS, you will get the following error:

              <code>The resolved paths set is empty. Please check whether the srcPaths exist, where srcPaths = [&lt;/directory/path&gt;]</code>

              To create the HDFS directories used in the preceding example, you would use the following series of
              commands:
            </p>
            <pre><code>hdfs dfs -mkdir /user/zoo
              hdfs dfs -mkdir /user/hadoop
              hdfs dfs -mkdir /user/hadoop/dir1
              hdfs dfs -mkdir /user/hadoop/dir2
            </code></pre>
          
        
        
          <h4 class="bold">Looking Up Files in Hadoop Archives</h4>
          
            <p>The
              <code>hdfs dfs -ls</code>
              command can be used to look up files in Hadoop archives. Using the example
              <code>/user/zoo/foo.har</code>
              archive created in the previous section, you would use the following command to list the files in the
              archive:
            </p>
            <pre><code>hdfs dfs -ls har:///user/zoo/foo.har/</code></pre>
            <p>The output would be:</p>
            <pre><code>har:///user/zoo/foo.har/dir1
              har:///user/zoo/foo.har/dir2
            </code></pre>
            <p>As you may recall, these archives were created with the following command:</p>
            <pre><code>hadoop archive -archiveName foo.har -p /user/hadoop dir1 dir2 /user/zoo</code></pre>
            <p>If we were to change the command to:</p>
            <pre><code>hadoop archive -archiveName foo.har -p /user/ hadoop/dir1 hadoop/dir2 /user/zoo</code></pre>
            <p>And then run the following command:</p>
            <pre><code>hdfs dfs -ls -R har:///user/zoo/foo.har</code></pre>
            <p>The output would be:</p>
            <pre><code>har:///user/zoo/foo.har/hadoop
              har:///user/zoo/foo.har/hadoop/dir1
              har:///user/zoo/foo.har/hadoop/dir2
            </code></pre>
            <p>Notice that with the modified parent argument, the files have been archived relative to
              <code>/user/</code>
              rather than<code>/user/hadoop</code>.
            </p>
          
        
        
          <h4 class="bold">Hadoop Archives and MapReduce</h4>
          
            <p>To use Hadoop Archives with MapReduce, you need to reference files slightly differently than with
              the default file system. If you have a Hadoop Archive stored in HDFS in<code>/user/
                zoo/foo.har</code>, you would need to specify the input directory as
              <code>har:///user/zoo/foo.har</code>
              to use it as a MapReduce input. Since Hadoop Archives are exposed as a file system, MapReduce is able to
              use all of the logical input files in Hadoop Archives as input.
            </p>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-d7a595c0-305a-4282-a4a7-6f82ff538b40">JMX Metrics APIs for HDFS Daemons</h3>
        
          <p>You can use the following methods to access HDFS metrics using the Java Management Extensions (JMX)
            APIs.
          </p>
        
        
          <span class="title"><strong>Use the HDFS Daemon Web Interface</strong></span>
          
            <p>You can access JMX metrics through the web interface of an HDFS daemon. This is the recommended
              method.

              For example, you would use the following command format to access the NameNode JMX.
            </p>
            <pre><code>curl -i http://localhost:50070/jmx</code></pre>
            <p>You can use the
              <code>qry</code>
              parameter to fetch only a particular key:
            </p>
            <pre><code>curl -i http://localhost:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo</code></pre>
          
        
        
          <span class="title"><strong>Directly Access the JMX Remote Agent</strong></span>
          
            <p>This method requires that the JMX remote agent is enbabled with a JVM option when starting HDFS
              services.

              For example, the following JVM options in
              <code>hadoop-env.sh</code>
              are used to enable the JMX remote agent for the NameNode. It listens on port 8004 with SSL disabled. The
              user name and password are saved in the
              <code>mxremote.password</code>
              file.
            </p>
            <pre><code>export HADOOP_NAMENODE_OPTS="-Dcom.sun.management.jmxremote
              -Dcom.sun.management.jmxremote.password.file=$HADOOP_CONF_DIR/jmxremote.password
              -Dcom.sun.management.jmxremote.ssl=false
              -Dcom.sun.management.jmxremote.port=8004 $HADOOP_NAMENODE_OPTS"
            </code></pre>
            <p>Details about related settings can be found<a href="http://docs.oracle.com/javase/7/docs/technotes/guides/management/agent.html">here</a>.
              You can also use the
              <a href="https://code.google.com/p/jmxquery/">jmxquery tool</a>
              to retrieve information through JMX.

              Hadoop also has a built-in JMX query tool,<code>jmxget</code>. For example:
            </p>
            <pre><code>hdfs jmxget -server localhost -port 8004 -service NameNode</code></pre>
            <p>Note that
              <code>jmxget</code>
              requires that authentication be disabled, as it does not accept a user name and password.

              Using JMX can be challenging for operations personnel who are not familiar with JMX setup, especially JMX
              with SSL and firewall tunnelling.
              Therefore, it is generally recommended that you collect JXM information through the web interface of HDFS
              daemons rather than directly accessing the JMX remote agent.
            </p>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-c32016e3-6397-42b3-8aa0-0790271e9883">Memory as Storage (Technical Preview)</h3>
        
          <p>This guide describes how to use DataNode memory as storage in HDFS.</p>
          <aside class="custom-note">
            <div class="icon"><img src="Icons/Note.png" width="50"></div>
            <div class="simple-block">
              <p>This feature is a technical preview and considered under development. Do not use this feature in
                your production systems. If you have questions regarding this feature, contact Support by logging a case
                on our Pivotal Support Portal at<a href="https://support.hortonworks.com">
                  https://support.hortonworks.com</a>.
              </p>
            </div>
          </aside>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>HDFS supports efficient writes of large data sets to durable storage, and also provides reliable
              access to the data. This works well for batch jobs that write large amounts of persistent data.

              Emerging classes of applications are driving use cases for writing smaller amounts of temporary data.
              Using DataNode memory as storage addresses the use case of applications that want to write relatively
              small amounts of intermediate data sets with low latency.

              Writing block data to memory reduces durability, as data can be lost due to process restart before it is
              saved to disk. HDFS attempts to save replica data to disk in a timely manner to reduce the window of
              possible data loss.

              DataNode memory is referenced using the RAM_DISK storage type and the LAZY_PERSIST storage policy.
            </p>
            <ul class="number-list">
              Using DataNode memory as HDFS storage involves the following steps:
              
                <li>
                  <p>Shut down the DataNode.</p>
                </li>
                <li>
                  <p>Mount a portion of DataNode memory for use by HDFS.</p>
                </li>
                <li>
                  <p>Assign the RAM_DISK storage type to the DataNode, and enable short-circuit reads.</p>
                </li>
                <li>
                  <p>Set the LAZY_PERSIST storage policy on the HDFS files and directories that will use memory
                    as storage.
                  </p>
                </li>
                <li>
                  <p>Restart the DataNode.</p>
                </li>
              
            </ul>
            <p>If you update a storage policy setting on a file or directory, you must use the HDFS
              <code>mover</code>
              data migration tool to actually move blocks as specified by the new storage policy.

              Memory as storage represents one aspect of YARN resource management capabilities that includes CPU
              scheduling, CGroups, node labels, and archival storage.
            </p>
          
        
        
          <h4 class="bold">HDFS Storage Types</h4>
          
            <ul class="bullet-list">
              HDFS storage types can be used to assign data to different types of physical storage media.
                The following storage types are available:
              
              
                <li>
                  <p>DISK -- Disk drive storage (default storage type)</p>
                </li>
                <li>
                  <p>ARCHIVE -- Archival storage (high storage density, low processing resources)</p>
                </li>
                <li>
                  <p>SSD -- Solid State Drive</p>
                </li>
                <li>
                  <p>RAM_DISK -- DataNode Memory</p>
                </li>
              
            </ul>
            <p>If no storage type is assigned, DISK is used as the default storage type.</p>
          
        
        
          <h4 class="bold">The LAZY_PERSIST Memory Storage Policy</h4>
          
            <p>You can store data on configured DataNode memory using the LAZY_PERSIST storage policy.

              For LAZY_PERSIST, the first replica is stored on RAM_DISK (DataNode memory), and the remaining replicas
              are stored on DISK. The fallback storage for both creation and replication is DISK.

              The following table summarizes these replication policies:
            </p>
            <div class="xyleme-table"><table border="1">
              
                
                
                
                
                
                <thead>
                  <tr>
                    <th rowspan="1">
                      <p>
                        <strong>Policy ID</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Policy Name</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Block Placement (for n replicas)</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Fallback storage for creation</strong>
                      </p>
                    </th>
                    <th rowspan="1">
                      <p>
                        <strong>Fallback storage for replication</strong>
                      </p>
                    </th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td rowspan="1">
                      <p>15</p>
                    </td>
                    <td rowspan="1">
                      <p>LAZY_PERSIST</p>
                    </td>
                    <td rowspan="1">
                      <p>RAM_DISK: 1, DISK:<i>n</i>-1
                      </p>
                    </td>
                    <td rowspan="1">
                      <p>DISK</p>
                    </td>
                    <td rowspan="1">
                      <p>DISK</p>
                    </td>
                  </tr>
                </tbody>
                
              
            </table></div>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>Currently, storage policies cannot be edited.</p>
              </div>
            </aside>
          
        
        
          <h4 class="bold">Configuring Memory as Storage</h4>
          
            <p>Use the following steps to configure DataNode memory as storage:</p>
          
          
            <span class="title"><strong>1. Shut Down the DataNode</strong></span>
            
              <p>Shut down DataNode using the applicable commands in the "Controlling HDP Services Manually"
                section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
          
            <span class="title"><strong>2. Mount a Portion of DataNode Memory for HDFS</strong></span>
            
              <p>To use DataNode memory as storage, you must first mount a portion of the DataNode memory for use
                by HDFS.
              </p>
              <p>For example, you would use the following commands to allocate 2GB of memory for HDFS storage:
              </p>
              <pre><code>sudo mkdir -p /mnt/hdfsramdisk
                sudo mount -t tmpfs -o size=2048m tmpfs /mnt/hdfsramdisk
                Sudo mkdir -p /usr/lib/hadoop-hdfs
              </code></pre>
            
          
          
            <span class="title"><strong>3. Assign the RAM_DISK Storage Type and Enable Short-Circuit Reads</strong></span>
            
              <p>Edit the following properties in the
                <code>/etc/hadoop/conf/hdfs-site.xml</code>
                file to assign the RAM_DISK storage type to DataNodes and enable short-circuit reads.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>The
                      <code>dfs.name.dir</code>
                      property determines where on the local filesystem a DataNode should store its blocks. To specify a
                      DataNode as RAM_DISK storage, insert [RAM_DISK] at the beginning of the local file system mount
                      path and add it to the
                      <code>dfs.name.dir</code>
                      property.
                    </p>
                  </li>
                  <li>
                    <p>To enable short-circuit reads, set the value of
                      <code>dfs.client.read.shortcircuit</code>
                      to<code>true</code>.
                    </p>
                  </li>
                
              </ul>
              <p>For example:</p>
              <pre><code> &lt;property&gt;
                &lt;name&gt;dfs.data.dir&lt;/name&gt;
                &lt;value&gt;file:///grid/3/aa/hdfs/data/,[RAM_DISK]file:///mnt/hdfsramdisk/&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
                &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.checksum.type&lt;/name&gt;
                &lt;value&gt;NULL&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
          
            <span class="title"><strong>4. Set the LAZY_PERSIST Storage Policy on Files or Directories</strong></span>
            
              <span class="title"><strong>Set Storage Policy</strong></span>
              
                <p>Sets a storage policy on a file or a directory.

                  Command:
                </p>
                <pre><code>hdfs dfsadmin -setStoragePolicy &lt;path&gt; &lt;policyName&gt;</code></pre>
                <p>Arguments:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>&lt;path&gt;</p>
                        </th>
                        <th rowspan="1">
                          <p>The path to a directory or a file.</p>
                        </th>
                      </tr>
                      <tr>
                        <td rowspan="1">
                          <p>&lt;policyName&gt;</p>
                        </td>
                        <td rowspan="1">
                          <p>The name of the storage policy.</p>
                        </td>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
                <p>Example:</p>
                <pre><code>hdfs dfsadmin -setStoragePolicy /memory1 LAZY_PERSIST</code></pre>
              
            
            
              <span class="title"><strong>Get Storage Policy</strong></span>
              
                <p>Gets the storage policy of a file or a directory.

                  Command:
                </p>
                <pre><code>hdfs dfsadmin -getStoragePolicy &lt;path&gt;</code></pre>
                <p>Arguments:</p>
                <div class="xyleme-table"><table border="1">
                  
                    
                    
                    <thead></thead>
                    <tbody>
                      <tr>
                        <th rowspan="1">
                          <p>&lt;path&gt;</p>
                        </th>
                        <th rowspan="1">
                          <p>The path to a directory or a file.</p>
                        </th>
                      </tr>
                    </tbody>
                    
                  
                </table></div>
                <p>Example:</p>
                <pre><code>hdfs dfsadmin -getStoragePolicy /memory1 LAZY_PERSIST</code></pre>
              
            
          
          
            <span class="title"><strong>5. Start the DataNode</strong></span>
            
              <p>Start the DataNode using the applicable commands in the "Controlling HDP Services Manually"
                section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
          
            <span class="title"><strong>Using Mover to Apply Storage Policies</strong></span>
            
              <p>When you update a storage policy setting on a file or directory, the new policy is not
                automatically enforced. You must use the HDFS
                <code>mover</code>
                data migration tool to actually move blocks as specified by the new storage policy.

                The
                <code>mover</code>
                data migration tool scans the specified files in HDFS and checks to see if the block placement satisfies
                the storage policy. For the blocks that violate the storage policy, it moves the replicas to the
                applicable storage type in order to fulfill the storage policy requirements.

                Command:
              </p>
              <pre><code>hdfs mover [-p &lt;files/dirs&gt; | -f &lt;local file name&gt;]</code></pre>
              <p>Arguments:</p>
              <div class="xyleme-table"><table border="1">
                
                  
                  
                  <thead></thead>
                  <tbody>
                    <tr>
                      <th rowspan="1">
                        <p>-p &lt;files/dirs&gt;</p>
                      </th>
                      <th rowspan="1">
                        <p>Specify a space-separated list of HDFS files/directories to migrate.</p>
                      </th>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>-f &lt;local file&gt;</p>
                      </td>
                      <td rowspan="1">
                        <p>Specify a local file containing a list of HDFS files/directories to migrate.
                        </p>
                      </td>
                    </tr>
                  </tbody>
                  
                
              </table></div>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>When both -p and -f options are omitted, the default path is the root directory.</p>
                </div>
              </aside>
              <p>Example:</p>
              <pre><code>hdfs mover /memory1/testfile</code></pre>
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-48c8e9eb-e788-49df-be79-0b48116b1ee4">Running DataNodes as Non-root</h3>
        
          <p>This guide describes how to run DataNodes as a non-root user.</p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>Historically, part of the security configuration for HDFS involved starting the DataNode as the
              root user, and binding to privileged ports for the server endpoints. This was done to address a security
              issue whereby if a MapReduce task was running and the DataNode stopped running, it would be possible for
              the MapReduce task to bind to the DataNode port and potentially do something malicious. The solution to
              this scenario was to run the DataNode as the root user and use privileged ports. Only the root user can
              access privileged ports.

              You can now use Simple Authentication and Security Layer (SASL) to securely run DataNodes as a non-root
              user. SASL is used to provide secure communication at the protocol level.
            </p>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Important.png" width="50"></div>
              <div class="simple-block">
                <p>Please note that it is very important to execute a migration from using root to start
                  DataNodes to using SASL to start DataNodes in a very specific sequence across the entire cluster.
                  Otherwise, there could be a risk of application downtime.

                  In order to migrate an existing cluster that used root authentication to start using SASL instead,
                  first ensure that version 2.6.0 or later has been deployed to all cluster nodes as well as any
                  external applications that need to connect to the cluster. Only versions 2.6.0 and later of the HDFS
                  client can connect to a DataNode that uses SASL for authentication of data transfer protocol, so it is
                  vital that all callers have the correct version before migrating. After version 2.6.0 or later has
                  been deployed everywhere, update the configuration of any external applications to enable SASL. If an
                  HDFS client is enabled for SASL, it can connect successfully to a DataNode running with either root
                  authentication or SASL authentication. Changing configuration for all clients guarantees that
                  subsequent configuration changes on DataNodes will not disrupt the applications. Finally, each
                  individual DataNode can be migrated by changing its configuration and restarting. It is acceptable to
                  temporarily have a mix of some DataNodes running with root authentication and some DataNodes running
                  with SASL authentication during this migration period, because an HDFS client enabled for SASL can
                  connect to both.
                </p>
              </div>
            </aside>
          
        
        
          <h4 class="bold">Configuring DataNode SASL</h4>
          
            <p>Use the following steps to configure DataNode SASL to securely run a DataNode as a non-root
              user:
            </p>
          
          
            <span class="title"><strong>1. Shut Down the DataNode</strong></span>
            
              <p>Shut down the DataNode using the applicable commands in the "Controlling HDP Services Manually"
                section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
          
            <span class="title"><strong>2. Enable SASL</strong></span>
            
              <p>Configure the following properties in the
                <code>/etc/hadoop/conf/hdfs-site.xml</code>
                file to enable DataNode SASL.
              </p>
              <ul class="bullet-list">
                The
                  <code>dfs.data.transfer.protection</code>
                  property enables DataNode SASL. You can set this property to one of the following values:
                
                
                  <li>
                    <p>
                      <code>authentication</code>
                      -- Establishes mutual authentication between the client and the server.
                    </p>
                  </li>
                  <li>
                    <p>
                      <code>integrity</code>
                      -- in addition to authentication, it guarantees that a man-in-the-middle cannot tamper with
                      messages exchanged between the client and the server.
                    </p>
                  </li>
                  <li>
                    <p>
                      <code>privacy</code>
                      -- in addition to the features offered by authentication and integrity, it also fully encrypts the
                      messages exchanged between the client and the server.
                    </p>
                  </li>
                
              </ul>
              <p>In addition to setting a value for the
                <code>dfs.data.transfer.protection</code>
                property, you must set the
                <code>dfs.http.policy</code>
                property to<code>HTTPS_ONLY</code>. You must also specify ports for the DataNode RPC and
                HTTP Servers.
              </p>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>For more information on configuring SSL, see<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Security_Guide_v22/index.html#Item1.1">
                    Enable SSL on HDP Components</a>.
                  </p>
                </div>
              </aside>
              <p>For example:</p>
              <pre><code> &lt;property&gt;
                &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;
                &lt;value&gt;integrity&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.datanode.address&lt;/name&gt;
                &lt;value&gt;0.0.0.0:10019&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;
                &lt;value&gt;0.0.0.0:10022&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.http.policy&lt;/name&gt;
                &lt;value&gt;HTTPS_ONLY&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>If you are already using the following encryption setting:</p>
                  <p>
                    <code>dfs.encrypt.data.transfer=true</code>
                  </p>
                  <p>This is similar to:</p>
                  <p>
                    <code>dfs.data.transfer.protection=privacy</code>
                  </p>
                  <p>These two settings are mutually exclusive, so you should not have both of them set. However,
                    if both are set,
                    <code>dfs.encrypt.data.transfer</code>
                    will not be used.
                  </p>
                </div>
              </aside>
            
          
          
            <span class="title"><strong>3. Update Environment Settings</strong></span>
            
              <p>Edit the following setting in the
                <code>/etc/hadoop/conf/hadoop-env.xml</code>
                file, as shown below:
              </p>
              <pre><code>#On secure datanodes, user to run the datanode as after dropping privileges
                export HADOOP_SECURE_DN_USER=
              </code></pre>
              <p>The
                <code>export HADOOP_SECURE_DN_USER=hdfs</code>
                line enables the legacy security configuration, and must be set to an empty value in order for SASL to
                be enabled.
              </p>
            
          
          
            <span class="title"><strong>4. Start the DataNode</strong></span>
            
              <p>Start the DataNode using the applicable commands in the "Controlling HDP Services Manually"
                section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-d5a57c15-e92c-4730-98b8-2804ad1afe4f">Short-Circuit Local Reads on HDFS</h3>
        
          <p>In HDFS, reads normally go through the DataNode. Thus, when a client asks the DataNode to read a
            file, the DataNode reads that file off of the disk and sends the data to the client over a TCP socket.
            So-called "short-circuit" reads bypass the DataNode, allowing the client to read the file directly.
            Obviously, this is only possible in cases where the client is co-located with the data. Short-circuit reads
            provide a substantial performance boost to many applications.
          </p>
        
        
          <h4 class="bold">Prerequisites</h4>
          
            <p>To configure short-circuit local reads, you must enable<code>libhadoop.so</code>. See
              <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/NativeLibraries.html">
                Native Libraries
              </a>
              
              for details on enabling this library.
            </p>
          
        
        
          <h4 class="bold">Configuring Short-Circuit Local Reads on HDFS</h4>
          
            <p>To configure short-circuit local reads, add the following properties to the
              <code>hdfs-site.xml</code>
              file. Short-circuit local reads need to be configured on both the DataNode and the client.
            </p>
            <p>.</p>
          
          
            <h4 class="bold">Table 15.1. Short-Circuit Local Read Properties in hdfs-site.xml</h4>
            
              <div class="xyleme-table"><table border="1">
                
                  
                  
                  
                  <thead>
                    <tr>
                      <th rowspan="1">
                        <p>
                          <strong>Property Name</strong>
                        </p>
                      </th>
                      <th rowspan="1">
                        <p>
                          <strong>Property Value</strong>
                        </p>
                      </th>
                      <th rowspan="1">
                        <p>
                          <strong>Description</strong>
                        </p>
                      </th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.read.shortcircuit</p>
                      </td>
                      <td rowspan="1">
                        <p>true</p>
                      </td>
                      <td rowspan="1">
                        <p>Set this to true to enable short-circuit local reads.</p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.domain.socket.path</p>
                      </td>
                      <td rowspan="1">
                        <p>/var/lib/hadoop-hdfs/ dn_socket</p>
                      </td>
                      <td rowspan="1">
                        <p>The path to the domain socket. Short-circuit reads make use of a UNIX domain socket.
                          This is a special path in the file system that allows the client and the DataNodes to
                          communicate. You will need to set a path to this socket. The DataNode needs to be able to
                          create this path. On the other hand, it should not be possible for any user except the hdfs
                          user or root to create this path. For this reason, paths under /var/run or /var/lib are often
                          used.
                        </p>
                        <p>in the file system that allows the client and the DataNodes to communicate. You will
                          need to set a path to this socket. The DataNode needs to be able to create this path. On the
                          other hand, it should not be possible for any user except the hdfs user or root to create this
                          path. For this reason, paths under
                        </p>
                        <p>/var/run or /var/lib are often used.</p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.domain.socket.data.traffic</p>
                      </td>
                      <td rowspan="1">
                        <p>false</p>
                      </td>
                      <td rowspan="1">
                        <p>This property controls whether or not normal data traffic will be passed through the
                          UNIX domain socket. This feature has not been certified with HDP releases, so it is
                          recommended that you set the value of this property to<code>false</code>.
                        </p>
                        <p>not normal data traffic will be passed through the UNIX domain socket. This feature
                          has not been certified with HDP releases, so it is recommended that you set the value of this
                          property to false.
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.use.legacy.blockreader.local</p>
                      </td>
                      <td rowspan="1">
                        <p>false</p>
                      </td>
                      <td rowspan="1">
                        <p>Setting this value to
                          <code>false</code>
                          specifies that the new version (based on HDFS-347) of the short-circuit reader is used. This
                          new new short-circuit reader implementation is supported and recommended for use with HDP.
                          Setting this value to
                          <code>true</code>
                          would mean that the legacy short-circuit reader would be used.
                        </p>
                        <p>and recommended for use with HDP. Setting this value to true would mean</p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.datanode.hdfs-blocks-metadata.enabled</p>
                      </td>
                      <td rowspan="1">
                        <p>true</p>
                      </td>
                      <td rowspan="1">
                        <p>Boolean which enables back-end DataNode-side support for the experimental
                          DistributedFileSystem#getFileVBlockStorageLocations API.
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.file-block-storage-locations.timeout</p>
                      </td>
                      <td rowspan="1">
                        <p>60</p>
                      </td>
                      <td rowspan="1">
                        <p>Timeout (in seconds) for the parallel RPCs made in
                          DistributedFileSystem#getFileBlockStorageLocations(). This property is deprecated but is still
                          supported for backward compatibility.
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.file-block-storage-locations.timeout.millis</p>
                      </td>
                      <td rowspan="1">
                        <p>60000</p>
                      </td>
                      <td rowspan="1">
                        <p>Timeout (in milliseconds) for the parallel RPCs made in
                          DistributedFileSystem#getFileBlockStorageLocations(). This property replaces<code>
                            dfs.client.file-block-storage-locations.timeout</code>, and offers a finer level of
                          granularity.
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.read.shortcircuit.skip.checksum</p>
                      </td>
                      <td rowspan="1">
                        <p>false</p>
                      </td>
                      <td rowspan="1">
                        <p>If this configuration parameter is set, short-circuit local reads will skip checksums.
                          This is normally not recommended, but it may be useful for special setups. You might consider
                          using this if you are doing your own checksumming outside of HDFS.
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.read.shortcircuit.streams.cache.size</p>
                      </td>
                      <td rowspan="1">
                        <p>256</p>
                      </td>
                      <td rowspan="1">
                        <p>The DFSClient maintains a cache of recently opened file descriptors. This parameter
                          controls the size of that cache. Setting this higher will use more file descriptors, but
                          potentially provide better performance on workloads involving lots of seeks.
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>dfs.client.read.shortcircuit.streams.cache.expiry.ms</p>
                      </td>
                      <td rowspan="1">
                        <p>300000</p>
                      </td>
                      <td rowspan="1">
                        <p>This controls the minimum amount of time (in milliseconds) file descriptors need to
                          sit in the client cache context before they can be closed for being inactive for too long.
                        </p>
                      </td>
                    </tr>
                  </tbody>
                  
                
              </table></div>
              <p>The XML for these entries:</p>
              <pre><code>&lt;configuration&gt;
                &lt;property&gt;
                &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
                &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.use.legacy.blockreader.local&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.file-block-storage-locations.timeout.millis&lt;/name&gt;
                &lt;value&gt;60000&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.read.shortcircuit.streams.cache.size&lt;/name&gt;
                &lt;value&gt;256&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;dfs.client.read.shortcircuit.streams.cache.expiry.ms&lt;/name&gt;
                &lt;value&gt;300000&lt;/value&gt;
                &lt;/property&gt;
                &lt;/configuration&gt;</code></pre>
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-f76cbf01-7a1e-490d-80db-57d1eaf4bf87">WebHDFS Administrator Guide</h3>
        
          <ul class="number-list">
            Use the following instructions to set up WebHDFS:
            
              <li>
                <p>Set up WebHDFS.

                  Add the following property to the
                  <code>hdfs-site.xml</code>
                  file:
                </p>
                <pre><code>&lt;property&gt;
                  &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
                  &lt;value&gt;true&lt;/value&gt;
                  &lt;/property&gt;</code></pre>
              </li>
              <li>
                <p>[Optional] - If running a secure cluster, follow the steps listed below.</p>
                <ul class="LowercaseAlpha">
                  
                    <li>
                      <p>Create an HTTP service user principal using the command given below:</p>
                      <pre><code>kadmin: addprinc -randkey
                        HTTP/$&lt;Fully_Qualified_Domain_Name&gt;@$&lt;Realm_Name&gt;.COM
                      </code></pre>
                      <p>where:</p>
                      <ul class="Bullet">
                        
                          <li>
                            <p><code>Fully_Qualified_Domain_Name</code>: Host where NameNode is
                              deployed
                            </p>
                          </li>
                          <li>
                            <p><code>Realm_Name</code>: Name of your Kerberos realm
                            </p>
                          </li>
                        
                      </ul>
                    </li>
                    <li>
                      <p>Create keytab files for the HTTP principals.</p>
                      <pre><code>kadmin: xst -norandkey -k /etc/security/spnego.service.keytab HTTP/$&lt;Fully_Qualified_Domain_Name&gt;</code></pre>
                    </li>
                    <li>
                      <p>Verify that the keytab file and the principal are associated with the correct service.
                      </p>
                      <pre><code>klist –k -t /etc/security/spnego.service.keytab</code></pre>
                    </li>
                    <li>
                      <p>Add the following properties to the
                        <code>hdfs-site.xml</code>
                        file.
                      </p>
                      <pre><code>&lt;property&gt;
                        &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt;
                        &lt;value&gt;HTTP/$&lt;Fully_Qualified_Domain_Name&gt;@$&lt;Realm_Name&gt;.COM&lt;/value&gt;
                        &lt;/property&gt;
                        &lt;property&gt;
                        &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt;
                        &lt;value&gt;/etc/security/spnego.service.keytab&lt;/value&gt;
                        &lt;/property&gt;</code></pre>
                      <p>where:</p>
                      <ul class="Bullet">
                        
                          <li>
                            <p><code>Fully_Qualified_Domain_Name</code>: Host where NameNode is
                              deployed
                            </p>
                          </li>
                          <li>
                            <p><code>Realm_Name</code>: Name of your Kerberos realm
                            </p>
                          </li>
                        
                      </ul>
                    </li>
                  
                </ul>
              </li>
              <li>
                <p>Restart the NameNode and DataNode services using the applicable commands in the "Controlling
                  HDP Services Manually" section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.3/HDP_Ref_Gd_v223/index.html">
                    HDP Reference Guide</a>.
                </p>
              </li>
            
          </ul>
        
      
      
        <h3 class="horton-blue bold" id="ref-98d2ae9a-20b4-44d3-8a0f-bdf49c99ff5d"></h3>
      
    
  </div></main>
</div>
<div class="container"><div class="footnotes">
<h2 class="horton-blue border-bottom">Footnotes</h2>
<ol></ol>
</div></div>
<div class="container"><footer><h3 class="horton-blue border-bottom">About Pivotal Data Platform</h3>
      Copyright
      
        <p>This work by
          <a href="http://hortonworks.com">Pivotal, Inc.</a>
          is licensed under a<a href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
            Attribution-ShareAlike 3.0 Unported License</a>.
        </p>
      
      
    <div class="copyright"><p>
    
      <p>The Pivotal Data Platform, powered by Apache Hadoop, is a massively scalable and 100% open source
        platform for storing, processing and analyzing large volumes of data. It is designed to deal with data from many
        sources and formats in a very quick, easy and cost-effective manner. The Pivotal Data Platform consists of
        the essential set of Apache Hadoop projects including MapReduce, Hadoop Distributed File System (HDFS),
        HCatalog, Pig, Hive, HBase, Zookeeper and Ambari. Pivotal is the major contributor of code and patches to
        many of these projects. These projects have been integrated and tested as part of the Pivotal Data Platform
        release process and installation and configuration tools have also been included.
      </p>
      <p>Unlike other providers of platforms built using Apache Hadoop, Pivotal contributes 100% of our code
        back to the Apache Software Foundation. The Pivotal Data Platform is Apache-licensed and completely open
        source. We sell only expert technical support,
        <a href="http://hortonworks.com/hadoop-training/">training</a>
        and partner enablement services.
        <strong>All of our technology is, and will remain, free and open source.
        </strong>
      </p>
      <p>For more information on Pivotal technology, Please visit the
        <a href="http://hortonworks.com/products/hdp/">Pivotal Data Platform</a>
        page. For more information on Pivotal services, please visit either the
        <a href="http://hortonworks.com/hadoop-support/">Support</a>
        or
        <a href="http://hortonworks.com/hadoop-training">Training</a>
        page. Feel free to
        <a href="http://hortonworks.com/about-us/contact-us/">Contact Us</a>
        directly to discuss your specific needs.
      </p>
    
    <span>© Copyright © 2012-2015 Pivotal, Inc. Some rights reserved.</span>
  </p></div></footer></div>
</div></div></body>
</html>
