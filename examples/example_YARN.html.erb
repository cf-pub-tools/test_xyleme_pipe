---
title: YARN Resource Management
---
<!DOCTYPE html><html xmlns:xy="http://xyleme.com/xylink">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>YARN Resource Management</title>
<link href="http://docs.pivotal.io/stylesheets/master.css" rel="stylesheet" type="text/css" media="screen,print">
<link href="../../xyleme.css" rel="stylesheet" type="text/css" media="screen, print">
</head>
<body><div class="viewport"><div class="wrap">
<div class="container">
<header></header><main class="content content-layout" id="js-content" role="main"><a id="top"></a><h1 class="bold horton-blue">YARN Resource Management</h1>
<h2 class="small-title thin">HDP 2.2</h2>
<div class="lessons">
    
      <h2 class="horton-green bold">YARN Resource Management</h2>
      
        <h3 class="horton-blue bold" id="ref-69e4d780-23b6-4a12-85f4-93c04161597e">Capacity Scheduler</h3>
        
          <p>This guide describes how to use the Capacity Scheduler to allocate shared cluster resources among
            users and groups.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <ul class="bullet-list">
              The fundamental unit of scheduling in YARN is the<i>queue</i>. Each queue in the
                Capacity Scheduler has the following properties:
              
              
                <li>
                  <p>A short queue name.</p>
                </li>
                <li>
                  <p>A full queue path name.</p>
                </li>
                <li>
                  <p>A list of associated child-queues and applications.</p>
                </li>
                <li>
                  <p>The guaranteed capacity of the queue.</p>
                </li>
                <li>
                  <p>The maximum capacity of the queue.</p>
                </li>
                <li>
                  <p>A list of active users and their corresponding resource allocation limits.</p>
                </li>
                <li>
                  <p>The state of the queue.</p>
                </li>
                <li>
                  <p>Access control lists (ACLs) governing access to the queue. The following sections will
                    describe how to configure these properties, and how Capacity Scheduler uses these properties to make
                    various scheduling decisions.
                  </p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Enabling Capacity Scheduler</h4>
          
            <p>To enable the Capacity Scheduler, set the following property in the /etc/hadoop/conf/
              yarn-site.xml file on the ResourceManager host:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;
              &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>The settings for the Capacity Scheduler are contained in the
              <code>/etc/hadoop/conf/capacity-scheduler.xml</code>
              file on the ResourceManager host. The Capacity Scheduler reads this file when starting, and also when an
              administrator modifies the capacity- scheduler.xml file and then reloads the settings by running the
              following command:
            </p>
            <pre><code>yarn rmadmin -refreshQueues</code></pre>
            <p>This command can only be run by cluster administrators. Administrator privileges are configured
              with the
              <code>yarn.admin.acl</code>
              property on the ResourceManager.
            </p>
          
        
        
          <h4 class="bold">Setting up Queues</h4>
          
            <p>The fundamental unit of scheduling in YARN is a<i>queue</i>. The<i>
              capacity</i>of each queue specifies the percentage of cluster resources that are available for
              applications submitted to the queue. Queues can be set up in a hierarchy that reflects the database
              structure, resource requirements, and access restrictions required by the various organizations, groups,
              and users that utilize cluster resources.
            </p>
            <p>
              For example, suppose that a company has three organizations: Engineering, Support, and Marketing. The
              Engineering organization has two sub-teams: Development and QA. The Support organization has two
              sub-teams: Training and Services. And finally, the Marketing organization is divided into Sales and
              Advertising. The following image shoes the queue hierarchy for this example:
            </p>
            <div class="figure">
              
                
                  
                  <img src="01-RawContent/zzzzz-LegacyContent/bk_system-admin-guide-20140829-image_3.jpeg" width="50">
                
              
            </div>
            <p>Each child queue is tied to its parent queue with the
              <code>yarn.scheduler.capacity.&lt;queue-path&gt;.queues</code>
              configuration property in the capacity-scheduler.xml file. The top-level "support", "engineering", and
              "marketing" queues would be tied to the "root" queue as follows:
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.queues</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>support,engineering,marketing</code>
            </p>
            <p>
              Example:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;
              &lt;value&gt;support,engineering,marketing&lt;/value&gt;
              &lt;description&gt;The top-level queues below root.&lt;/description&gt;
              &lt;/property&gt;</code></pre>
            <p>Similarly, the children of the "support" queue would be defined as follows:</p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.support.queues</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>training,services</code>
            </p>
            <p>
              Example:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.support.queues&lt;/name&gt;
              &lt;value&gt;training,services&lt;/value&gt;
              &lt;description&gt;child queues under support&lt;/description&gt;
              &lt;/property&gt;</code></pre>
            <p>The children of the "engineering" queue would be defined as follows:
              <strong>
                Property:
              </strong>
              <code>yarn.scheduler.capacity.engineering.queues</code>
              <strong>
                Value:
              </strong>
              <code>development,qa</code>
            </p>
            <p>
              Example:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.engineering.queues&lt;/name&gt;
              &lt;value&gt;development,qa&lt;/value&gt;
              &lt;description&gt;child queues under engineering&lt;/description&gt;
              &lt;/property&gt;</code></pre>
            <p>And the children of the "marketing" queue would be defined as follows:
              <strong>
                Property:
              </strong>
              <code>yarn.scheduler.capacity.marketing.queues</code>
              <strong>
                Value:
              </strong>
              <code>sales,advertising</code>
            </p>
            <p>
              Example:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.marketing.queues&lt;/name&gt;
              &lt;value&gt;sales,advertising&lt;/value&gt;
              &lt;description&gt;child queues under marketing&lt;/description&gt;
              &lt;/property&gt;</code></pre>
          
          
            <h4 class="bold">Hierarchical Queue Characteristics</h4>
            
              <ul class="bullet-list">
                
                  <li>
                    <p>There are two types of queues:<i>parent</i>queues and<i>leaf</i>
                      queues.
                    </p>
                  </li>
                  <li>
                    <p>Parent queues enable the management of resources across organizations and sub-
                      organizations. They can contain more parent queues or leaf queues. They do not themselves accept
                      any application submissions directly.
                    </p>
                  </li>
                  <li>
                    <p>Leaf queues are the queues that live under a parent queue and accept applications. Leaf
                      queues do not have any child queues, and therefore do not have any configuration property that
                      ends with ".queues".
                    </p>
                  </li>
                
              </ul>
              <ul class="bullet-list">
                
                  <li>
                    <p>There is a top-level parent<i>root</i>queue that does not belong to any
                      organization, but instead represents the cluster itself.
                    </p>
                  </li>
                  <li>
                    <p>Using parent and leaf queues, administrators can specify capacity allocations for various
                      organizations and sub-organizations.
                    </p>
                  </li>
                
              </ul>
            
          
          
            <h4 class="bold">Scheduling Among Queues</h4>
            
              <p>Hierarchical queues ensure that guaranteed resources are first shared among the sub- queues of
                an organization before any remaining free resources are shared with queues belonging to other
                organizations. This enables each organization to have control over the utilization of its guaranteed
                resources.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>At each level in the hierarchy, every parent queue keeps the list of its child queues in a
                      sorted manner based on demand. The sorting of the queues is determined by the currently used
                      fraction of each queue’s capacity (or the full-path queue names if the reserved capacity of any
                      two queues is equal).
                    </p>
                  </li>
                  <li>
                    <p>The root queue understands how the cluster capacity needs to be distributed among the
                      first level of parent queues and invokes scheduling on each of its child queues.
                    </p>
                  </li>
                  <li>
                    <p>Every parent queue applies its capacity constraints to all of its child queues.</p>
                  </li>
                  <li>
                    <p>Leaf queues hold the list of active applications (potentially from multiple users) and
                      schedules resources in a FIFO (first-in, first-out) manner, while at the same time adhering to
                      capacity limits specified for individual users.
                    </p>
                  </li>
                
              </ul>
            
          
        
        
          <h4 class="bold">Controlling Access to Queues with ACLs</h4>
          
            <p>Access-control lists (ACLs) can be used to restrict user and administrator access to queues.
              Application submission can really only happen at the leaf queue level, but an ACL restriction set on a
              parent queue will be applied to all of its descendant queues.
            </p>
            <p>
              In the Capacity Scheduler, ACLs are configured by granting queue access to a list of users and groups with
              the
              <code>acl_submit_applications</code>
              property. The format of the list is "user1,user2 group1,group2" -- a comma-separated list of users,
              followed by a space, followed by a comma-separated list of groups.
            </p>
            <p>
              The value of
              <code>acl_submit_applications</code>
              can also be set to "*" (asterisk) to allow access to all users and groups, or can be set to " " (space
              character) to block access to all users and groups.
            </p>
            <p>As mentioned previously, ACL settings on a parent queue are applied to all of its descendant
              queues. Therefore, if the parent queue uses the "*" (asterisk) value (or is not specified) to allow access
              to all users and groups, its child queues cannot restrict access. Similarly, before you can restrict
              access to a child queue, you must first set the parent queue to " " (space character) to block access to
              all users and groups.
            </p>
            <p>
              For example, the following properties would set the root acl_submit_applications value to " " (space
              character) to block access to all users and groups, and also restrict access to its child "support" queue
              to the users "sherlock" and "pacioli" and the members of the "cfo-group" group:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.root.acl_submit_applications&lt;/name&gt;
              &lt;value&gt; &lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.root.support.acl_submit_applications&lt;/name&gt;
              &lt;value&gt;sherlock,pacioli cfo-group&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>A separate ACL can be used to control the administration of queues at various levels. Queue
              administrators can submit applications to the queue, kill applications in the queue, and obtain
              information about any application in the queue (whereas normal users are restricted from viewing all of
              the details of other users' applications).
            </p>
            <p>
              Administrator ACLs are configured with the
              <code>acl_administer_queue</code>
              property. ACLs for this property are inherited from the parent queue if not specified. For example, the
              following properties would set the root
              <code>acl_administer_queue</code>
              value to " " (space character) to block access to all users and groups, and also grant administrator
              access to its child "support" queue to the users "sherlock" and "pacioli" and the members of the "cfo-
              group" group:
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.root.acl_administer_queue&lt;/name&gt;
              &lt;value&gt; &lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;yarn.scheduler.capacity.root.support.acl_administer_queue&lt;/name&gt;
              &lt;value&gt;sherlock,pacioli cfo-group&lt;/value&gt;
              &lt;/property&gt;</code></pre>
          
        
        
          <h4 class="bold">Managing Cluster Capacity with Queues</h4>
          
            <p>The Capacity Scheduler is designed to allow organizations to share compute clusters using the very
              familiar notion of FIFO (first-in, first-out) queues. YARN does not assign entire nodes to queues. Queues
              own a fraction of the capacity of the cluster, and this specified queue capacity can be fulfilled from any
              number of nodes in a dynamic fashion.
            </p>
            <p>
              Scheduling is the process of matching resource requirements -- of multiple applications from various
              users, and submitted to different queues at multiple levels in the queue hierarchy -- with the free
              capacity available on the nodes in the cluster. Because total cluster capacity can vary, capacity
              configuration values are expressed as percents.
            </p>
            <p>
              The
              <code>capacity</code>
              property can be used by administrators to allocate a percentage of cluster capacity to a queue. The
              following properties would divide the cluster resources between the Engineering, Support, and Marketing
              organizations in a 6:1:3 ratio (60%, 10%, and 30%).
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.engineering.capacity</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>60</code>
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.support.capacity</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>10</code>
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.marketing.capacity</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>30</code>
            </p>
            <p>
              Now suppose that the Engineering group decides to split its capacity between the Development and QA
              sub-teams in a 1:4 ratio. That would be implemented by setting the following properties:
            </p>
          
          
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.engineering.development.capacity</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>20</code>
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.engineering.qa.capacity</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>80</code>
            </p>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>The sum of capacities at any level in the hierarchy must equal 100%. Also, the capacity of an
                  individual queue at any level in the hierarchy must be 1% or more (you cannot set a capacity to a
                  value of 0).
                </p>
              </div>
            </aside>
            <p></p>
            <p>The following image illustrates this cluster capacity configuration:</p>
            <div class="figure">
              
                
                  
                  <img src="System%20Admin%20Guides/capacity_scheduler_queue_percents.png" width="50">
                
              
            </div>
          
          
            <h4 class="bold">Resource Distribution Workflow</h4>
            
              <ul class="bullet-list">
                During scheduling, queues at any level in the hierarchy are sorted in the order of their
                  current used capacity, and available resources are distributed among them starting with queues that
                  are currently the most under-served. With respect to capacities alone, the resource scheduling has the
                  following workflow:
                
                
                  <li>
                    <p>The more under-served a queue is, the higher the priority it receives during resource
                      allocation. The most under-served queue is the queue with the least ratio of used capacity as
                      compared to the total cluster capacity.
                    </p>
                    <ul class="Bullet">
                      
                        <li>
                          <p>The used capacity of any parent queue is defined as the aggregate sum of used
                            capacity of all of its descendant queues, recursively.
                          </p>
                        </li>
                        <li>
                          <p>The used capacity of a leaf queue is the amount of resources used by the allocated
                            Containers of all of the applications running in that queue.
                          </p>
                        </li>
                      
                    </ul>
                  </li>
                  <li>
                    <p>Once it is decided to give a parent queue the currently available free resources, further
                      scheduling is done recursively to determine which child queue gets to use the resources -- based
                      on the previously described concept of used capacities.
                    </p>
                  </li>
                  <li>
                    <p>Further scheduling happens inside each leaf queue to allocate resources to applications in
                      a FIFO order.
                    </p>
                    <ul class="Bullet">
                      
                        <li>
                          <p>This is also dependent on locality, user level limits, and application limits.
                          </p>
                        </li>
                        <li>
                          <p>Once an application within a leaf queue is chosen, scheduling also happens within
                            the application. Applications may have different priorities of resource requests.
                          </p>
                        </li>
                      
                    </ul>
                  </li>
                  <li>
                    <p>To ensure elasticity, capacity that is configured but not utilized by any queue due to
                      lack of demand is automatically assigned to the queues that are in need of resources.
                    </p>
                  </li>
                
              </ul>
            
          
          
            <h4 class="bold">Resource Distribution Workflow Example</h4>
            
              <p>Suppose our cluster has 100 nodes, each with 10 GB of memory allocated for YARN Containers, for
                a total cluster capacity of 1000 GB (1 TB). According to the previously described configuration, the
                Engineering organization is assigned 60% of the cluster capacity, i.e., an absolute capacity of 600 GB.
                Similarly, the Support organization is assigned 100 GB, and the Marketing organization gets 300 GB.
              </p>
              <p>Under the Engineering organization, capacity is distributed between the Development team and the
                QA team in a in a 1:4 ratio. So Development gets 120 GB, and 480 GB is assigned to QA.
              </p>
              <ul class="bullet-list">
                Now consider the following timeline of events:
                
                  <li>
                    <p>Initially, the entire "engineering" queue is free with no applications running, while the
                      "support" and "marketing" queues are utilizing their full capacities.
                    </p>
                  </li>
                  <li>
                    <p>Users Sid and Hitesh first submit applications to the "development" leaf queue. Their
                      applications are elastic and can run with either all of the resources available in the cluster, or
                      with a subset of cluster resources (depending upon the state of the resource-usage).
                    </p>
                    <ul class="Bullet">
                      
                        <li>
                          <p>Even though the "development" queue is allocated 120 GB, Sid and Hitesh are each
                            allowed to occupy 120 GB, for a total of 240 GB.
                          </p>
                        </li>
                        <li>
                          <p>This can happen despite the fact that the "development" queue is configured to be
                            run with a capacity of 120 GB. Capacity Scheduler allows elastic sharing of cluster
                            resources for better utilization of available cluster resources. Since there are no other
                            users in the "engineering" queue, Sid and Hitesh are allowed to use the available free
                            resources.
                          </p>
                        </li>
                      
                    </ul>
                  </li>
                  <li>
                    <p>Next, users Jian, Zhijie and Xuan submit more applications to the "development" leaf
                      queue. Even though each is restricted to 120 GB, the overall used capacity in the queue becomes
                      600 GB -- essentially taking over all of the resources allocated to the "qa" leaf queue.
                    </p>
                  </li>
                  <li>
                    <p>User Gupta now submits an application to the "qa" queue. With no free resources available
                      in the cluster, his application must wait.
                    </p>
                    <ul class="Bullet">
                      
                        <li>
                          <p>Given that the "development" queue is utilizing all of the available cluster
                            resources, Gupta may or may not be able to immediately get back the guaranteed capacity of
                            his "qa" queue -- depending upon whether or not preemption is enabled.
                          </p>
                        </li>
                      
                    </ul>
                  </li>
                  <li>
                    <p>As the applications of Sid, Hitesh, Jian, Zhijie, and Xuan finish running and resources
                      become available, the newly available Containers will be allocated to Gupta’s application.
                    </p>
                  </li>
                
              </ul>
              <p>This will continue until the cluster stabilizes at the intended 1:4 resource usage ratio for the
                "development" and "qa" queues.
              </p>
              <p>From this example, you can see that it is possible for abusive users to submit applications
                continuously, and thereby lock out other queues from resource allocation until Containers finish running
                or get preempted. To avoid this scenario, Capacity Scheduler supports limits on the elastic growth of
                any queue. For example, to restrict the "development" queue from monopolizing the "engineering" queue
                capacity, an administrator can set a the maximum- capacity property:
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.scheduler.capacity.root.engineering.development.maximum- capacity</code>
              </p>
              <p>
                <strong>Value:</strong>40
              </p>
              <p>
                Once this is set, users of the "development" queue can still go beyond their capacity of 120 GB, but
                they will not be allocated any more than 40% of the "engineering" parent queue's capacity (i.e., 40% of
                600 GB = 240 GB).
              </p>
              <p>
                The capacity and maximum-capacity properties can be used to control sharing and elasticity across the
                organizations and sub-organizations utilizing a YARN cluster. Administrators should balance these
                properties to avoid strict limits that result in a loss of utilization, and to avoid excessive
                cross-organization sharing.
              </p>
              <p>
                Capacity and maximum capacity settings can be dynamically changed at run-time using<code>yarn
                rmadmin -refreshQueues</code>.
              </p>
            
          
        
        
          <h4 class="bold">Setting User Limits</h4>
          
            <p>The
              <code>minimum-user-limit-percent</code>
              property can be used to set the minimum percentage of resources allocated to each leaf queue user. For
              example, to enable equal sharing of the "services" leaf queue capacity among five users, you would set the
              minimum- user-limit property to 20%:
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.support.services.minimum-user- limit-percent</code>
            </p>
            <p>
              <strong>Value:</strong>20
            </p>
            <p>
              This setting determines the minimum limit that any user’s share of the queue capacity can shrink to.
              Irrespective of this limit, any user can come into the queue and take more than his or her allocated share
              if there are idle resources available.
            </p>
            <p>
              The following table shows how the queue resources are adjusted as users submit jobs to a queue with a
              <code>minimum-user-limit-percent</code>
              value of 20%:
            </p>
            <div class="figure">
              
                
                  
                  <img src="01-RawContent/zzzzz-LegacyContent/bk_system-admin-guide-20140829-image_5.png" width="50">
                
              
            </div>
            <ul class="bullet-list">
              
                <li>
                  <p>The Capacity Scheduler also manages resources for decreasing numbers of users. As users’
                    applications finish running, other existing users with outstanding requirements begin to reclaim
                    that share.
                  </p>
                </li>
                <li>
                  <p>Note that despite this sharing among users, the FIFO application scheduling order of
                    Capacity Scheduler does not change. This guarantees that users cannot monopolize queues by
                    submitting new applications continuously. Applications (and thus the corresponding users) that are
                    submitted first always get a higher priority than applications that are submitted later.
                  </p>
                </li>
              
            </ul>
            <p>Capacity Scheduler’s leaf queues can also use the
              <code>user-limit-factor</code>
              property to control user resource allocations. This property denotes the fraction of queue capacity that
              any single user can consume up to a maximum value, regardless of whether or not there are idle resources
              in the cluster.
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.support.user-limit-factor

              </code>
              <strong>Value:</strong>
              <code>1

              </code>The default value of "1" means that any single user in the queue can at maximum only occupy
              the queue’s configured capacity. This prevents users in a single queue from monopolizing resources across
              all queues in a cluster. Setting the value to "2" would restrict the queue's users to twice the queue’s
              configured capacity. Setting it to a value of 0.5 would restrict any user from using resources beyond half
              of the queue capacity.

              These settings can also be dynamically changed at run-time using<code>yarn rmadmin -
              refreshQueues</code>.
            </p>
          
        
        
          <h4 class="bold">Application Reservations</h4>
          
            <p>The Capacity Scheduler is responsible for matching free resources in the cluster with the resource
              requirements of an application. Many times, a scheduling cycle occurs such that even though there are free
              resources on a node, they are not sized large enough to satisfy the application waiting for a resource at
              the head of the queue. This typically happens with high-memory applications whose resource demand for
              Containers is much larger than the typical application running in the cluster. This mismatch can lead to
              starving these resource- intensive applications.
            </p>
            <ul class="bullet-list">
              The Capacity Scheduler<i>reservations</i>feature addresses this issue:
              
              
                <li>
                  <p>When a node reports in with a finished Container, the Capacity Scheduler selects an
                    appropriate queue to utilized the newly available resources based on capacity and maximum capacity
                    settings.
                  </p>
                </li>
                <li>
                  <p>Within that selected queue, the Capacity Scheduler looks at the applications in a FIFO order
                    along with the user limits. Once a needy application is found, the Capacity Scheduler tries to see
                    if the requirements of that application can be met by the node’s free capacity.
                  </p>
                </li>
                <li>
                  <p>If there is a size mismatch, the Capacity Scheduler immediately creates a reservation on the
                    node for the application’s required Container.
                  </p>
                </li>
                <li>
                  <p>Once a reservation is made for an application on a node, those resources are not used by the
                    Capacity Scheduler for any other queue, application, or Container until the application reservation
                    is fulfilled.
                  </p>
                </li>
                <li>
                  <p>The node on which a reservation is made reports back when enough Containers finish running
                    such that the total free capacity on the node now matches the reservation size. When that happens,
                    the Capacity Scheduler marks the reservation as fulfilled, removes it, and allocates a Container on
                    the node.
                  </p>
                </li>
                <li>
                  <p>In some cases another node fulfills the resources required by the application, so the
                    application no longer needs the reserved capacity on the first node. In this situation, the
                    reservation is simply cancelled.
                  </p>
                </li>
              
            </ul>
            <p>To prevent the number of reservations from growing in an unbounded manner, and to avoid any
              potential scheduling deadlocks, the Capacity Scheduler maintains only one active reservation at a time on
              each node.
            </p>
          
        
        
          <h4 class="bold">Starting and Stopping Queues</h4>
          
            <p>Queues in YARN can be in two states: RUNNING or STOPPED. A RUNNING state indicates that a queue
              can accept application submissions, and a STOPPED queue does not accept application submissions. The
              default state of any configured queue is RUNNING.
            </p>
            <p>
              In Capacity Scheduler, parent queues, leaf queues, and the root queue can all be stopped. For an
              application to be accepted at any leaf queue, all the queues in the hierarchy all the way up to the root
              queue must be running. This means that if a parent queue is stopped, all of the descendant queues in that
              hierarchy are inactive, even if their own state is RUNNING.
            </p>
            <p>
              The following example sets the value of the state property of the "support" queue to RUNNING:
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.root.support.state</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>RUNNING</code>
            </p>
            <p>
              Administrators can use the ability to stop and drain applications in a queue for a number of reasons, such
              as when decommissioning a queue and migrating its users to other queues. Administrators can stop queues at
              run-time, so that while current applications run to completion, no new applications are admitted. Existing
              applications can continue until they finish running, and thus the queue can be drained gracefully without
              any end-user impact.
            </p>
            <p>
              Administrators can also restart the stopped queues by modifying the state configuration property and then
              refreshing the queue using
              <code>yarn rmadmin -refreshQueues</code>
              as previously described.
            </p>
          
        
        
          <h4 class="bold">Setting Application Limits</h4>
          
            <p>To avoid system-thrash due to an unmanageable load -- caused either by malicious users, or by
              accident -- the Capacity Scheduler enables you to place a static, configurable limit on the total number
              of concurrently active (both running and pending) applications at any one time. The maximum-applications
              configuration property is used to set this limit, with a default value of 10,000:
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.maximum-applications</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>10000</code>
            </p>
            <p>
              The limit for running applications in any specific queue is a fraction of this total limit, proportional
              to its capacity. This is a hard limit, which means that once this limit is reached for a queue, any new
              applications to that queue will be rejected, and clients will have to wait and retry later. This limit can
              be explicitly overridden on a per-queue basis with the following configuration property:
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-applications</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>absolute-capacity * yarn.scheduler.capacity.maximum- applications</code>
            </p>
            <p>
              There is another resource limit that can be used to set a maximum percentage of cluster resources
              allocated specifically to ApplicationMasters. The
              <code>maximum-am-resource-percent</code>
              property has a default value of 10%, and exists to avoid cross-application deadlocks where significant
              resources in the cluster are occupied entirely by the Containers running ApplicationMasters. This property
              also indirectly controls the number of concurrent running applications in the cluster, with each queue
              limited to a number of running applications proportional to its capacity.
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.maximum-am-resource-percent</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>0.1</code>
            </p>
            <p>
              As with maximum-applications, this limit can also be overridden on a per-queue basis:
            </p>
            <p>
              <strong>Property:</strong>
              <code>yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-am-resource-percent</code>
            </p>
            <p>
              <strong>Value:</strong>
              <code>0.1</code>
            </p>
            <p>
              All of these limits ensure that no single application, user, or queue can cause catastrophic failure, or
              monopolize the cluster and cause excessive degradation of cluster performance.
            </p>
          
        
        
          <h4 class="bold">Preemption</h4>
          
            <p>As mentioned previously, a scenario can occur in which a queue has a guaranteed level of cluster
              resources, but must wait to run applications because other queues are utilizing all of the available
              resources. If Preemption is enabled, higher-priority applications do not have to wait because lower
              priority applications have taken up the available capacity. With Preemption enabled, under-served queues
              can begin to claim their allocated cluster resources almost immediately, without having to wait for other
              queues' applications to finish running.
            </p>
          
          
            <h4 class="bold">Preemption Workflow</h4>
            
              <p>Preemption is governed by a set of capacity monitor policies, which must be enabled by setting
                the
                <code>yarn.resourcemanager.scheduler.monitor.enable</code>
                property to "true". These capacity monitor policies apply Preemption in configurable intervals based on
                defined capacity allocations, and in as graceful a manner as possible. Containers are only killed as a
                last resort. The following image demonstrates the Preemption workflow:
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/zzzzz-LegacyContent/bk_system-admin-guide-20140829-image_6.jpeg" width="50">
                  
                
              </div>
            
          
          
            <h4 class="bold">Preemption Configuration</h4>
            
              <p>The following properties in the
                <code>/etc/hadoop/conf/yarn-site.xml</code>
                file on the ResourceManager host are used to enable and configure Preemption.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>
                      <strong>Property:</strong>
                      <code>yarn.resourcemanager.scheduler.monitor.enable

                      </code>
                      <strong>Value:</strong>
                      <code>true

                      </code>
                      <strong>Description:</strong>
                      Setting this property to "true" enables Preemption. It enables a set of periodic monitors that
                      affect the Capacity Scheduler. This default value for this property is "false" (disabled).
                    </p>
                  </li>
                  <li>
                    <p>
                      <strong>Property:</strong>
                      <code>yarn.resourcemanager.scheduler.monitor.policies

                      </code>
                      <strong>Value:</strong>
                      <code>
                        org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy

                      </code>
                      <strong>Description:</strong>The list of SchedulingEditPolicy classes that interact with the
                      scheduler. The only policy currently available for preemption is the
                      “ProportionalCapacityPreemptionPolicy”.
                    </p>
                  </li>
                  <li>
                    <p>
                      <strong>Property:</strong>
                      <code>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval</code>

                      <strong>Value:</strong>
                      <code>3000</code>

                      <strong>Description:</strong>
                      <code>The time in milliseconds between invocations of this policy. Setting this value to a
                        longer time interval will cause the Capacity Monitor to run less frequently.
                      </code>
                    </p>
                  </li>
                  <li>
                    <p>
                      <strong>Property:</strong>
                      <code>yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill</code>

                      <strong>Value:</strong>
                      <code>15000</code>

                      <strong>Description:</strong>
                      The time in milliseconds between requesting a preemption from an application and killing the
                      container. Setting this to a higher value will give applications more time to respond to
                      preemption requests and gracefully release Containers.
                    </p>
                  </li>
                  <li>
                    <p>
                      <strong>Property:</strong>
                      <code>yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round
                      </code>

                      <strong>Value:</strong>
                      <code>0.1</code>

                      <strong>Description:</strong>The maximum percentage of resources preempted in a single round. You can
                      use this value to restrict the pace at which Containers are reclaimed from the cluster. After
                      computing the total desired preemption, the policy scales it back to this limit.
                    </p>
                  </li>
                
              </ul>
            
          
        
        
          <h4 class="bold">Scheduler User Interface</h4>
          
            <p>You can use the Scheduler page in the Hadoop User Interface (UI) page to view the status and
              settings of Capacity Scheduler queues. The following image show the Hadoop UI Scheduler page (http://&lt;hostname&gt;:8088/cluster/scheduler)
              with the "support" queue selected:
            </p>
            <div class="figure">
              
                
                  
                  <img src="01-RawContent/zzzzz-LegacyContent/bk_system-admin-guide-20140829-image_7.png" width="50">
                
              
            </div>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-f86086a9-e34f-4cc2-b590-e41c40253a1c">CGroups</h3>
        
          <p>You can use CGroups to isolate CPU-heavy processes in a Hadoop cluster. If you are using CPU
            scheduling, you should also use CGroups to constrain and manage CPU processes. If you are not using CPU
            scheduling, do not enable CGroups.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>When you enable CPU scheduling, queues are still used to allocate cluster resources, but both CPU
              and memory are taken into consideration using a scheduler that utilizes Dominant Resource Fairness (DRF).
              In the DRF model, resource allocation takes into account the dominant resource required by a process.
              CPU-heavy processes (such as Storm-on-YARN) receive more CPU and less memory. Memory-heavy processes (such
              as MapReduce) receive more memory and less CPU. The DRF scheduler is designed to fairly distribute memory
              and CPU resources among different types of processes in a mixed- workload cluster.

              CGroups compliments CPU scheduling by providing CPU resource isolation. It enables you to set limits on
              the amount of CPU resources granted to individual YARN containers, and also lets you set a limit on the
              total amount of CPU resources used by YARN processes.

              CGroups represents one aspect of YARN resource management capabilities that includes CPU scheduling, node
              labels, archival storage, and memory as storage. If CPU scheduling is used, CGroups should be used along
              with it to constrain and manage CPU processes.
            </p>
          
        
        
          <h4 class="bold">Enabling CGroups</h4>
          
            <p>CGroups is a Linux kernel feature. Currently HDP supports CGroups on RHEL6 only. At this time
              there is no CGroups equivalent for Windows. CGroups are not enabled by default on HDP.
            </p>
          
          
            <span class="title"><strong>Enable CGroups</strong></span>
            
              <p>On Centos 6, CGroups are not set up by default. Run the following commands to set up CGroups:
              </p>
              <pre><code>yum install libcgroup
                sudo mkdir /cgroup
                mount -t cgroup -o cpu cpu /cgroup
              </code></pre>
              <p>Set the following properties in the
                <code>/etc/hadoop/conf/yarn-site.xml</code>
                file on the ResourceManager and NodeManager hosts:
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.container-executor.class</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.container-executor.class&lt;/name&gt;
                &lt;value&gt;org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.container-executor.group</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>hadoop</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.container-executor.group&lt;/name&gt;
                &lt;value&gt;hadoop&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.container-executor.resources-handler.class</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.container-executor.resources-handler.class&lt;/name&gt;
                &lt;value&gt;org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.container-executor.cgroups.hierarchy</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>/yarn</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.container-executor.cgroups.hierarchy&lt;/name&gt;
                &lt;value&gt;/yarn&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.container-executor.cgroups.mount</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>true</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.container-executor.cgroups.mount&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.linux-container-executor.cgroups.mount-path</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>/cgroup</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.linux-container-executor.cgroups.mount-path&lt;/name&gt;
                &lt;value&gt;/cgroup&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
            
              <span class="title"><strong>Set the Percentage of CPU used by YARN</strong></span>
              
                <p>Set the percentage of CPU that can be allocated for YARN containers. In most cases, the
                  default value of 100% should be used. If you have another process that needs to run on a node that
                  also requires CPU resources, you can lower the percentage of CPU allocated to YARN to free up
                  resources for the other process.
                </p>
                <p>
                  <strong>Property:</strong>
                  <code>yarn.nodemanager.resource.percentage-physical-cpu-limit</code>
                </p>
                <p>
                  <strong>Value:</strong>
                  <code>100</code>
                </p>
                <p>Example:</p>
                <pre><code>&lt;property&gt;
                  &lt;name&gt;yarn.nodemanager.resource.percentage-physical-cpu-limit&lt;/name&gt;
                  &lt;value&gt;100&lt;/value&gt;
                  &lt;/property&gt;</code></pre>
              
            
            
              <span class="title"><strong>Set Flexible or Strict CPU limits</strong></span>
              
                <p>CPU jobs are constrained with CPU scheduling and CGroups enabled, but by default these are
                  flexible limits. If spare CPU cycles are available, containers are allowed to exceed the CPU limits
                  set for them. With flexible limits, the amount of CPU resources available for containers to use can
                  vary based on cluster usage -- the amount of CPU available in the cluster at any given time.

                  You can use CGroups to set strict limits on CPU usage. When strict limits are enabled, each process
                  receives only the amount of CPU resources it requests. With strict limits, a CPU process will receive
                  the same amount of cluster resources every time it runs.

                  Strict limits are not enabled (set to<code>false</code>) by default. To enable strict CPU
                  limits, set the following property to<code>true</code>.
                </p>
                <p>
                  <strong>Property:</strong>
                  <code>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</code>
                </p>
                <p>
                  <strong>Value:</strong>
                  <code>true</code>
                </p>
                <p>Example:</p>
                <pre><code>&lt;property&gt;
                  &lt;name&gt;yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage&lt;/name&gt;
                  &lt;value&gt;true&lt;/value&gt;
                  &lt;/property&gt;</code></pre>
                <aside class="custom-note">
                  <div class="icon"><img src="Icons/Note.png" width="50"></div>
                  <div class="simple-block">
                    <p>Irrespective of whether this property is
                      <code>true</code>
                      or<code>false</code>, at no point will total container CPU usage exceed the limit set
                      in<code>yarn.nodemanager.resource.percentage-physical-cpu-limit</code>.
                    </p>
                  </div>
                </aside>
              
            
          
        

        
          <h4 class="bold">Using CGroups</h4>
          
            <p>Strict CGroup CPU limits can be used to constrain CPU processes in mixed workload clusters.

              One example of a mixed workload is a cluster that runs both MapReduce and Storm-on-YARN. MapReduce is not
              CPU-constrained (MapReduce containers do not ask for much CPU). Storm-on-YARN is CPU-constrained: its
              containers ask for more CPU than memory. As you start adding Storm jobs along with MapReduce jobs, the DRF
              scheduler attempts to balance memory and CPU resources, but as more CPU-intensive Storm jobs are added,
              they may begin to take up the majority of the cluster CPU resources.

              You can use CGroups along with CPU scheduling to help manage mixed workloads. CGroups provide isolation
              for CPU-heavy processes such as Storm-on-YARN, thereby enabling you to predictably plan and constrain the
              CPU-intensive Storm containers.

              When you enable strict CGroup CPU limits, each resource gets only what it asks for, even if there is extra
              CPU available. This would be useful for scenarios involving charge-backs or strict SLA enforcement, where
              you always need to know exactly what percentage or CPU is being used.

              Also, enabling strict CPU limits would make job performance predictable, whereas without setting strict
              limits a CPU-intensive job would run faster when the cluster was not under heavy use, but slower when more
              jobs were running in the cluster. Strict CPU limits would therefore also be useful for benchmarking.

              You could also use node labels in conjunction with CGroups and CPU scheduling to restrict Storm-on-YARN
              jobs to a subset of cluster nodes.

              If you are using CGroups and want more information on CPU performance, you can review the statistics
              available in the /cgroup/cpu/yarn/cpu.stat file.
            </p>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-f41ba6f5-532b-44c8-9b9b-94a0ac2efb30">CPU Scheduling</h3>
        
          <p>This guide describes how to allocate shared CPU and memory resources among users and groups in a
            Hadoop cluster.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>As discussed in the Capacity Scheduler guide, the fundamental unit of scheduling in YARN is the
              <i>queue</i>. The<i>capacity</i>of each queue specifies the percentage of cluster
              resources that are available for applications submitted to the queue. Queues can be set up in a hierarchy
              that reflects the database structure, resource requirements, and access restrictions required by the
              various organizations, groups, and users that utilize cluster resources. When the default resource
              calculator (<code>DefaultResourceCalculator</code>) is used, resources are allocated based on
              memory alone.
            </p>
          
          
            <p>CPU scheduling is enabled by using the Dominant Resource Calculator (<code>
              DominantResourceCalculator</code>) rather than the default resource calculator. The Dominant
              Resource Calculator is based on the Dominant Resource Fairness (DRF) model of resource allocation.
            </p>
          
          
            <p>With the Dominant Resource Calculator, queues are still used to allocate cluster resources, but
              both CPU and memory are taken into consideration. In the DRF model, resource allocation takes into account
              the dominant resource required by a process. The Dominant Resource Calculator schedules both CPU-heavy and
              memory-heavy processes on the same node. CPU-heavy processes (such as Storm-on-YARN) receive more CPU and
              less memory. Memory-heavy processes (such as MapReduce) receive more memory and less CPU. The DRF
              scheduler is designed to fairly distribute memory and CPU resources among different types of processes in
              a mixed-workload cluster.
            </p>
          
          
            <p>CPU scheduling represents one aspect of YARN resource management capabilities that includes
              CGroups, node labels, archival storage, and memory as storage. CGroups should be used with CPU scheduling
              to constrain and manage CPU processes.
            </p>
          
          
            <p>CPU scheduling is only recommended for Linux. Currently there is no isolation mechanism (CGroups
              equivalent) for Windows, so do not enable CPU scheduling on Windows.
            </p>
          
        
        
          <h4 class="bold">Enabling CPU Scheduling</h4>
          
            <span class="title"><strong>Enable CPU Scheduling in
              <code>capacity-scheduler.xml</code>
            </strong></span>
            
              <p>CPU scheduling is not enabled by default. To enable the CPU Scheduling, set the following
                property in the
                <code>/etc/hadoop/conf/capacity-scheduler.xml</code>
                file on the ResourceManager and NodeManager hosts:
              </p>
              <p>Replace the
                <code>DefaultResourceCalculator</code>
                with the<code>DominantResourceCalculator</code>.
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.scheduler.capacity.resource-calculator</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</code>
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;
                &lt;!-- &lt;value&gt;org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator&lt;/value&gt; --&gt;
                &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
          
            <span class="title"><strong>Set Vcores in
              <code>yarn-site.xml</code>
            </strong></span>
            
              <p>In YARN, vcores (virtual cores) are used to normalize CPU resources across the cluster. The
                <code>yarn.nodemanager.resource.cpu-vcores</code>
                value sets the number of CPU cores that can be allocated for containers.
              </p>
              <p>The number of vcores should be set to match the number of physical CPU cores on the NodeManager
                hosts. Set the following property in the
                <code>/etc/hadoop/conf/yarn-site.xml</code>
                file on the ResourceManager and NodeManager hosts:
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.nodemanager.resource.cpu-vcores</code>
              </p>
              <p>
                <strong>Value:</strong>
                <code>&lt;number_of_physical_cores&gt;</code>
              </p>
              <p>Example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
                &lt;value&gt;16&lt;/value&gt;
                &lt;/property&gt;</code></pre>
              <p>It is also recommended that you enable CGroups along with CPU scheduling. CGroups are used as
                the isolation mechanism for CPU processes. With CGroups strict enforcement turned on, each CPU process
                gets only the resources it asks for. Without CGroups turned on, the DRF scheduler attempts to balance
                the load, but unpredictable behavior may occur.

                Currently there is no isolation mechanism (CGroups equivalent) for Windows, so do not enable CPU
                scheduling on Windows.
              </p>
            
          
        
        
          <h4 class="bold">Using CPU Scheduling</h4>
          
            <span class="title"><strong>MapReduce Jobs Only</strong></span>
            
              <p>If you primarily run MapReduce jobs on your cluster, you probably will not see much of a change
                in performance if you enable CPU scheduling. The dominant resource for MapReduce is memory, so the DRF
                scheduler continues to balance MapReduce jobs out similar to the default resource calculator. In the
                single resource case, the DRF reduces to max-min fairness for that resource.
              </p>
            
          
          
            <span class="title"><strong>Mixed Workloads</strong></span>
            
              <p>One example of a mixed workload would be a cluster that runs both MapReduce and Storm-on-YARN.
                MapReduce is not CPU-constrained (MapReduce containers do not ask for much CPU). Storm on YARN is
                CPU-constrained: its containers ask for more CPU than memory. As you start adding a Storm jobs along
                with MapReduce jobs, the DRF scheduler does its best to balance memory and CPU resources, but you might
                start to see some degradation in performance. If you were to then add more CPU-intensive Storm jobs,
                individual jobs will start to take longer to run as the cluster CPU resources become consumed.
              </p>
              <p>CGroups can be used along with CPU scheduling to help manage mixed workloads. CGroups provides
                isolation for CPU-intensive processes such as Storm-on-YARN, thereby enabling you to predictably plan
                and constrain the CPU-intensive Storm containers.
              </p>
              <p>You could also use node labels in conjunction with CPU scheduling and CGroups to restrict
                Storm-on-YARN jobs to a subset of cluster nodes.
              </p>
            
          
        
        
          <h4 class="bold">Dominant Resource Fairness (DRF)</h4>
          
            <p>The Dominant Resource Calculator (DominantResourceCalculator) is used to enable CPU scheduling.
              The Dominant Resource Calculator is based on the Dominant Resource Fairness (DRF) model of resource
              allocation.

              DRF uses the concept of the
              <i>dominant resource</i>
              to compare multi-dimensional resources. The idea is that in a multi-resource environment, resource
              allocation should be determined by the dominant share of an entity (user or queue), which is the maximum
              share that the entity has been allocated of any resource (memory or CPU). Essentially, the DRF seeks to
              maximize the minimum dominant share across all entities.

              For example, if user A runs CPU-heavy tasks and user B runs memory-heavy tasks, the DRF attempts to
              equalize CPU share of user A with the memory share of user B. In this case, the DRF would allocate more
              CPU and less memory to the tasks run by user A, and allocate less CPU and more memory to the tasks run by
              user B. In the single resource case -- where all jobs are requesting the same resources -- the DRF reduces
              to max-min fairness for that resource.
            </p>
            <p>For more information about DRF, see<a href="http://www.cs.berkeley.edu/%7Ematei/papers/2011/nsdi_drf.pdf">Dominant Resource Fairness:
              Fair Allocation of Mulitple Resources</a>.
              
            </p>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-fef78bca-0fbd-482a-814f-d449c8ca13de">Log Aggregation for Long-Running Applications</h3>
        
          <p>This guide describes how to use log aggregation to collect logs for long-running YARN
            applications.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>In Hadoop logs are collected for an application only when it finishes running. This works well for
              applications that only run for a short time, but is not ideal for long-running applications such as Storm
              and HBase running on YARN using Slider.

              If an application runs for days or weeks, it is useful to collect log information while the application is
              running. This log information can be used to:
            </p>
            <ul class="bullet-list">
              
                <li>
                  <p>Debug hardware and software issues</p>
                </li>
                <li>
                  <p>Review and optimize application performance</p>
                </li>
                <li>
                  <p>Review application resource utilization</p>
                </li>
              
            </ul>
            <p>A long-running application may generate a large amount of log information. The application can use
              log rotation to prevent the log file from becoming excessively large. The YARN NodeManager will
              periodically aggregate the latest completed log files and make them accessible.
            </p>
          
        
        
          <h4 class="bold">Enable Log Aggregation</h4>
          
            <p>Log aggregation is enabled in the
              <code>yarn-site.xml</code>
              file. The
              <code>yarn.log-aggregation-enable</code>
              property enables log aggregation for running applications. You must also specify a log aggregation time
              interval using the
              <code>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds</code>
              property. The logs for running applications are aggregated at the specified interval. The minimum
              monitoring interval value is 3600 seconds (one hour).

              You can also set the monitoring interval value to -1 to disable log aggregation for running applications,
              and wait until the application finishes running to enable log aggregation.
            </p>
            <pre><code>  &lt;property&gt;
              &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
              &lt;value&gt;true&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds&lt;/name&gt;
              &lt;value&gt;3600&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>Clients should roll over their logs using a log roll-over application such as log4j.
                </p>
              </div>
            </aside>
          
        
        
          <h4 class="bold">Including and Excluding Log Files in YARN Applications Running on Slider</h4>
          
            <p>For YARN applications running on Slider, log aggregation is specified in the
              <code>global</code>
              section of the<code>resources.json</code>: configuration file:
            </p>
            <pre><code>"global": {
              "yarn.log.include.patterns": "",
              "yarn.log.exclude.patterns": ""
              },
            </code></pre>
            <p>If
              <code>yarn.log.include.patterns</code>
              is empty, all container logs are included. You can specify the name(s) of log files (for example,
              <code>agent.log</code>) that you do not want to aggregate using<code>
                yarn.log.exclude.patterns</code>.

              For example, the following
              <code>resources.json</code>
              property settings will aggregate all logs beginning with "std", but exclude the
              <code>stderr</code>
              log.
            </p>
            <pre><code>"global": {
              "yarn.log.include.patterns": "std*",
              "yarn.log.exclude.patterns": "stderr"
              },
            </code></pre>
            <p>The aggregated logs are stored in the HDFS
              <code>/app-logs/</code>
              directory. The following command can be used to retrieve the logs:
            </p>
            <pre><code>yarn logs -applicationId &lt;app_id&gt;</code></pre>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>For applications deployed directly on YARN, you can use
                  <code>LogAggregationContext</code>
                  inside
                  <code>ApplicationSubmissionContext</code>
                  to specify the include and exclude patterns.
                </p>
              </div>
            </aside>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-488d307a-6ccd-40ec-8455-ad40ac8e9a6b">Node Labels</h3>
        
          <p>This guide describes how to use Node labels to restrict YARN applications so that they run only on
            cluster nodes that have a specified node label.
          </p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>As discussed in the Capacity Scheduler guide, the fundamental unit of scheduling in YARN is the
              queue. The capacity of each queue specifies the percentage of cluster resources that are available for
              applications submitted to the queue. Queues can be set up in a hierarchy that reflects the resource
              requirements and access restrictions required by the various organizations, groups, and users that utilize
              cluster resources.

              Node labels can be assigned to cluster nodes. You can then associate node labels with capacity scheduler
              queues to specify which node label each queue is allowed to access.

              When a queue is associated with one or more node labels, all applications submitted by the queue run only
              on nodes with those specified labels. If no node label is assigned to a queue, the applications submitted
              by the queue can run on any node without a node label.

              Node labels represent one aspect of YARN resource management capabilities that includes CPU scheduling,
              CGroups, archival storage, and memory as storage.
            </p>
          
        
        
          <h4 class="bold">Configuring Node Labels</h4>
          
            <p>To enable node labels, make the following configuration changes on the YARN ResourceManager
              host.
            </p>
          
          
            <span class="title"><strong>1. Create a Label Directory in HDFS</strong></span>
            
              <p>Use the following commands to create a "node-labels" directory in which to store the node labels
                in HDFS.
              </p>
              <pre><code>sudo su hdfs
                hadoop fs -mkdir -p /yarn/node-labels
                hadoop fs -chown -R yarn:yarn /yarn
                hadoop fs -chmod -R 700 /yarn
              </code></pre>
              <p>
                <code>-chmod -R 700</code>
                specifies that only the yarn user can access the "node-labels" directory.

                You can then use the following command to confirm that the directory was created in HDFS.
              </p>
              <pre><code>hadoop fs -ls /yarn</code></pre>
              <p>The new node label directory should appear in the list returned by the following command. The
                owner should be<code>yarn</code>, and the permission should be<code>drwx</code>.
              </p>
              <pre><code>Found 1 items
                drwx------ - yarn yarn 0 2014-11-24 13:09 /yarn/node-labels
              </code></pre>
              <p>Use the following commands to create a
                <code>/user/yarn</code>
                directory that is required by the distributed shell.
              </p>
              <pre><code>hadoop fs -mkdir -p /user/yarn
                hadoop fs -chown -R yarn:yarn /user/yarn
                hadoop fs -chmod -R 700 /user/yarn
              </code></pre>
              <p>The preceding commands assume that the
                <code>yarn</code>
                user will be submitting jobs with the distributed shell. To run the distributed shell with a different
                user, create the user, then use
                <code>/user/&lt;user_name&gt;</code>
                in the file paths of the commands above to create a new user directory.
              </p>
            
          
          
            <span class="title"><strong>2. Configure YARN for Node Labels</strong></span>
            
              <p>Add the following properties to the
                <code>/etc/hadoop/conf/yarn-site.xml</code>
                file on the ResourceManager host.

                Set the following property to enable node labels:
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.node-labels.manager-class&lt;/name&gt;
                &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager&lt;/value&gt;
                &lt;/property&gt;</code></pre>
              <p>Set the following property to reference the HDFS node label directory:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.node-labels.fs-store.root-dir&lt;/name&gt;
                &lt;value&gt;hdfs://&lt;host&gt;:&lt;port&gt;/&lt;absolute_path_to_node_label_directory&gt;&lt;/value&gt;
                &lt;/property&gt;</code></pre>
              <p>For example:</p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.node-labels.fs-store.root-dir&lt;/name&gt;
                &lt;value&gt;hdfs://node-1.example.com:8020/yarn/node-labels/&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
          
            <span class="title"><strong>3. Start or Restart the YARN ResourceManager</strong></span>
            
              <p>In order for the configuration changes in the
                <code>yarn-site.xml</code>
                file to take effect, you must stop and restart the YARN ResourceManager if it is running, or start the
                ResourceManager if it is not running. To start or stop the ResourceManager, use the applicable commands
                in the "Controlling HDP Services Manually" section of the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Ref_Gd_v22/index.html">
                  HDP Reference Guide</a>.
              </p>
            
          
          
            <span class="title"><strong>4. Add and Assign Node Labels</strong></span>
            
              <p>For demonstration purposes, the following commands show how to use the
                <code>yarn rmadmin</code>
                client to add the node labels "x" and "y", but you can add your own node labels. You should run these
                commands as the
                <code>yarn</code>
                user. Node labels must be added before they can be assigned to nodes and associated with queues.
              </p>
              <pre><code>sudo su yarn
                yarn rmadmin -addToClusterNodeLabels "x,y"
              </code></pre>
              <p>You can use the
                <code>yarn cluster --list-node-labels</code>
                command to confirm that node labels have been added:
              </p>
              <pre><code>[root@node-1 /]# yarn cluster --list-node-labels
                14/11/21 13:09:55 INFO impl.TimelineClientImpl: Timeline service address:
                http://node-1.example.com:8188/ws/v1/timeline/
                14/11/21 13:09:55 INFO client.RMProxy: Connecting to ResourceManager at
                node-1.example.com/240.0.0.10:8032
                Node Labels: x,y
                [root@node-1 /]#
              </code></pre>
              <p>You can use the following command format to remove node labels:</p>
              <pre><code>yarn rmadmin -removeFromClusterNodeLabels "&lt;label1&gt;,&lt;label2&gt;"</code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>You cannot remove a node label if it is associated with a queue.</p>
                </div>
              </aside>
              <p>You can use the following command format to add or replace node label assignments on cluster
                nodes:
              </p>
              <pre><code>yarn rmadmin -replaceLabelsOnNode "&lt;node1&gt;:&lt;port&gt;,&lt;label1&gt;,&lt;label2&gt; &lt;node2&gt;:&lt;port&gt;,&lt;label1&gt;,&lt;label2&gt;"</code></pre>
              <p>For example, the following commands assign node label "x" to "node-1.example.com", and node
                label "y" to "node-2.example.com".
              </p>
              <pre><code>sudo su yarn
                yarn rmadmin -replaceLabelsOnNode "node-1.example.com,x node-2.example.com,y"
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>You can only assign one node label to each node. Also, if you do not specify a port, the
                    node label change will be applied to all NodeManagers on the host.
                  </p>
                </div>
              </aside>
              <p>To remove node label assignments from a node, use<code>-replaceLabelsOnNode</code>,
                but do not specify any labels. For example, you would use the following commands to remove the "x" label
                from node-1.example.com:
              </p>
              <pre><code>sudo su yarn
                yarn rmadmin -replaceLabelsOnNode "node-1.example.com"
              </code></pre>
            
          
          
            <span class="title"><strong>5. Associating Node Labels with Queues</strong></span>
            
              <p>Now that we have created node labels, we can associate them with queues in the
                <code>/etc/hadoop/conf/capacity-scheduler.xml</code>
                file.

                You must specify capacity on each node label of each queue, and also ensure that the sum of capacities
                of each node-label of direct children of a parent queue at every level is equal to 100%. Node labels
                that a queue can access (accessible node labels of a queue) must be the same as, or a subset of, the
                accessible node labels of its parent queue.

                <strong>Example:</strong>

                Assume that a cluster has a total of 8 nodes. The first 3 nodes (n1-n3) have node label=x, the next 3
                nodes (n4-n6) have node label=y, and the final 2 nodes (n7, n8) do not have any node labels. Each node
                can run 10 containers.

                The queue hierarchy is as follows:
              </p>
              <div class="figure">
                
                  
                    
                    <img src="System%20Admin%20Guides/node_labels_example_queue.png" width="50">
                  
                
              </div>
              <p>Assume that queue “a” can access node labels “x” and “y”, and queue “b” can only access node
                label “y”. By definition, nodes without labels can be accessed by all queues.

                Consider the following example label configuration for the queues:

                capacity(a) = 40, capacity(a, label=x) = 100, capacity(a, label=y) = 50;
                capacity(b) = 60, capacity(b, label=y) = 50

                This means that:
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Queue “a” can access 40% of the resources on nodes without any labels, 100% of the
                      resources on nodes with label=x, and 50% of the resources on nodes with label=y.
                    </p>
                  </li>
                  <li>
                    <p>Queue “b” can access 60% of the resources on nodes without any labels, and 50% of the
                      resources on nodes with label=y.
                    </p>
                  </li>
                
              </ul>
              <p>You can also see that for this configuration:

                capacity(a) + capacity(b) = 100
                capacity(a, label=x) + capacity(b, label=x) (b cannot access label=x, it is 0) = 100
                capacity(a, label=y) + capacity(b, label=y) = 100

                For child queues under the same parent queue, the sum of the capacity for each label should equal 100%.

                Similarly, we can set the capacities of the child queues a1, a2, and b1:

                a1 and a2:
                capacity(a.a1) = 40, capacity(a.a1, label=x) =30, capacity(a.a1, label=y) =50
                capacity(a.a2) = 60, capacity(a.a2, label=x) =70, capacity(a.a2, label=y) =50
                b1:
                capacity(b.b1) = 100
                capacity(b.b1, label=y) = 100

                You can see that for the a1 and a2 configuration:

                capacity(a.a1) + capacity(a.a2) = 100
                capacity(a.a1, label=x) + capacity(a.a2, label=x) = 100
                capacity(a.a1, label=y) + capacity(a.a2, label=y) = 100

                How many resources can queue a1 access?

                Resources on nodes without any labels:

                Resource = 20 (total containers that can be allocated on nodes without label, in this case n7, n8)
                * 40% (a.capacity)
                * 40% (a.a1.capacity)
                = 3.2 (containers)

                Resources on nodes with label=x

                Resource = 30 (total containers that can be allocated on nodes with label=x, in this case n1-n3)
                * 100% (a.label-x.capacity)
                * 30%
                = 9 (containers)
              </p>
              <p>To implement this example configuration, you would add the following properties in the
                <code>/etc/hadoop/conf/capacity-scheduler.xml</code>
                file.
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;
                &lt;value&gt;a,b&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.accessible-node-labels.x.capacity&lt;/name&gt;
                &lt;value&gt;100&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.accessible-node-labels.y.capacity&lt;/name&gt;
                &lt;value&gt;100&lt;/value&gt;
                &lt;/property&gt;

                &lt;!-- configuration of queue-a --&gt;
                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.accessible-node-labels&lt;/name&gt;
                &lt;value&gt;x,y&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.capacity&lt;/name&gt;
                &lt;value&gt;40&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.accessible-node-labels.x.capacity&lt;/name&gt;
                &lt;value&gt;100&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.accessible-node-labels.y.capacity&lt;/name&gt;
                &lt;value&gt;50&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.queues&lt;/name&gt;
                &lt;value&gt;a1,a2&lt;/value&gt;
                &lt;/property&gt;

                &lt;!-- configuration of queue-b --&gt;
                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.accessible-node-labels&lt;/name&gt;
                &lt;value&gt;y&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.capacity&lt;/name&gt;
                &lt;value&gt;60&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.accessible-node-labels.y.capacity&lt;/name&gt;
                &lt;value&gt;50&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.queues&lt;/name&gt;
                &lt;value&gt;b1&lt;/value&gt;
                &lt;/property&gt;

                &lt;!-- configuration of queue-a.a1 --&gt;
                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a1.accessible-node-labels&lt;/name&gt;
                &lt;value&gt;x,y&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a1.capacity&lt;/name&gt;
                &lt;value&gt;40&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a1.accessible-node-labels.x.capacity&lt;/name&gt;
                &lt;value&gt;30&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a1.accessible-node-labels.y.capacity&lt;/name&gt;
                &lt;value&gt;50&lt;/value&gt;
                &lt;/property&gt;

                &lt;!-- configuration of queue-a.a2 --&gt;
                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a2.accessible-node-labels&lt;/name&gt;
                &lt;value&gt;x,y&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a2.capacity&lt;/name&gt;
                &lt;value&gt;60&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a2.accessible-node-labels.x.capacity&lt;/name&gt;
                &lt;value&gt;70&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.a.a2.accessible-node-labels.y.capacity&lt;/name&gt;
                &lt;value&gt;50&lt;/value&gt;
                &lt;/property&gt;

                &lt;!-- configuration of queue-b.b1 --&gt;
                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.b1.accessible-node-labels&lt;/name&gt;
                &lt;value&gt;y&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.b1.capacity&lt;/name&gt;
                &lt;value&gt;100&lt;/value&gt;
                &lt;/property&gt;

                &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.b1.accessible-node-labels.y.capacity&lt;/name&gt;
                &lt;value&gt;100&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
          
            <span class="title"><strong>6. Refresh Queues</strong></span>
            
              <p>After adding or updating queue node label properties in the
                <code>capacity-scheduler.xml</code>
                file, you must run the following commands to refresh the queues:
              </p>
              <pre><code>sudo su yarn
                yarn rmadmin -refreshQueues
              </code></pre>
            
          
          
            <span class="title"><strong>8. Confirm Node Label Assignments</strong></span>
            
              <ul class="bullet-list">
                You can use the following commands to view information about node labels.
                
                  <li>
                    <p>List all running nodes in the cluster:

                      <code>yarn node -list</code>

                      Example:
                    </p>
                    <pre><code>[root@node-1 /]# yarn node -list
                      14/11/21 12:14:06 INFO impl.TimelineClientImpl: Timeline service address:
                      http://node-1.example.com:8188/ws/v1/timeline/
                      14/11/21 12:14:07 INFO client.RMProxy: Connecting to ResourceManager at
                      node-1.example.com/240.0.0.10:8032
                      Total Nodes:3
                      Node-Id Node-State Node-Http-Address Number-of-Running-Containers
                      node-3.example.com:45454 RUNNING node-3.example.com:50060 0
                      node-1.example.com:45454 RUNNING node-1.example.com:50060 0
                      node-2.example.com:45454 RUNNING node-2.example.com:50060 0
                      [root@node-1 /]#
                    </code></pre>
                  </li>
                  <li>
                    <p>List all node labels in the cluster:

                      <code>yarn cluster --list-node-labels

                      </code>Example:
                    </p>
                    <pre><code>[root@node-1 /]# yarn cluster --list-node-labels
                      14/11/21 13:09:55 INFO impl.TimelineClientImpl: Timeline service address:
                      http://node-1.example.com:8188/ws/v1/timeline/
                      14/11/21 13:09:55 INFO client.RMProxy: Connecting to ResourceManager at
                      node-1.example.com/240.0.0.10:8032
                      Node Labels: x,y
                      [root@node-1 /]#
                    </code></pre>
                  </li>
                  <li>
                    <p>List the status of a node (includes node labels):

                      <code>yarn node -status &lt;Node_ID&gt;</code>

                      Example:
                    </p>
                    <pre><code>[root@node-1 /]# yarn node -status node-1.example.com:45454
                      14/11/21 06:32:35 INFO impl.TimelineClientImpl: Timeline service address:
                      http://node-1.example.com:8188/ws/v1/timeline/
                      14/11/21 06:32:35 INFO client.RMProxy: Connecting to ResourceManager at
                      node-1.example.com/240.0.0.10:8032
                      Node Report :
                      Node-Id : node-1.example.com:45454
                      Rack : /default-rack
                      Node-State : RUNNING
                      Node-Http-Address : node-1.example.com:50060
                      Last-Health-Update : Fri 21/Nov/14 06:32:09:473PST
                      Health-Report :
                      Containers : 0
                      Memory-Used : 0MB
                      Memory-Capacity : 1408MB
                      CPU-Used : 0 vcores
                      CPU-Capacity : 8 vcores
                      Node-Labels : x

                      [root@node-1 /]#
                    </code></pre>
                  </li>
                
              </ul>
              <p>Node labels are also displayed in the ResourceManager UI on the Nodes and Scheduler pages.
              </p>
            
          
          
            <span class="title"><strong>Specifying a Child Queue with No Node Label</strong></span>
            
              <p>If no node label is specified for a child queue, it inherits the node label setting of its
                parent queue. To specify a child queue with no node label, use a blank space for the value of the node
                label. For example:
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.b1.accessible-node-labels&lt;/name&gt;
                &lt;value&gt; &lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
          
            <span class="title"><strong>Setting a Default Queue Node Label Expression</strong></span>
            
              <p>You can set a default node label on a queue. The default node label will be used if no label is
                specified when the job is submitted.
              </p>
              <p>For example, to set "x"as the default node label for queue "b1", you would add the following
                property in the
                <code>capacity-scheduler.xml</code>
                file.
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.scheduler.capacity.root.b.b1.default-node-label-expression&lt;/name&gt;
                &lt;value&gt;x&lt;/value&gt;
                &lt;/property&gt; </code></pre>
            
          
        
        
          <h4 class="bold">Using Node Labels</h4>
          
            <span class="title"><strong>ResourceManger UI</strong></span>
            
              <p>The ResourceManager UI displays the node labels on the Nodes page, and also on the Scheduler
                page.
              </p>
              <div class="figure">
                
                  
                    
                    <img src="System%20Admin%20Guides/rm_ui_node_labels.png" width="50">
                  
                
              </div>
              <div class="figure">
                
                  
                    
                    <img src="System%20Admin%20Guides/rm_ui_queue_a_node_labels.png" width="50">
                  
                
              </div>
              <div class="figure">
                
                  
                    
                    <img src="System%20Admin%20Guides/rm_ui_queue_b_node_labels.png" width="50">
                  
                
              </div>
            
          
          
            <span class="title"><strong>Setting Node Labels when Submitting Jobs</strong></span>
            
              <p>You can specify a label (using the
                <code>-node_label_expression</code>
                parameter) and a queue when you submit YARN jobs using the distributed shell client. If the queue has a
                label that satisfies the label expression, it will run the job on the labeled node(s). If the label
                expression does not reference a label associated with the specified queue, the job will not run and an
                error will be returned.
              </p>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>You can only specify one node label in the<code>-node_label_expression</code>.
                  </p>
                </div>
              </aside>
              <p>For example, the following commands run a simple YARN distributed shell "sleep for a long time"
                job. In this example we are asking for more containers than the cluster can run so we can see which node
                the job runs on. We are specifying that the job should run on queue "a1", which our user has permission
                to run jobs on. We are also using the
                <code>-label_expression</code>
                parameter to specify that the job will run on all nodes with label "x".
              </p>
              <pre><code>sudo su yarn
                hadoop jar /usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar
                -shell_command "sleep 100" -jar
                /usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar -num_containers 30
                -queue a1 -node_label_expression x
              </code></pre>
              <p>If we run this job on the example cluster we configured previously, containers are allocated on
                node-1, as this node has been assigned node label "x", and queue "a1" also has node label "x":
              </p>
              <div class="figure">
                
                  
                    <img src="System%20Admin%20Guides/rm_ui_job_running_label_x.png" width="50">
                  
                
              </div>
              <p>The following commands run the same job that we specifies node label "x", but this time we will
                specify queue "b1" rather than queue "a1".
              </p>
              <pre><code>sudo su yarn
                hadoop jar /usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar
                -shell_command "sleep 100000" -jar
                /usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar -num_containers 30
                -queue b1 -node_label_expression x
              </code></pre>
              <p>When we attempt to run this job on our example cluster, the job will fail with the following
                error message because label "x" is not associated with queue "b1".
              </p>
              <pre><code>14/11/24 13:42:21 INFO distributedshell.Client: Submitting application to ASM
                14/11/24 13:42:21 FATAL distributedshell.Client: Error running Client
                org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, queue=b1
                doesn't have permission to access all labels in resource request. labelExpression of resource request=x.
                Queue labels=y
              </code></pre>
            
          
          
            <span class="title"><strong>MapReduce Jobs and Node Labels</strong></span>
            
              <p>Currently you cannot specify a node label when submitting a MapReduce job. However, if you
                submit a MapReduce job to a queue that has a default node label expression, the default node label will
                be applied to the MapReduce job.

                Using default node label expressions tends to constrain larger portions of the cluster, which at some
                point starts to become counter-productive for jobs -- such as MapReduce jobs -- that benefit from the
                advantages offered by distributed parallel processing.
              </p>
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-bec73a32-204f-4c07-a436-55d160953f6e">Running Applications on YARN Using Slider</h3>
        
          <p>This guide describes how to run applications on YARN using Slider.</p>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>Apache Slider lets you deploy distributed applications across a Hadoop cluster. Slider leverages
              the YARN ResourceManager to allocate and distribute components of an application across a cluster.
            </p>
            <ul class="bullet-list">
              Key Slider features:
              
                <li>
                  <p>Run applications on YARN without changing the application code (as long as the application
                    follows Slider developer guidelines). There is no need to develop a custom Application Master or
                    other YARN code.
                  </p>
                </li>
                <li>
                  <p>Use the application registry for publishing and discovery of dynamic metadata.</p>
                </li>
                <li>
                  <p>Run multiple instances of applications with different configurations or versions in one
                    Hadoop cluster.
                  </p>
                </li>
                <li>
                  <p>Expand or shrink application component instances while an application is running.</p>
                </li>
                <li>
                  <p>Transparently deploy applications in secure Kerberos clusters.</p>
                </li>
                <li>
                  <p>Aggregate application logs from different containers.</p>
                </li>
                <li>
                  <p>Run applications on a subset of cluster nodes using YARN node labels.</p>
                </li>
                <li>
                  <p>Manage application, component, and container failures.</p>
                </li>
              
            </ul>
            <ul class="bullet-list">
              Slider leverages YARN capabilities to manage:
              
                <li>
                  <p>Application recovery in cases of container failure</p>
                </li>
                <li>
                  <p>Resource allocation (adding and removing containers)</p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">System Requirements</h4>
          
            <ul class="bullet-list">
              To run Slider you will need the following minimum system requirements:
              
                <li>
                  <p>Pivotal Data Platform 2.2</p>
                </li>
                <li>
                  <p>Required Services: HDFS, YARN and ZooKeeper</p>
                </li>
                <li>
                  <p>Oracle JDK 1.7 (64-bit)</p>
                </li>
                <li>
                  <p>OpenSSL</p>
                </li>
                <li>
                  <p>Python 2.6</p>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Operating System Requirements</h4>
          
            <p>Slider is compatible with all operating systems supported by HDP.</p>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>Accumulo on Slider is not supported on Windows, as Accumulo is not currently supported on
                  Windows.
                </p>
              </div>
            </aside>
          
        
        
          <h4 class="bold">Installing Apache Slider</h4>
          
            <p>To install Apache Slider, follow the instructions in the "Installing Apache Slider" section in the
              <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Man_Install_v22/index.html">
                Installing HDP Manually
              </a>
              Guide.
            </p>
          
        
        
          <h4 class="bold">Running Applications on Slider</h4>
          
            <p>The following sections describe how to configure, load, start, and verify Slider applications.
            </p>
          
          
            <h4 class="bold">The Slider Application Specification</h4>
            
              <p>Configuring a Slider application includes two specification components: the Resource
                Specification (<code>resources.json</code>) and the Application Configuration (<code>
                  appConfig.json</code>).
              </p>
            
            
              <span class="title"><strong>The Resource Specification</strong></span>
              
                <p>Slider needs to know what components (and how many components) are in an application package
                  to deploy.
                </p>
                <p>As Slider creates each instance of a component in its own YARN container, it also needs to
                  know what to ask YARN for in terms of
                  <strong>memory</strong>
                  and
                  <strong>CPU</strong>
                  for those containers.
                </p>
                <p>All this information goes into the
                  <strong>Resources Specification</strong>
                  file (“Resource Spec”) named resources.json. The Resource Specification tells Slider how many
                  instances of each component in the application to deploy, and also the parameters for YARN.
                </p>
              
            
            
              <span class="title"><strong>Resource Specification Example -- Apache Memcached</strong></span>
              
                <p></p>
                <pre><code>{
                  "schema" : "http://example.org/specification/v2.0.0",
                  "metadata" : {
                  },
                  "global" : {
                  },
                  "components": {
                  "slider-appmaster": {
                  },
                  "MEMCACHED": {
                  "yarn.role.priority": "1",
                  "yarn.component.instances": "1",
                  "yarn.vcores": "1",
                  "yarn.memory": "256"
                  }
                  }
                  }
                </code></pre>
              
            
            
              <span class="title"><strong>The Application Configuration</strong></span>
              
                <p>The Application Configuration file includes parameters that are specific to the application,
                  rather than to YARN. The Application Configuration is applied to the default configuration provided by
                  the application definition and then handed off to the associated component agent.
                </p>
              
            
            
              <span class="title"><strong>Application Configuration Example -- memcached</strong></span>
              
                <p></p>
                <pre><code>{
                  "schema": "http://example.org/specification/v2.0.0",
                  "metadata": {
                  },
                  "global": {
                  "application.def": "jmemcached-1.0.0.zip",
                  "java_home": "/usr/jdk64/jdk1.7.0_45",

                  "site.global.additional_cp": "/usr/hdp/current/hadoop-client/lib/*",
                  "site.global.xmx_val": "256m",
                  "site.global.xms_val": "128m",
                  "site.global.memory_val": "200M",
                  "site.global.listen_port": "${MEMCACHED.ALLOCATED_PORT}{PER_CONTAINER}"
                  },
                  "components": {
                  "slider-appmaster": {
                  "jvm.heapsize": "256M"
                  }
                  }
                  }
                </code></pre>
              
            
          
          
            <h4 class="bold">The Slider Application Package</h4>
            
              <p></p>
              <p>The Slider application package includes all of the information needed to deploy a YARN
                application using Slider. It includes the application configuration files, as well as all application
                artifacts and the scripts required by the application.

                Pivotal provides the following application packages for Accumulo, HBase, and Storm on Linux:
              </p>
              <p>
                <strong>centos6:</strong>
              </p>
              <pre><code>
                accumulo_pkg_url=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                hbase_pkg_url=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                storm_pkg_url=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
              </code></pre>
              <p>
                <strong>centos5:</strong>
              </p>
              <pre><code>
                accumulo_pkg_url=http://public-repo-1.hortonworks.com/HDP/centos5/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                hbase_pkg_url=http://public-repo-1.hortonworks.com/HDP/centos5/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                storm_pkg_url=http://public-repo-1.hortonworks.com/HDP/centos5/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
              </code></pre>
              <p>
                <strong>suse11sp3</strong>
              </p>
              <pre><code>
                accumulo_pkg_url=http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                hbase_pkg_url=http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                storm_pkg_url=http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
              </code></pre>
              <p>
                <strong>sles11sp1</strong>
              </p>
              <pre><code>
                accumulo_pkg_url=http://public-repo-1.hortonworks.com/HDP/sles11sp1/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                hbase_pkg_url=http://public-repo-1.hortonworks.com/HDP/sles11sp1/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                storm_pkg_url=http://public-repo-1.hortonworks.com/HDP/sles11sp1/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
              </code></pre>
              <p>
                <strong>ubuntu12</strong>
              </p>
              <pre><code>
                accumulo_pkg_url=http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                hbase_pkg_url=http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                storm_pkg_url=http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
              </code></pre>
              <p>
                <strong>debian6</strong>
              </p>
              <pre><code>
                accumulo_pkg_url=http://public-repo-1.hortonworks.com/HDP/debian6/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                hbase_pkg_url=http://public-repo-1.hortonworks.com/HDP/debian6/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                storm_pkg_url=http://public-repo-1.hortonworks.com/HDP/debian6/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>Accumulo on Slider is not supported on Windows, as Accumulo is not currently supported on
                    Windows.
                  </p>
                </div>
              </aside>
              <p>For more information on Apache Slider application packaging, see<a href="http://slider.incubator.apache.org/docs/slider_specs/index.html">this page</a>.
                
                For more information on creating a Slider application package for Memcached, see<a href="http://slider.incubator.apache.org/docs/slider_specs/hello_world_slider_app.html">
                  this page</a>.
                
              </p>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>The HDP .msi installer file for Windows includes the application packages for HBase and
                    Storm on Windows.
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Install the Application Package</h4>
            
              <p>Use the following command format to install the application package on YARN.</p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider install-package --name &lt;name&gt; --package &lt;package_path_on_disk&gt;</code></pre>
            
          
          
            <h4 class="bold">Start the Application</h4>
            
              <p>Once the steps above are completed, the Slider Command Line Interface (CLI) can be used to start
                the application using the following command format:
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider create &lt;application_name&gt; --template
                appConfig.json --resources resources.json
              </code></pre>
              <p>Default files (
                <code>appConfig-default.json</code>
                and<code>resources-default.json</code>) files are included in each application package.
              </p>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>You can create multiple instances of an application by running the Slider
                    <code>create</code>
                    command with different application names. For example, you might change configuration settings in
                    appConfig.json and create a separate instance to evaluate the new configuration.
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Verify the Application</h4>
            
              <p>You can use the Slider
                <code>status</code>
                CLI command to verify the application launch:
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider status &lt;application_name&gt;</code></pre>
            
            
              <p>You can also verify the successful launch of the application with the YARN Resource Manager Web
                UI. In most instances, this UI is accessible via a web browser at port 8088 of the Resource Manager
                Host:
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/Slider/HBase_on_Slider_RM_page.png" width="50">
                  
                
              </div>
              <p>The specific information for the running application is accessible via the “ApplicationMaster”
                link that can be seen in the far right column of the row associated with the running application
                (probably the top row):
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/Slider/Slider_HBase_App_Master.png" width="50">
                  
                
              </div>
            
          
          
            <h4 class="bold">Manage the Application Lifecycle</h4>
            
              <p>Once started, applications can be stopped, restarted, destroyed, and flexed as follows:
              </p>
              <p>
                <strong>
                  stop &lt;name&gt; [--force] [--wait time] [--message text]
                </strong>
              </p>
              <p>
                Stops the application instance. The running application is stopped. Its settings are retained in HDFS.

                The
                <code>--wait</code>
                argument can specify a time in seconds to wait for the application instance to be stopped.

                The -
                <code>-force</code>
                flag causes YARN asked directly to terminate the application instance. The
                <code>--message</code>
                argument supplies an optional text message to be used in the request: this will appear in the
                application's diagnostics in the YARN RM UI.

                If an unknown (or already stopped) application instance is named, no error is returned.

                Example:
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider stop app1</code></pre>
              <p>
                <strong>start &lt;name&gt; [--wait milliseconds] [--out &lt;filename&gt;]
                </strong>
                Start a stopped application instance, recreating it from its previously saved state.

                After the application is launched, if an
                <code>--out</code>
                argument specified a file, the "YARN application report" will be saved as a .json document into the file
                specified.

                Example
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider start app1</code></pre>
              <p><strong>
                destroy &lt;name&gt;

              </strong>Destroy a (stopped) application instance. Important: This deletes all persistent data.

                Example:
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider destroy app1</code></pre>
              <p>
                <strong>flex &lt;name&gt; [--component component count]*

                </strong>Flex the number of workers in an application instance to the new value. If greater than before,
                new copies of the component will be requested. If less, component instances will be destroyed.

                This operation has a return value of 0 if the change was submitted. Changes are not immediate and depend
                on the availability of resources in the YARN cluster

                It returns -1 if there is no running application instance.

                Examples:
              </p>
              <pre><code>slider flex app1 --component worker 8 --filesystem hdfs://&lt;host&gt;:&lt;port&gt;
                slider flex app1 --component master 2 --filesystem hdfs://&lt;host&gt;:&lt;port&gt;</code></pre>
              <p>For more information on Slider CLI commands, see the Apache<a href="http://slider.incubator.apache.org/docs/manpage.html">Slider documentation</a>.
              </p>
            
            
              <p>You can use the Slider
                <code>help</code>
                command to display a list of available commands.
              </p>
              <pre><code>[root@node-1 /]# /usr/hdp/current/slider-client/bin/./slider help
                2014-10-31 09:20:10,852 [main] INFO client.SliderClient -
                Usage: slider COMMAND [options]
                where COMMAND is one of
                am-suicide Tell the Slider Application Master to simulate a process failure by terminating itself
                build Build a Slider cluster specification -but do not start it
                create Create a live Slider application
                update Update template for a Slider application
                destroy Destroy a frozen Slider application)
                diagnostics Diagnose the configuration of the running slider application and slider client
                exists Probe for an application running
                flex Flex a Slider application
                stop Stop a running application
                help Print help information
                install-package Install the application package in the home directory under sub-folder packages
                install-keytab Install the Kerberos keytab file in the sub-folder 'keytabs' of the user's Slider base
                directory
                kill-container Kill a container in the application
                list List running Slider applications
                registry Query the registry of a YARN application
                resolve Query the registry of a YARN application
                status Get the status of an application
                start Start a stopped application
                version Print the Slider version information
                Most commands print help when invoked without parameters
                2014-10-31 09:20:10,856 [main] INFO util.ExitUtil - Exiting with status 0
              </code></pre>
              <p>Most commands display Help when invoked without parameters.</p>
              <pre><code>[root@node-1 /]# slider status
                2014-10-31 09:45:11,817 [main] ERROR main.ServiceLauncher -
                org.apache.slider.core.exceptions.BadCommandArgumentsException: Not enough arguments for action: status
                Expected minimum 1 but got 0

                Usage: slider status &lt;application&gt;
                --manager &lt;manager&gt; Binding (usually hostname:port) of the YARN resource manager (optional)
                --basepath &lt;basePath&gt; Slider base path on the filesystem (optional)
                --filesystem &lt;filesystemBinding&gt;Filesystem Binding (optional)
                -D &lt;definitions&gt; Definitions (optional)
                --debug Debug mode (optional)
                -S &lt;sysprops&gt; system properties in the form name value These are set after the JVM is started.
                (optional)
                --out &lt;output&gt; Output file for the configuration data (optional)

                2014-10-31 09:45:11,826 [main] INFO util.ExitUtil - Exiting with status 40
                [root@node-1 /]#
              </code></pre>
            
          
          
            <h4 class="bold">The Application Registry</h4>
            
              <p>Each application publishes several artifacts that can be used by an application administrator or
                an application client. Typical data published includes the applied configuration, links to application
                JMX endpoint or monitoring UI, and log folders.
              </p>
              <p>All published data is available at the publisher endpoint that is hosted by Application Master
                launched by Slider. Here is an example of the publisher endpoint:
              </p>
              <pre><code>http://c6401.ambari.apache.org:47457/ws/v1/slider/publisher</code></pre>
              <p>From this endpoint, you can access several named property containing useful administrative
                information.
              </p>
              <div class="xyleme-table"><table border="1">
                
                  
                  
                  <thead></thead>
                  <tbody>
                    <tr>
                      <th rowspan="1">
                        <p>
                          <strong>Publisher URI</strong>
                        </p>
                      </th>
                      <th rowspan="1">
                        <p>
                          <strong>Description</strong>
                        </p>
                      </th>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>
                          <code>{publisher-endpoint}/slider/quicklinks</code>
                        </p>
                      </td>
                      <td rowspan="1">
                        <p>Named URLs that the application publishes.</p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>
                          <code>{publisher-endpoint}/slider/logfolders</code>
                        </p>
                      </td>
                      <td rowspan="1">
                        <p>Log folders for the application components (YARN should be configured to retain
                          logs).
                        </p>
                      </td>
                    </tr>
                    <tr>
                      <td rowspan="1">
                        <p>
                          <code>{publisher-endpoint}/slider/storm-site</code>
                        </p>
                      </td>
                      <td rowspan="1">
                        <p>Applied configurations by the application (for example, storm-site, hbase-site).
                        </p>
                      </td>
                    </tr>
                  </tbody>
                  
                
              </table></div>
              <p>Example output from<code>/slider/quicklinks</code>:
              </p>
              <pre><code>{
                "description": "QuickLinks",
                "entries": {
                "org.apache.slider.jmx": "http://c6401.ambari.apache.org:50154/api/cluster/summary",
                "org.apache.slider.metrics": "http://c6401.ambari.apache.org/cgi-bin/rrd.py?c=Application2",
                "org.apache.slider.monitor": "http://c6401.ambari.apache.org:41806",
                "org.apache.slider.ganglia": "http://c6401.ambari.apache.org/ganglia?c=Application2"
                },
                "updated": 0,
                "empty": false
                }
              </code></pre>
            
          
        
        
          <h4 class="bold">Running HBase on YARN via Slider</h4>
          
            <p>This guide describes how to install and configure HBase on YARN using Slider.</p>
          
          
            <h4 class="bold">Introduction</h4>
            
              <p>Running HBase on YARN via Slider offers the following advantages:</p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Easier installation and start-up.</p>
                  </li>
                  <li>
                    <p>You can run mulitple HBase clusters on one Hadoop cluster.</p>
                  </li>
                  <li>
                    <p>Process management (start/stop) is much easier with Slider.</p>
                  </li>
                  <li>
                    <p>You can run different versions of HBase on the same Hadoop cluster.</p>
                  </li>
                  <li>
                    <p>HBase cluster size is now a parameter that you can easily change.</p>
                  </li>
                
              </ul>
            
          
          
            <h4 class="bold">Downloading and Installing the HBase Application Package</h4>
            
              <ul class="number-list">
                
                  <strong>To install the HBase Application Package:</strong>
                
                
                  <li>
                    <p>Make a working directory for the HBase-on-Slider application package:</p>
                    <pre><code>mkdir -p /usr/work/app-packages/hbase</code></pre>
                  </li>
                  <li>
                    <p>Download the HBase-on-Slider application package for your operating system to the
                      <code>/usr/work/app-packages/hbase</code>
                      directory. For example, use the following command to download the centos6 version:
                    </p>
                    <pre><code>cd /usr/work/app-packages/hbase
                      wget
                      http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0/slider-app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                    </code></pre>
                  </li>
                  <li>
                    <p>Use the following command format to install the application package on YARN:</p>
                    <pre><code>su &lt;user&gt;
                      /usr/hdp/current/slider-client/bin/./slider install-package --name HBASE --package &lt;package
                      path&gt;</code></pre>
                    <p>Where
                      <code>&lt;user&gt;</code>
                      is the user who will create the HBase application instance.
                    </p>
                    <p>
                      For example, if you downloaded the HBase package to the
                      <code>/usr/work/app-packages/hbase</code>
                      folder:
                    </p>
                    <pre><code>su &lt;user&gt;
                      /usr/hdp/current/slider-client/bin/./slider install-package --name HBASE --package
                      /usr/work/app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                    </code></pre>
                    <p>Note that the
                      <code>--name</code>
                      (in this case, "HBASE") should match the value of the
                      <code>&lt;name&gt;</code>
                      element in the package
                      <code>metainfo.xml</code>
                      flle, and that the install destination in HDFS is in the user's home directory under
                      ./slider/package/HBASE/*.zip
                    </p>
                  </li>
                
              </ul>
            
          
          
            <h4 class="bold">Configuring HBase on YARN via Slider</h4>
            
              <span class="title"><strong>Accessing the HBase Configuration Files</strong></span>
              
                <p>The HBase application package includes default application and resource specification files.
                  The package includes both non-secure (<code>appConfig-default.json</code>) and secure (
                  <code>appConfig-secured-default.json</code>) versions of the application specification.
                  You can save these files as another name, and then edit the files to customize the HBase
                  configuration.

                  You can use the
                  <code>unzip</code>
                  command to extract the HBase application and resource specification files from the HBase-on-Slider
                  application package. For example, you would use the following command to extract the files from the
                  HBase application package in the
                  <code>/usr/work/app-packages/hbase</code>
                  directory:
                </p>
                <pre><code>unzip /usr/work/app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                  appConfig-default.json -d /usr/work/app-packages/hbase/
                  unzip /usr/work/app-packages/hbase/slider-hbase-app-package-0.98.4.2.2.0.0-2041-hadoop2.zip
                  resources-default.json -d /usr/work/app-packages/hbase/
                </code></pre>
                <p>You can use the following commands to copy and rename the default HBase application and
                  resource specification files in the
                  <code>/usr/work/app-packages/hbase/</code>
                  directory:
                </p>
                <pre><code>cp /usr/work/app-packages/hbase/appConfig-default.json /usr/work/app-packages/hbase/appConfig.json
                  cp /usr/work/app-packages/hbase/resources-default.json /usr/work/app-packages/hbase/resources.json
                </code></pre>
                <p>The default configuration files should work as-is, but you can also edit the files to adjust
                  the HBase configuration.
                </p>
              
            
          
          
            <h4 class="bold">Configuring HBase on YARN on Secure Clusters</h4>
            
              <p>As previously mentioned, the HBase-on-Slider application package includes both non-secure (
                <code>appConfig-default.json</code>) and secure (<code>
                  appConfig-secured-default.json</code>) versions of the application specification.

                On secure clusters, you should use the secure version of the application specification. The
                security-related entries in the
                <code>appConfig-secured.json</code>
                file are listed below. You will need to replace the values labeled "TODO" with the security settings for
                the cluster.
              </p>
              <pre><code>"site.hbase-site.hbase.coprocessor.master.classes":
                "org.apache.hadoop.hbase.security.access.AccessController",
                "site.hbase-site.hbase.coprocessor.region.classes" :
                "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController",
                "site.hbase-site.hbase.regionserver.kerberos.principal": "${TODO-RS-PRINCIPAL}",
                "site.hbase-site.hbase.regionserver.keytab.file": "${TODO-RS-KEYTAB}",
                "site.hbase-site.hbase.master.kerberos.principal": "${TODO-MASTER-PRINCIPAL}",
                "site.hbase-site.hbase.master.keytab.file": "${TODO-MASTER-KEYTAB}",
                "site.hbase-site.hbase.rest.authentication.kerberos.keytab" : "${TODO-REST-AUTH-KEYTAB}",
                "site.hbase-site.hbase.rest.kerberos.principal" : "${TODO-REST-PRINCIPAL}",
                "site.hbase-site.hbase.rest.keytab.file" : "${TODO-REST-KEYTAB}",
                "site.hbase-site.hbase.thrift.keytab.file" : "${TODO-THRIFT-KEYTAB}",
                "site.hbase-site.hbase.thrift.kerberos.principal" : "${TODO-THRIFT-PRINCIPAL}",
                "site.hdfs-site.dfs.namenode.kerberos.principal": "${TODO-NN-PRINCIPAL}",
                "site.hdfs-site.dfs.namenode.kerberos.internal.spnego.principal": "${TODO-NN-SPNEGO-PRINCIPAL}",
                "site.hdfs-site.dfs.secondary.namenode.kerberos.principal": "${TODO-SNN-PRINCIPAL}",
                "site.hdfs-site.dfs.secondary.namenode.kerberos.internal.spnego.principal":
                "${TODO-SNN-SPNEGO-PRINCIPAL}",
                "site.hdfs-site.dfs.datanode.kerberos.principal": "${TODO-DN-PRINCIPAL}",

                Note: rest and thrift components are included above. The values in curly braces need to be filled out.
                In components section:
                "slider-appmaster": {
                "jvm.heapsize": "256M",
                "slider.am.keytab.local.path": "${TODO-HEADLESS-KEYTAB}",
                "slider.keytab.principal.name": "${TODO-HEADLESS-PRINCIPAL}"
                },
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>For more information on configuring HBase-on-Slider on secure clusters, including
                    information about keytab-associated properties and the available keytab distribution options, see
                    <a href="http://slider.incubator.apache.org/docs/security.html">Apache Slider
                      Security</a>.
                    
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Components in HBase on YARN</h4>
            
              <p>You can specify the following components (also referred to as "roles") when deploying HBase on
                YARN via Slider:
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>
                      <code>HBASE_MASTER</code>
                      ⎯ This corresponds to HBase master process.
                    </p>
                  </li>
                  <li>
                    <p>
                      <code>HBASE_REGIONSERVER</code>
                      ⎯ This corresponds to region server process.
                    </p>
                  </li>
                  <li>
                    <p>
                      <code>HBASE_REST</code>
                      ⎯ This corresponds to REST (aka Stargate) gateway.
                    </p>
                  </li>
                  <li>
                    <p>
                      <code>HBASE_THRIFT</code>
                      ⎯ This corresponds to Thrift gateway.
                    </p>
                  </li>
                  <li>
                    <p>
                      <code>HBASE_THRIFT2</code>
                      ⎯ This corresponds to Thrift2 gateway.
                    </p>
                  </li>
                
              </ul>
              <p>The following is a sample
                <code>resources.json</code>
                file with each of these roles configured:
              </p>
              <pre><code>{
                "schema": "http://example.org/specification/v2.0.0",
                "metadata": {
                },
                "global": {
                "yarn.log.include.patterns": "",
                "yarn.log.exclude.patterns": "",
                "yarn.log.interval": "0"
                },
                "components": {
                "HBASE_MASTER": {
                "yarn.role.priority": "1",
                "yarn.component.instances": "1"
                },
                "slider-appmaster": {
                },
                "HBASE_REGIONSERVER": {
                "yarn.role.priority": "2",
                "yarn.component.instances": "4"
                },
                "HBASE_THRIFT": {
                "yarn.role.priority": "4",
                "yarn.component.instances": "1",
                "yarn.memory": "256"
                },
                "HBASE_THRIFT2": {
                "yarn.role.priority": "5",
                "yarn.component.instances": "0",
                "yarn.memory": "256"
                },
                "HBASE_REST": {
                "yarn.role.priority": "3",
                "yarn.component.instances": "1",
                "yarn.memory": "256"
                }
                }
                }
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>You should use either Thrift or Thrift2, but not both, as they provide similar
                    functionality.
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Launching an HBase Application Instance</h4>
            
              <p>Use the following command format to launch HBase:</p>
              <pre><code>su &lt;user&gt;
                /usr/hdp/current/slider-client/bin/./slider create &lt;hb_cluster_name&gt; --template appConfig.json
                --resources resources.json
              </code></pre>
              <p>Where
                <code>&lt;user&gt;</code>
                is the user who installed the HBase application package.
              </p>
              <p>
                For example:
              </p>
              <pre><code>su &lt;user&gt;
                /usr/hdp/current/slider-client/bin/./slider create hb1 --template
                /usr/work/app-packages/hbase/appConfig.json --resources /usr/work/app-packages/hbase/resources.json
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>Uppercase characters are not allowed in the cluster name.</p>
                </div>
              </aside>
            
            
              <p>You can use the Slider CLI
                <code>status</code>
                command to verify the application launch:
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider status &lt;application_name&gt;</code></pre>
            
            
              <p>You can also verify the successful launch of the HBase application with the YARN Resource
                Manager Web UI. In most instances, this UI is accessible via a web browser at port 8088 of the Resource
                Manager Host:
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/Slider/HBase_on_Slider_RM_page.png" width="50">
                  
                
              </div>
              <p>The specific information for the running application is accessible via the “ApplicationMaster”
                link that can be seen in the far right column of the row associated with the running application
                (probably the top row):
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/Slider/Slider_HBase_App_Master.png" width="50">
                  
                
              </div>
            
          
          
            <h4 class="bold">Deployment Considerations</h4>
            
              <p>For HA purposes, it is not advisable to deploy two Masters on the same host.

                Thrift/Thrift2/REST gateways are likely to receive heavy traffic. They should be deployed on dedicated
                hosts.
              </p>
            
            
              <span class="title"><strong>Memory Considerations for Running One Component on a Node</strong></span>
              
                <p>You can adjust the amount of memory given to a component to achieve mutual exclusion of
                  components, depending upon the NodeManager configuration on each node. Typically all nodes have the
                  same value independent of the actual memory.

                  Assuming the memory capacity for each NodeManager is known (<code>
                    yarn.nodemanager.resource.memory-mb</code>), you can configure the component to ask for 51%
                  (basically more than half) of the maximum capacity. You also need to ensure that the maximum possible
                  memory allocation (<code>yarn.scheduler.maximum-allocation-mb</code>) allows that value.

                  For example, if
                  <code>yarn.nodemanager.resource.memory-mb</code>
                  =
                  <code>yarn.scheduler.maximum-allocation-mb</code>
                  = 2048
                  Set
                  <code>yarn.memory</code>
                  = 1280 for the RegionServer.

                  Then set the HBase Master/RegionServer max memory to be 256 MB less than that to allow for the agent
                  memory consumption ⎯ the agent should not be more than 100 MB but you can just assume that it consumes
                  ~256 MB. So you can set the HBase Master/RegionServer variables for memory limit to 1024 MB.
                </p>
              
            
            
              <span class="title"><strong>Log Aggregation</strong></span>
              
                <p>This feature is backed by<a href="https://issues.apache.org/jira/browse/YARN-2468">
                  https://issues.apache.org/jira/browse/YARN-2468</a>.
                  

                  Log aggregation is specified in the
                  <code>global</code>
                  section of<code>resources.json</code>:
                </p>
                <pre><code>"global": {
                  "yarn.log.include.patterns": "",
                  "yarn.log.exclude.patterns": "",
                  "yarn.log.interval": "0"
                  },
                </code></pre>
                <p>The
                  <code>yarn.log.interval</code>
                  unit is seconds.

                  You can specify the name(s) of log files (for example,<code>agent.log</code>) that you do
                  not want to aggregate using<code>yarn.log.exclude.patterns</code>.

                  The aggregated logs are stored in the HDFS
                  <code>/app-logs/</code>
                  directory.

                  The following command can be used to retrieve the logs:
                </p>
                <pre><code>yarn logs -applicationId &lt;app_id&gt;</code></pre>
              
            
            
              <span class="title"><strong>Reserving Nodes for HBase</strong></span>
              
                <p>You can use YARN node labels to reserve cluster nodes for applications and their components.

                  Node labels are specified with the
                  <code>yarn.label.expression</code>
                  property. If no label is specified, only non-labeled nodes are used when allocating containers for
                  component instances.

                  If label expression is specified for slider-appmaster, then it also becomes the default label
                  expression for all components. To take advantage of default label expression, leave out the property
                  (see HBASE_REGIONSERVER in the example). A label expression with an empty string (<code>
                    "yarn.label.expression":""</code>) is equivalent to nodes without labels.

                  For example, the following is a resources.json file for an HBase cluster that uses node labels. The
                  label for the application instance is "hbase1", and the label expression for the
                  <code>HBASE_MASTER</code>
                  components is "hbase1_master".
                  <code>HBASE_REGIONSERVER</code>
                  instances will automatically use label "hbase1". Alternatively, if you specify (<code>
                    "yarn.label.expression":""</code>) for
                  <code>HBASE_REGIONSERVER</code>
                  then the containers will only be allocated on nodes with no labels.
                </p>
                <pre><code>{
                  "schema": "http://example.org/specification/v2.0.0",
                  "metadata": {
                  },
                  "global": {
                  },
                  "components": {
                  "HBASE_MASTER": {
                  "yarn.role.priority": "1",
                  "yarn.component.instances": "1",
                  "yarn.label.expression":"hbase1_master"
                  },
                  "HBASE_REGIONSERVER": {
                  "yarn.role.priority": "1",
                  "yarn.component.instances": "1",
                  },
                  "slider-appmaster": {
                  "yarn.label.expression":"hbase1"
                  }
                  }
                  }
                </code></pre>
                <ul class="bullet-list">
                  Specifically, for the above example you would need to:
                  
                    <li>
                      <p>Create two node labels, "hbase1" and "hbase1_master" (using
                        <code>yarn rmadmin</code>
                        commands)
                      </p>
                    </li>
                    <li>
                      <p>Assign the labels to nodes (using
                        <code>yarn rmadmin</code>
                        commands)
                      </p>
                    </li>
                    <li>
                      <p>Create a queue by defining it in the
                        <code>capacity-scheduler.xml</code>
                        configuration file.
                      </p>
                    </li>
                    <li>
                      <p>Allow the queue to access to the labels and ensure that appropriate min/max capacity is
                        assigned.
                      </p>
                    </li>
                    <li>
                      <p>Refresh the queues (<code>yarn rmadmin -refreshQueues</code>)
                      </p>
                    </li>
                    <li>
                      <p>Create the Slider application against the above queue using parameter
                        <code>--queue</code>
                        while creating the application.
                      </p>
                    </li>
                  
                </ul>
              
            
            
              <span class="title"><strong>Retrieving Effective hbase-site.xml</strong></span>
              
                <p>Once HBase is running on Slider, you can use the following command to retrieve
                  <code>hbase-site.xml</code>
                  so that your client can connect to the cluster:
                </p>
                <pre><code>slider registry --getconf hbase-site --name &lt;cluster name&gt; --format xml --dest &lt;path to
                  local hbase-site.xml&gt; --filesystem &lt;hdfs namenode&gt; --manager &lt;resource manager&gt;</code></pre>
                <p>Note that the
                  <code>hbase.tmp.dir</code>
                  in the returned file may be inside the YARN local directory for the container (which is local to the
                  host where the container is running). For example:
                </p>
                <pre><code>&lt;property&gt;
                  &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/grid/0/hadoop/yarn/local/usercache/test/appcache/application_1413942326640_0001/container_1413942326640_0001_01_000005/work/app/tmp&lt;/value&gt;
                  &lt;source/&gt;
                  &lt;/property&gt;</code></pre>
                <p>If the client does not have access to this directory, changing the
                  <code>hbase.tmp.dir</code>
                  to a directory writable to the user would solve the permission issue.
                </p>
                <p>
                  <strong>Retrieving REST gateway for Slider HBase</strong>
                  You can retrieve quicklinks for Slider HBase first. The "org.apache.slider.hbase.rest" entry would
                  show hostname and port for REST gateway.

                  <strong>Retrieving thrift gateway for Slider HBase</strong>
                  You can retrieve quicklinks for Slider HBase first. The "org.apache.slider.hbase.thrift" entry would
                  show hostname and port for thrift gateway.

                  <strong>Retrieving thrift2 gateway for Slider HBase</strong>
                  You can retrieve quicklinks for Slider HBase first. The "org.apache.slider.hbase.thrift2" entry would
                  show hostname and port for thrift2 gateway.
                </p>
              
            
            
              <span class="title"><strong>Workaround for HBase Client Issue</strong></span>
              
                <p>After you create an HBase application instance in Slider, you can use the
                  <code>slider registry</code>
                  command to retrieve the
                  <code>hbase-site.xml</code>
                  file:
                </p>
                <pre><code>slider registry --name hbase1 --getconf hbase-site --format xml --out hbase-site.xml</code></pre>
                <p>The path of the
                  <code>hbase.tmp.dir</code>
                  property in the returned file will be inside the YARN local directory for the container (which is
                  local to the host where the container is running):
                </p>
                <pre><code>  &lt;property&gt;
                  &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/grid/0/hadoop/yarn/local/usercache/test/appcache/application_1413942326640_0001/container_1413942326640_0001_01_000005/work/app/tmp&lt;/value&gt;
                  &lt;source/&gt;
                  &lt;/property&gt;</code></pre>
                <p>The HBase client (specifically the HBase shell) expects to see a directory named "jars" in
                  this directory. If it is not there the HBase client attempts to create it, resulting in a failure to
                  perform shell operations:
                </p>
                <pre><code>[test@ip-10-0-0-66 hbase]$ hbase --config ./conf shell
                  ...
                  hbase(main):001:0&gt; status 'simple'
                  ...
                  ERROR: Failed to create local dir
                  /grid/0/hadoop/yarn/local/usercache/test/appcache/application_1413942326640_0001/container_1413942326640_0001_01_000005/work/app/tmp/local/jars,
                  DynamicClassLoader failed to init
                </code></pre>
                <p>
                  <strong>Workaround:</strong>
                  Change the
                  <code>hbase.tmp.dir</code>
                  to a directory that is writable for the user running "hbase shell" or to a directory which already has
                  the "jars" directory.
                </p>
              
            
          
        
        
          <h4 class="bold">Running Storm on YARN via Slider</h4>
          
            <p>This guide describes how to install and configure Storm on YARN using Slider.</p>
          
          
            <h4 class="bold">Introduction</h4>
            
              <p>Running Storm on YARN via Slider offers the following advantages.</p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Easier installation and start-up.</p>
                  </li>
                  <li>
                    <p>You can run multiple instances of Storm on one Hadoop cluster.</p>
                  </li>
                  <li>
                    <p>Process management (start/stop) is much easier with Slider.</p>
                  </li>
                  <li>
                    <p>You can run different versions of Storm on the same Hadoop cluster.</p>
                  </li>
                
              </ul>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>You can run Storm stand-alone and Storm on Slider on the same cluster, but only if they use
                    different topologies.
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Downloading and Installing the Storm Application Package</h4>
            
              <ul class="number-list">
                
                  <strong>To install the Storm Application Package:</strong>
                
                
                  <li>
                    <p>Make a working directory for the Storm-on-Slider application package:</p>
                    <pre><code>mkdir -p /usr/work/app-packages/storm</code></pre>
                  </li>
                  <li>
                    <p>Download the Storm-on-Slider application package for your operating system to the
                      <code>/usr/work/app-packages/storm</code>
                      directory. For example, use the following command to download the centos6 version:
                    </p>
                    <pre><code>cd /usr/work/app-packages/storm
                      wget
                      http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0/slider-app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
                    </code></pre>
                  </li>
                  <li>
                    <p>Use the following command format to install the application package on YARN:</p>
                    <pre><code>su &lt;user&gt;
                      /usr/hdp/current/slider-client/bin/./slider install-package --name STORM --package &lt;package
                      path&gt;</code></pre>
                    <p>Where
                      <code>&lt;user&gt;</code>
                      is the user who will create the Storm application instance.

                      For example, if you downloaded the Storm package to the
                      <code>/usr/work/app-packages/storm</code>
                      folder:
                    </p>
                    <pre><code>su &lt;user&gt;
                      /usr/hdp/current/slider-client/bin/./slider install-package --name STORM --package
                      /usr/work/app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
                    </code></pre>
                    <p>Note that the
                      <code>--name</code>
                      (in this case, "STORM") should match the value of the
                      <code>&lt;name&gt;</code>
                      element in the package
                      <code>metainfo.xml</code>
                      flle, and that the install destination in HDFS is in the user's home directory under
                      ./slider/package/STORM/*.zip
                    </p>
                  </li>
                
              </ul>
            
          
          
            <h4 class="bold">Configuring Storm on YARN</h4>
            
              <span class="title"><strong>Accessing the Storm Configuration Files</strong></span>
              
                <p>The Storm application package includes default application and resource specification files.
                  The package includes both non-secure (<code>appConfig-default.json</code>) and secure (
                  <code>appConfig-secured-default.json</code>) versions of the application specification.
                  You can save these files as a another name, and then edit the files to customize the Storm
                  configuration.

                  You can use the
                  <code>unzip</code>
                  command to extract the Storm application and resource specification files from the Storm-on-Slider
                  application package. For example, you would use the following command to extract the files from the
                  Storm application package in the
                  <code>/usr/work/app-packages/storm</code>
                  directory:
                </p>
                <pre><code>unzip /usr/work/app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
                  appConfig-default.json -d /usr/work/app-packages/storm
                  unzip /usr/work/app-packages/storm/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip
                  resources-default.json -d /usr/work/app-packages/storm
                </code></pre>
                <p>You can use the following commands to copy and rename the default Storm application and
                  resource specification files in the
                  <code>/usr/work/app-packages/storm</code>
                  directory:
                </p>
                <pre><code>cp /usr/work/app-packages/storm/appConfig-default.json /usr/work/app-packages/storm/appConfig.json
                  cp /usr/work/app-packages/storm/resources-default.json /usr/work/app-packages/storm/resources.json
                </code></pre>
              
            
            
              <span class="title"><strong>Application Configuration for Storm on YARN</strong></span>
              
                <p>The basic properties to adjust are the heapsize parameters for Storm daemons such as nimbus,
                  supervisor, UI, and worker childopts to fit your system. Memory considerations are discussed in the
                  "Deployment Considerations" section of this guide.

                  The following is an example of a Storm
                  <code>appConfig.json</code>
                  file:
                </p>
                <pre><code>{
                  "schema": "http://example.org/specification/v2.0.0",
                  "metadata": {
                  },
                  "global": {
                  "application.def": ".slider/package/STORM/slider-storm-app-package-0.9.3.2.2.0.0-2041.zip",
                  "java_home": "/usr/jdk64/jdk1.7.0_67",
                  "create.default.zookeeper.node": "true",
                  "system_configs": "core-site",

                  "site.global.app_user": "yarn",
                  "site.global.app_root": "${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-2041",
                  "site.global.user_group": "hadoop",
                  "site.global.ganglia_server_host": "${NN_HOST}",
                  "site.global.ganglia_server_id": "Application2",
                  "site.global.ganglia_enabled":"true",
                  "site.global.ganglia_server_port": "8668",

                  "site.storm-site.storm.log.dir" : "${AGENT_LOG_ROOT}",
                  "site.storm-site.storm.zookeeper.servers": "['${ZK_HOST}']",
                  "site.storm-site.nimbus.thrift.port": "${NIMBUS.ALLOCATED_PORT}",
                  "site.storm-site.storm.local.dir": "${AGENT_WORK_ROOT}/app/tmp/storm",
                  "site.storm-site.transactional.zookeeper.root": "/transactional",
                  "site.storm-site.storm.zookeeper.port": "2181",
                  "site.storm-site.nimbus.childopts": "-Xmx1024m
                  -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-2041/external/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=${@//site/global/ganglia_server_host},port=${@//site/global/ganglia_server_port},wireformat31x=true,mode=multicast,config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-908/external/storm-jmxetric/conf/jmxetric-conf.xml,process=Nimbus_JVM",
                  "site.storm-site.worker.childopts": "-Xmx768m
                  -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-2041/external/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=${@//site/global/ganglia_server_host},port=${@//site/global/ganglia_server_port},wireformat31x=true,mode=multicast,config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-908/external/storm-jmxetric/conf/jmxetric-conf.xml,process=Worker_%ID%_JVM",
                  "site.storm-site.dev.zookeeper.path": "${AGENT_WORK_ROOT}/app/tmp/dev-storm-zookeeper",
                  "site.storm-site.drpc.invocations.port": "0",
                  "site.storm-site.storm.zookeeper.root": "${DEFAULT_ZK_PATH}",
                  "site.storm-site.transactional.zookeeper.port": "null",
                  "site.storm-site.nimbus.host": "${NIMBUS_HOST}",
                  "site.storm-site.ui.port": "${STORM_UI_SERVER.ALLOCATED_PORT}",
                  "site.storm-site.supervisor.slots.ports":
                  "[${SUPERVISOR.ALLOCATED_PORT}{PER_CONTAINER},${SUPERVISOR.ALLOCATED_PORT}{PER_CONTAINER}]",
                  "site.storm-site.supervisor.childopts": "-Xmx256m -Dcom.sun.management.jmxremote
                  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
                  -Dcom.sun.management.jmxremote.port=0
                  -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-2041/external/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=${NN_HOST},port=8668,wireformat31x=true,mode=multicast,config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-908/external/storm-jmxetric/conf/jmxetric-conf.xml,process=Supervisor_JVM",
                  "site.storm-site.drpc.port": "0",
                  "site.storm-site.logviewer.port": "${SUPERVISOR.ALLOCATED_PORT}{PER_CONTAINER}"
                  },
                  "components": {
                  "slider-appmaster": {
                  "jvm.heapsize": "256M"
                  }
                  }
                  }
                </code></pre>
                <aside class="custom-note">
                  <div class="icon"><img src="Icons/Note.png" width="50"></div>
                  <div class="simple-block">
                    <p>The user name for the
                      <code>site.global.app_user</code>
                      property should be the user name you will use to launch the Storm application instance.
                    </p>
                  </div>
                </aside>
              
            
            
              <span class="title"><strong>Resource Components in Storm on YARN</strong></span>
              
                <p>You can specify the following components (also referred to as "roles") when deploying Storm on
                  YARN via Slider:
                </p>
                <ul class="bullet-list">
                  
                    <li>
                      <p>
                        <code>NIMBUS</code>
                        ⎯ Storm Nimbus process.
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>STORM_UI_SERVER</code>
                        ⎯ Storm Web UI process.
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>DRPC_SERVER</code>
                        ⎯ Storm DRPC Server.
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>SUPERVISOR</code>
                        ⎯ Storm Supervisor process.
                      </p>
                    </li>
                  
                </ul>
                <p>The following is an example of a Storm
                  <code>resources.json</code>
                  file with these roles configured:
                </p>
                <pre><code>{
                  "schema" : "http://example.org/specification/v2.0.0",
                  "metadata" : {
                  },
                  "global" : {
                  "yarn.log.include.patterns": "",
                  "yarn.log.exclude.patterns": "",
                  "yarn.log.interval": "0"
                  },
                  "components": {
                  "slider-appmaster": {
                  },
                  "NIMBUS": {
                  "yarn.role.priority": "1",
                  "yarn.component.instances": "1",
                  "yarn.memory": "2048",
                  "yarn.label.expression":"storm1_nimbus_label"
                  },
                  "STORM_UI_SERVER": {
                  "yarn.role.priority": "2",
                  "yarn.component.instances": "1",
                  "yarn.memory": "1278",
                  "yarn.label.expression":"storm1_nimbus_label"
                  },
                  "DRPC_SERVER": {
                  "yarn.role.priority": "3",
                  "yarn.component.instances": "1",
                  "yarn.memory": "1278",
                  “"yarn.label.expression":"storm1_nimbus_label"
                  },
                  "SUPERVISOR": {
                  "yarn.role.priority": "4",
                  "yarn.component.instances": "1",
                  "yarn.memory": "3072",
                  "yarn.label.expression":"storm1_supervisor_label"
                  }
                  }
                  }
                </code></pre>
                <p>The memory and number of instances of
                  <code>SUPERVISOR</code>
                  should be adjusted for your system and desired application instance size. By default
                  <code>SUPERVISOR</code>
                  has two worker ports. If you allocate more workers per
                  <code>SUPERVISOR</code>
                  you should also adjust the
                  <code>yarn.memory</code>
                  vaule in the
                  <code>SUPERVISOR</code>
                  section of the
                  <code>resources.json</code>
                  file.

                  For all of the other roles (<code>NIMBUS</code>,<code>DRPC_SERVER</code>,
                  <code>STORM_UI_SERVER</code>), you should configure only one instance.
                </p>
              
            
          
          
            <h4 class="bold">Configuring Storm on YARN on Secure Clusters</h4>
            
              <span class="title"><strong>1. Generate and Distribute Keytab Files for Storm</strong></span>
              
                <p>On the KDC host:</p>
                <ul class="number-list">
                  
                    <li>
                      <p>Log in as root.</p>
                    </li>
                    <li>
                      <p>Create a Storm server principal for each NodeManager host:</p>
                      <pre><code>kadmin.local -q "addprinc -randkey &lt;server_principal_name&gt;/&lt;node_manager_host_name&gt;@EXAMPLE.COM"</code></pre>
                    </li>
                    <li>
                      <p>Create a Storm client principal:</p>
                      <pre><code>kadmin.local -q "addprinc -randkey &lt;client_principal_name&gt;@EXAMPLE.COM"</code></pre>
                    </li>
                    <li>
                      <p>Export the principals to a keytab file:</p>
                      <ul class="Bullet">
                        
                          <li>
                            <p>For each NodeManager host:</p>
                            <pre><code>kadmin.local -q "xst -norandkey -k /etc/security/keytabs/nimbus.keytab &lt;server_principal_name&gt;/&lt;node_manager_host_name&gt;@EXAMPLE.COM"</code></pre>
                          </li>
                          <li>
                            <p>For the Storm client principal:</p>
                            <pre><code>kadmin.local -q "xst -norandkey -k /etc/security/keytabs/storm.keytab &lt;client_principal_name&gt;@EXAMPLE.COM"</code></pre>
                          </li>
                        
                      </ul>
                    </li>
                    <li>
                      <p>Distribute the keytab file to the NodeManager hosts on which the application components
                        will be launched. Be sure to set the permissions so that the runtime elements are allowed to
                        access the keytab files, for example:
                      </p>
                      <pre><code>sudo su -
                        chown root:users &lt;keytab_file&gt;
                        chmod 440 &lt;keytab_file&gt;</code></pre>
                    </li>
                  
                </ul>
              
            
            
              <span class="title"><strong>2. Add an OS User for HDFS Access</strong></span>
              
                <p>You need to add an OS user for proper HDFS access (user and group availability) to the secure
                  Storm deployment:
                </p>
                <ul class="number-list">
                  
                    <li>
                      <p>Create system users with the same short names as the generated server principal and
                        client principal:
                      </p>
                      <pre><code>useradd -n &lt;storm server or client principal short name&gt;
                        passwd &lt;storm server or client principal short name&gt;</code></pre>
                      <p>You must specify a password in order to activate the user account.</p>
                    </li>
                    <li>
                      <p>Associate the users to the appropriate user groups:</p>
                      <pre><code>usermod -a -G hadoop &lt;storm server or client principal short name&gt;</code></pre>
                    </li>
                  
                </ul>
              
            
            
              <span class="title"><strong>Edit the Secure Version of the Application Configuration Files</strong></span>
              
                <p>As previously mentioned, the Storm-on-Slider application package includes both non-secure (
                  <code>appConfig-default.json</code>) and secure (<code>
                    appConfig-secured-default.json</code>) versions of the application specification.

                  On secure clusters, you should use the secure version of the application specification. The
                  security-related entries in the
                  <code>appConfig-secured.json</code>
                  file are listed below.
                </p>
                <pre><code>"site.storm-site.nimbus.authorizer":
                  "backtype.storm.security.auth.authorizer.SimpleACLAuthorizer",
                  "site.storm-site.storm.thrift.transport":
                  "backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin",
                  "site.storm-site.java.security.auth.login.config":
                  "${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.3.2.2.0.0-2041/conf/storm_jaas.conf",
                  "site.storm-site.storm.principal.tolocal": "backtype.storm.security.auth.KerberosPrincipalToLocal",
                  "site.storm-site.storm.zookeeper.superACL": "sasl:storm",
                  "site.storm-site.nimbus.admins": "['jon', 'storm']",
                  "site.storm-site.nimbus.supervisor.users": "['storm']",
                  "site.storm-site.nimubs.authorizer": "backtype.storm.security.auth.authorizer.SimpleACLAuthorizer",
                  "site.storm-site.storm.thrift.transport":
                  "backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin",
                  "site.storm-site.storm.principal.tolocal": "backtype.storm.security.auth.KerberosPrincipalToLocal",
                  "site.storm-site.ui.filter": "org.apache.hadoop.security.authentication.server.AuthenticationFilter",
                  "site.storm-site.ui.filter.params": "{'type': 'kerberos', 'kerberos.principal': 'HTTP/_HOST',
                  'kerberos.keytab': '/etc/security/keytabs/spnego.service.keytab', 'kerberos.name.rules':
                  'RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/
                  RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT'}",
                  "site.storm-env.kerberos_domain": "EXAMPLE.COM",
                  "site.storm-env.storm_client_principal_name": "storm@EXAMPLE.COM",
                  "site.storm-env.storm_server_principal_name": "storm_server/_HOST@EXAMPLE.COM",
                  "site.storm-env.storm_client_keytab": "/etc/security/keytabs/storm.keytab",
                  "site.storm-env.storm_server_keytab": "/etc/security/keytabs/nimbus.keytab"
                </code></pre>
                <ul class="bullet-list">
                  Some key points regarding these configuration properties:
                  
                    <li>
                      <p>The properties assume the use of the Kerberos domain "EXAMPLE.COM". Change the domain
                        name to match the name configured for your environment.
                      </p>
                    </li>
                    <li>
                      <p>The JAAS configuration (<code>storm_jaas.conf</code>) path will be dependent
                        on the version of the Storm distribution you are using (for example, version
                        apache-storm-0.9.3.2.2.0.0-2041 as shown above).
                      </p>
                    </li>
                    <li>
                      <p>The
                        <code>superACL</code>
                        property should point to the client principal short name.
                      </p>
                    </li>
                    <li>
                      <p>The
                        <code>nimbus.admins</code>
                        property values should include both the Storm client principal short name and the principal
                        associated with the Slider user who launches the application.
                      </p>
                    </li>
                    <li>
                      <p>The
                        <code>supervisor.users</code>
                        property should be set to the short name of the Storm client principal.
                      </p>
                    </li>
                    <li>
                      <p>The
                        <code>ui.filter.params</code>
                        property requires an HTTP/Web principal. This principle can be found in the
                        <code>spnego.service.keytab</code>
                        file.
                      </p>
                    </li>
                    <li>
                      <p>The
                        <code>storm-env</code>
                        properties are fairly straightforward -- simply provide the server principal, client principal,
                        and keytab file locations.
                      </p>
                    </li>
                  
                </ul>
                <p>At this point you should be ready to launch a Storm cluster using the Slider
                  <code>create</code>
                  command. You will need to authenticate against Kerberos and obtain a TGT using the
                  <code>kinit</code>
                  command prior to invoking the Slider
                  <code>create</code>
                  command:
                </p>
                <pre><code>kinit &lt;user name&gt;</code></pre>
                <aside class="custom-note">
                  <div class="icon"><img src="Icons/Note.png" width="50"></div>
                  <div class="simple-block">
                    <p>For more information on configuring Storm-on-Slider on secure clusters, including
                      information about keytab-associated properties and the available keytab distribution options, see
                      <a href="http://slider.incubator.apache.org/docs/security.html">Apache Slider
                        Security</a>.
                      
                    </p>
                  </div>
                </aside>
              
            
          
          
            <h4 class="bold">Launching a Storm Application Instance</h4>
            
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>The application instance can only be launched by the user specified in the
                    <code>site.global.app_user</code>
                    in the Storm
                    <code>appConfig.json</code>
                    file.
                  </p>
                </div>
              </aside>
              <p>Use the following command format to launch Storm:</p>
              <pre><code>su &lt;user&gt;
                /usr/hdp/current/slider-client/bin/./slider create &lt;storm_cluster_name&gt; --template appConfig.json
                --resources resources.json
              </code></pre>
              <p>Where
                <code>&lt;user&gt;</code>
                is the user who installed the Storm application package.
              </p>
              <p>
                For example:
              </p>
              <pre><code>su &lt;user&gt;
                /usr/hdp/current/slider-client/bin/./slider create storm1 --template
                /usr/work/app-packages/storm/appConfig.json --resources /usr/work/app-packages/storm/resources.json
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>Uppercase characters are not allowed in the cluster name.</p>
                </div>
              </aside>
            
            
              <p>You can use the Slider CLI
                <code>status</code>
                command to verify the application launch:
              </p>
              <pre><code>/usr/hdp/current/slider-client/bin/./slider status &lt;application_name&gt;</code></pre>
            
            
              <p>The successful launch of the Storm application can also be verified via the YARN Resource
                Manager Web UI. This UI is usually accessible via a Web browser at port 8088 of the Resource Manager
                Host:
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/Slider/Storm_on_Slider_RM_page.png" width="50">
                  
                
              </div>
              <p>The specific information for the running application is accessible via the “ApplicationMaster”
                link that can be seen in the far right column of the row associated with the running application
                (probably the top row):
              </p>
              <div class="figure">
                
                  
                    
                    <img src="01-RawContent/Slider/Slider_Storm_App_Master.png" width="50">
                  
                
              </div>
            
          
          
            <h4 class="bold">Deployment Considerations</h4>
            
              <span class="title"><strong>Memory Considerations for Running One Component on a Node</strong></span>
              
                <p>You can adjust the amount of memory given to a component to achieve mutual exclusion of
                  components, depending upon the NodeManager configuration on each node. Typically all nodes have the
                  same value independent of the actual memory.

                  Assuming the memory capacity for each NodeManager is known (<code>
                    yarn.nodemanager.resource.memory-mb</code>), you can configure the component to ask for 51%
                  (basically more than half) of the maximum capacity. You also need to ensure that the maximum possible
                  memory allocation (<code>yarn.scheduler.maximum-allocation-mb</code>) allows that value.

                  For example, if
                  <code>yarn.nodemanager.resource.memory-mb</code>
                  =
                  <code>yarn.scheduler.maximum-allocation-mb</code>
                  = 2048
                  Set
                  <code>yarn.memory</code>
                  = 1280 for the
                  <code>nimbus.childopts</code>
                  property in the appConfig.json file.

                  For
                  <code>SUPERVISOR</code>
                  allocate 1/4th of 1280 to
                  <code>supervisor.childopts</code>
                  and depending on how many workers you plan to run, divide the rest of the available container memory
                  by the number of workers and adjust
                  <code>worker.childopts</code>
                  accordingly.
                </p>
              
            
            
              <span class="title"><strong>Log Aggregation</strong></span>
              
                <p>This feature is backed by<a href="https://issues.apache.org/jira/browse/YARN-2468">
                  https://issues.apache.org/jira/browse/YARN-2468</a>.

                  Log aggregation is specified in the
                  <code>global</code>
                  section of<code>resources.json</code>:
                </p>
                <pre><code>"global": {
                  "yarn.log.include.patterns": "",
                  "yarn.log.exclude.patterns": "",
                  "yarn.log.interval": "0"
                  },
                </code></pre>
                <p>The
                  <code>yarn.log.interval</code>
                  unit is seconds.

                  You can specify the name(s) of log files (for example,<code>agent.log</code>) that you do
                  not want to aggregate using<code>yarn.log.exclude.patterns</code>.

                  The aggregated logs are stored in the HDFS
                  <code>/app-logs/</code>
                  directory.

                  The following command can be used to retrieve the logs:
                </p>
                <pre><code>yarn logs -applicationId &lt;app_id&gt;</code></pre>
                <p>For Storm you should exclude all active logs. Any file under
                  <code>storm.log.dir/*.log</code>
                  and the
                  <code>storm.log.dir/metadata</code>
                  directory should be excluded. You should be collecting the rolled-over logs. So any file with
                  <code>*.log.%i</code>
                  is ready to be collected for log aggregation.

                  The following is an example of a Storm
                  <code>log4j.properties</code>
                  file:
                </p>
                <pre><code>&lt;configuration scan="true" scanPeriod="60 seconds"&gt;
                  &lt;appender name="A1" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
                  &lt;file&gt;${storm.log.dir}/${logfile.name}&lt;/file&gt;
                  &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt;
                  &lt;fileNamePattern&gt;${storm.log.dir}/${logfile.name}.%i&lt;/fileNamePattern&gt;
                  &lt;minIndex&gt;1&lt;/minIndex&gt;
                  &lt;maxIndex&gt;9&lt;/maxIndex&gt;
                  &lt;/rollingPolicy&gt;

                  &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt;
                  &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt;
                  &lt;/triggeringPolicy&gt;

                  &lt;encoder&gt;
                  &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} %c{1} [%p] %m%n&lt;/pattern&gt;
                  &lt;/encoder&gt;
                  &lt;/appender&gt;

                  &lt;appender name="ACCESS" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
                  &lt;file&gt;${storm.log.dir}/access.log&lt;/file&gt;
                  &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt;
                  &lt;fileNamePattern&gt;${storm.log.dir}/access.log.%i&lt;/fileNamePattern&gt;
                  &lt;minIndex&gt;1&lt;/minIndex&gt;
                  &lt;maxIndex&gt;9&lt;/maxIndex&gt;
                  &lt;/rollingPolicy&gt;

                  &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt;
                  &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt;
                  &lt;/triggeringPolicy&gt;

                  &lt;encoder&gt;
                  &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} %c{1} [%p] %m%n&lt;/pattern&gt;
                  &lt;/encoder&gt;
                  &lt;/appender&gt;

                  &lt;appender name="METRICS" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
                  &lt;file&gt;${storm.log.dir}/metrics.log&lt;/file&gt;
                  &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt;
                  &lt;fileNamePattern&gt;${storm.log.dir}/logs/metrics.log.%i&lt;/fileNamePattern&gt;
                  &lt;minIndex&gt;1&lt;/minIndex&gt;
                  &lt;maxIndex&gt;9&lt;/maxIndex&gt;
                  &lt;/rollingPolicy&gt;

                  &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt;
                  &lt;maxFileSize&gt;2MB&lt;/maxFileSize&gt;
                  &lt;/triggeringPolicy&gt;

                  &lt;encoder&gt;
                  &lt;pattern&gt;%d %-8r %m%n&lt;/pattern&gt;
                  &lt;/encoder&gt;
                  &lt;/appender&gt;

                  &lt;root level="INFO"&gt;
                  &lt;appender-ref ref="A1"/&gt;
                  &lt;/root&gt;

                  &lt;logger name="backtype.storm.security.auth.authorizer" additivity="false"&gt;
                  &lt;level value="INFO" /&gt;
                  &lt;appender-ref ref="ACCESS" /&gt;
                  &lt;/logger&gt;

                  &lt;logger name="backtype.storm.metric.LoggingMetricsConsumer" additivity="false" &gt;
                  &lt;level value="INFO"/&gt;
                  &lt;appender-ref ref="METRICS"/&gt;
                  &lt;/logger&gt;

                  &lt;/configuration&gt;</code></pre>
              
            
            
              <span class="title"><strong>Reserving Nodes for Storm</strong></span>
              
                <p>You can use YARN node labels to reserve cluster nodes for applications and their components.
                  You could use node labels to reserve cluster nodes for Storm to ensure that NIMBUS and SUPERVISOR
                  provide a consistent performance level.

                  Node labels are specified with the
                  <code>yarn.label.expression</code>
                  property. If no label is specified, only non-labeled nodes are used when allocating containers for
                  component instances.

                  A brief summary is that you could label particular YARN nodes for Storm, say with labels
                  “storm1_nimbus” for nimbus and “storm1_supervisor” for supervisors, and create a separate queue for
                  assigning containers to these nodes.
                  “STORM_UI_SERVER” can run in the same label as storm1_nimbus.
                  “storm1_drpc” label should be for DRPC.
                  To use these labeled nodes, you would add yarn.label.expression parameters to the Storm components in
                  your resources.json file (including the slider-appmaster), e.g. "yarn.label.expression":
                  "storm1_supervisor". When you run the “slider create” command for your Storm cluster, you would add
                  the parameter “--queue &lt;queue name&gt;”.
                </p>
              
            
            
              <span class="title"><strong>Running Example Topologies on Storm-on-YARN</strong></span>
              
                <span class="title"><strong>Installing the Storm-Slider Client</strong></span>
                
                  <ul class="bullet-list">
                    
                      <li>
                        <p>Make sure you are using HDP-2.2</p>
                      </li>
                      <li>
                        <p>
                          <code>yum search storm</code>
                        </p>
                      </li>
                      <li>
                        <p>
                          <code>storm_&lt;latest_version&gt;-slider-client</code>
                        </p>
                      </li>
                      <li>
                        <p>Pick the latest package</p>
                      </li>
                      <li>
                        <p>
                          <code>yum install storm_&lt;latest_version&gt;-slider-client</code>
                        </p>
                      </li>
                      <li>
                        <p>edit
                          <code>/etc/storm-slider-client/conf/storm-slider-env.sh</code>
                        </p>
                      </li>
                      <li>
                        <p>
                          <code>export SLIDER_HOME=/usr/hdp/current/slider</code>
                        </p>
                      </li>
                      <li>
                        <p>
                          <code>export JAVA_HOME=/usr/jdk64/jdk1.7</code>
                        </p>
                      </li>
                    
                  </ul>
                
              
              
                <span class="title"><strong>Using Quicklinks to Access the Storm UI</strong></span>
                
                  <p>Use the following command format:</p>
                  <pre><code>storm-slider --app &lt;yarn-app-name&gt; quicklinks</code></pre>
                  <p>For example:</p>
                  <pre><code>/usr/hdp/&lt;hdp_version&gt;/storm-slider-client/bin/storm-slider --app cl2 quicklinks</code></pre>
                  <p>Example output:</p>
                  <pre><code>{
                    "org.apache.slider.jmx" :
                    "http://ec2-54-172-39-242.compute-1.amazonaws.com:34579/api/v1/cluster/summary",
                    "org.apache.slider.metrics" :
                    "http://ec2-54-172-207-11.compute-1.amazonaws.com/cgi-bin/rrd.py?c=Application2",
                    "nimbus.host_port" : "http://ec2-54-172-39-242.compute-1.amazonaws.com:38818",
                    "org.apache.slider.monitor" : "http://ec2-54-172-39-242.compute-1.amazonaws.com:34579",
                    "org.apache.slider.metrics.ui" :
                    "http://ec2-54-172-207-11.compute-1.amazonaws.com/ganglia?c=Application2"
                    }
                  </code></pre>
                  <p>
                    <code>org.apache.slider.metrics.monitor</code>
                    points to the Storm UI.
                  </p>
                
              
              
                <span class="title"><strong>Deploy the Storm Word Count Topology</strong></span>
                
                  <p>Use the following command to deploy the Storm wordcount topology:</p>
                  <pre><code>/usr/hdp/current/storm-slider-client/bin/storm-slider --app &lt;cluster_name&gt; jar
                    /usr/hdp/current/storm-slider-client/contrib/storm-starter/storm-starter-topologies-*.jar
                    storm.starter.WordCountTopology wordcount
                  </code></pre>
                  <p>Replace
                    <code>&lt;cluster_name&gt;</code>
                    with the cluster name deployed using Slider.
                  </p>
                
              
              
                <span class="title"><strong>Tests</strong></span>
                
                  <ul class="bullet-list">
                    
                      <li>
                        <p>Deploy wordcount topology and go to the Storm UI. Check to see if wordcount topology
                          is listed under topologies. Click on wordcount topology and check to see if emitted and
                          transferred numbers are increasing.
                        </p>
                      </li>
                      <li>
                        <p>Start multiple Storm clusters, then deploy wordcount topologies on each cluster and
                          check to see if they are running.
                        </p>
                      </li>
                      <li>
                        <p>Check to see if the labeling of torm components is working by listing out the NIMBUS
                          process on the storm1_nimbus labeled node.
                        </p>
                      </li>
                    
                  </ul>
                
              
            
          
        
        
          <h4 class="bold">Running Accumulo on YARN via Slider</h4>
          
            <p>This guide describes how to install and configure Accumulo on YARN using Slider.</p>
          
          
            <h4 class="bold">Introduction</h4>
            
              <p>Running Accumulo on YARN via Slider offers the following advantages:</p>
              <ul class="bullet-list">
                
                  <li>
                    <p>Easier installation and start-up.</p>
                  </li>
                  <li>
                    <p>You can run multiple instances of Accumulo on one Hadoop cluster.</p>
                  </li>
                  <li>
                    <p>Process management (start/stop) is much easier with Slider.</p>
                  </li>
                  <li>
                    <p>You can run different versions of Accumulo on the same Hadoop cluster.</p>
                  </li>
                  <li>
                    <p>You can take advantage of YARN features such as container restart and allocation
                      flexibility.
                    </p>
                  </li>
                
              </ul>
            
          
          
            <h4 class="bold">Downloading and Installing the Accumulo Application Package</h4>
            
              <ul class="number-list">
                
                  <strong>To install the Accumulo Application Package:</strong>
                
                
                  <li>
                    <p>Make a working directory for the Accumulo-on-Slider application package:</p>
                    <pre><code>mkdir -p /usr/work/app-packages/accumulo</code></pre>
                  </li>
                  <li>
                    <p>Download the Accumulo-on-Slider application package for your operating system to the
                      <code>/usr/work/app-packages/accumulo</code>
                      directory. For example, use the following command to download the centos6 version:
                    </p>
                    <pre><code>cd /usr/work/app-packages/accumulo
                      wget
                      http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0/slider-app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                    </code></pre>
                  </li>
                  <li>
                    <p>Use the following command format to install the application package on YARN:</p>
                    <pre><code>su &lt;user&gt;
                      /usr/hdp/current/slider-client/bin/./slider install-package --name ACCUMULO --package &lt;package
                      path&gt;</code></pre>
                    <p>Where
                      <code>&lt;user&gt;</code>
                      is the user who will create the Accumulo application instance.

                      For example, if user "hw_qa" downloaded the Accumulo package to the
                      <code>/usr/work/app-packages/accumulo</code>
                      folder:
                    </p>
                    <pre><code>su hw_qa
                      /usr/hdp/current/slider-client/bin/./slider install-package --name ACCUMULO --package
                      /usr/work/app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                    </code></pre>
                    <p>Note that the
                      <code>--name</code>
                      (in this case, "ACCUMULO") should match the value of the
                      <code>&lt;name&gt;</code>
                      element in the package
                      <code>metainfo.xml</code>
                      flle, and that the install destination in HDFS is in the user's home directory under
                      ./slider/package/ACCUMULO/*.zip
                    </p>
                  </li>
                
              </ul>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>Accumulo on Slider is not supported on Windows, as Accumulo is not currently supported on
                    Windows.
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Configuring Accumulo on YARN</h4>
            
              <span class="title"><strong>Accessing the Accumulo Configuration Files</strong></span>
              
                <p>The Accumulo application package includes default application and resource specification
                  files. The package includes both non-secure (<code>appConfig-default.json</code>) and
                  secure (<code>appConfig-secured-default.json</code>) versions of the application
                  specification. You can save these files as a another name, and then edit the files to customize the
                  Accumulo configuration.

                  You can use the
                  <code>unzip</code>
                  command to extract the Accumulo application and resource specification files from the
                  Accumulo-on-Slider application package. For example, you would use the following command to extract
                  the files from the Accumulo application package in the
                  <code>/usr/work/app-packages/accumulo</code>
                  directory:
                </p>
                <pre><code>unzip /usr/work/app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                  appConfig-default.json -d /usr/work/app-packages/accumulo
                  unzip /usr/work/app-packages/accumulo/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip
                  resources-default.json -d /usr/work/app-packages/accumulo
                </code></pre>
                <p>You can use the following commands to copy and rename the default Storm application and
                  resource specification files in the
                  <code>/usr/work/app-packages/accumulo</code>
                  directory:
                </p>
                <pre><code>cp /usr/work/app-packages/accumulo/appConfig-default.json
                  /usr/work/app-packages/accumulo/appConfig.json
                  cp /usr/work/app-packages/accumulo/resources-default.json
                  /usr/work/app-packages/accumulo/resources.json
                </code></pre>
              
            
            
              <span class="title"><strong>Application Configuration for Accumulo on YARN</strong></span>
              
                <p>The following is an example of an
                  <code>appConfig.json</code>
                  file for Accumulo on YARN via Slider. The basic properties to adjust for your system are the heap
                  size, the Accumulo memory properties, and the location of JAVA_HOME. The directories and classpaths
                  are configured properly for HDP in the default
                  <code>appConfig-default.json</code>
                  file, but you must set the JAVA_HOME value in the "global" section of the
                  <code>appConfig.json</code>
                  file to match your system JAVA_HOME setting.
                </p>
                <pre><code>{
                  "schema": "http://example.org/specification/v2.0.0",
                  "metadata": {
                  },
                  "global": {
                  "application.def": ".slider/package/ACCUMULO/slider-accumulo-app-package-1.6.1.2.2.0.0-2041.zip",
                  "java_home": "/usr/hadoop-jdk1.6.0_31",

                  "site.global.app_root": "${AGENT_WORK_ROOT}/app/install/accumulo-1.6.1.2.2.0.0-2041",
                  "site.global.app_user": "${USER}",
                  "site.global.user_group": "hadoop",

                  "site.accumulo-env.java_home": "${JAVA_HOME}",
                  "site.accumulo-env.tserver_heapsize": "256m",
                  "site.accumulo-env.master_heapsize": "128m",
                  "site.accumulo-env.monitor_heapsize": "64m",
                  "site.accumulo-env.gc_heapsize": "64m",
                  "site.accumulo-env.other_heapsize": "128m",
                  "site.accumulo-env.hadoop_prefix": "/usr/hdp/current/hadoop-client",
                  "site.accumulo-env.hadoop_conf_dir": "/etc/hadoop/conf",
                  "site.accumulo-env.zookeeper_home": "${zk.dir}",

                  "site.client.instance.name": "${USER}-${CLUSTER_NAME}",

                  "site.global.accumulo_root_password": "NOT_USED",
                  "site.global.ssl_cert_dir": "ssl",
                  "site.global.monitor_protocol": "http",

                  "site.accumulo-site.instance.volumes": "${DEFAULT_DATA_DIR}/data",
                  "site.accumulo-site.instance.zookeeper.host": "${ZK_HOST}",
                  "site.accumulo-site.instance.security.authenticator":
                  "org.apache.slider.accumulo.CustomAuthenticator",

                  "site.accumulo-site.general.security.credential.provider.paths":
                  "jceks://hdfs/user/${USER}/accumulo-${CLUSTER_NAME}.jceks",
                  "site.accumulo-site.instance.rpc.ssl.enabled": "false",
                  "site.accumulo-site.instance.rpc.ssl.clientAuth": "false",
                  "site.accumulo-site.general.kerberos.keytab": "",
                  "site.accumulo-site.general.kerberos.principal": "",

                  "site.accumulo-site.tserver.memory.maps.native.enabled": "false",
                  "site.accumulo-site.tserver.memory.maps.max": "80M",
                  "site.accumulo-site.tserver.cache.data.size": "7M",
                  "site.accumulo-site.tserver.cache.index.size": "20M",
                  "site.accumulo-site.tserver.sort.buffer.size": "50M",
                  "site.accumulo-site.tserver.walog.max.size": "40M",

                  "site.accumulo-site.trace.user": "root",

                  "site.accumulo-site.master.port.client": "0",
                  "site.accumulo-site.trace.port.client": "0",
                  "site.accumulo-site.tserver.port.client": "0",
                  "site.accumulo-site.gc.port.client": "0",
                  "site.accumulo-site.monitor.port.client": "${ACCUMULO_MONITOR.ALLOCATED_PORT}",
                  "site.accumulo-site.monitor.port.log4j": "0",
                  "site.accumulo-site.master.replication.coordinator.port": "0",
                  "site.accumulo-site.replication.receipt.service.port": "0",

                  "site.accumulo-site.general.classpaths":
                  "$ACCUMULO_HOME/lib/accumulo-server.jar,\n$ACCUMULO_HOME/lib/accumulo-core.jar,\n$ACCUMULO_HOME/lib/accumulo-start.jar,\n$ACCUMULO_HOME/lib/accumulo-fate.jar,\n$ACCUMULO_HOME/lib/accumulo-proxy.jar,\n$ACCUMULO_HOME/lib/[^.].*.jar,\n$ZOOKEEPER_HOME/zookeeper[^.].*.jar,\n$HADOOP_CONF_DIR,\n$HADOOP_PREFIX/[^.].*.jar,\n$HADOOP_PREFIX/lib/[^.].*.jar,\n$HADOOP_PREFIX/share/hadoop/common/.*.jar,\n$HADOOP_PREFIX/share/hadoop/common/lib/.*.jar,\n$HADOOP_PREFIX/share/hadoop/hdfs/.*.jar,\n$HADOOP_PREFIX/share/hadoop/m
                  <strong></strong>
                  apreduce/.*.jar,\n$HADOOP_PREFIX/share/hadoop/yarn/.*.jar,\n/usr/hdp/current/hadoop-client/.*.jar,\n/usr/hdp/current/hadoop-client/lib/.*.jar,\n/usr/hdp/current/hadoop-hdfs-client/.*.jar,\n/usr/hdp/current/hadoop-mapreduce-client/.*.jar,\n/usr/hdp/current/hadoop-yarn-client/.*.jar,"
                  },
                  "credentials": {
                  "jceks://hdfs/user/${USER}/accumulo-${CLUSTER_NAME}.jceks": ["root.initial.password",
                  "instance.secret", "trace.token.property.password"]
                  },
                  "components": {
                  "slider-appmaster": {
                  "jvm.heapsize": "256M",
                  "slider.am.keytab.local.path": "",
                  "slider.keytab.principal.name": ""
                  }
                  }
                  }
                </code></pre>
              
            
            
              <span class="title"><strong>Resource Components in Accumulo on YARN</strong></span>
              
                <p>You can specify the following components (also referred to as "roles") when deploying Accumulo
                  on YARN via Slider:
                </p>
                <ul class="bullet-list">
                  
                    <li>
                      <p>
                        <code>ACCUMULO_MASTER</code>
                        ⎯ Accumulo master process.
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>ACCUMULO_TSERVER</code>
                        ⎯ Accumulo tablet server process.
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>ACCUMULO_MONITOR</code>
                        ⎯ Accumulo monitor web UI
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>ACCUMULO_GC</code>
                        ⎯ Accumulo garbage collector process
                      </p>
                    </li>
                    <li>
                      <p>
                        <code>ACCUMULO_TRACER</code>
                        ⎯ Accumulo trace collector process
                      </p>
                    </li>
                  
                </ul>
                <p>The following is an example of an Accumulo
                  <code>resources.json</code>
                  file with these roles configured:
                </p>
                <pre><code>{
                  "schema": "http://example.org/specification/v2.0.0",
                  "metadata": {
                  },
                  "global": {
                  "yarn.log.include.patterns": "",
                  "yarn.log.exclude.patterns": ""
                  },
                  "components": {
                  "ACCUMULO_MASTER": {
                  "yarn.role.priority": "1",
                  "yarn.component.instances": "1",
                  "yarn.memory": "256"
                  },
                  "slider-appmaster": {
                  },
                  "ACCUMULO_TSERVER": {
                  "yarn.role.priority": "2",
                  "yarn.component.instances": "1",
                  "yarn.memory": "512"
                  },
                  "ACCUMULO_MONITOR": {
                  "yarn.role.priority": "3",
                  "yarn.component.instances": "1",
                  "yarn.memory": "128"
                  },
                  "ACCUMULO_GC": {
                  "yarn.role.priority": "4",
                  "yarn.component.instances": "1",
                  "yarn.memory": "128"
                  },
                  "ACCUMULO_TRACER": {
                  "yarn.role.priority": "5",
                  "yarn.component.instances": "1",
                  "yarn.memory": "256"
                  }
                  }
                  }
                </code></pre>
                <p>The memory and number of instances of each component should be adjusted for your system and
                  desired application instance size. You typically only need to request one instance of the<code>
                    ACCUMULO_MONITOR</code>,<code>ACCUMULO_GC</code>, and
                  <code>ACCUMULO_TRACER</code>
                  processes. For HA (High Availability) purposes, you will generally want two instances of<code>
                    ACCUMULO_MASTER</code>, and enough instances of
                  <code>ACCUMULO_TSERVER</code>
                  to support your application.
                </p>
              
            
          
          
            <h4 class="bold">Configuring Accumulo on YARN on Secure Clusters</h4>
            
              <p>As previously mentioned, the Accumulo-on-Slider application package includes both non-secure (
                <code>appConfig-default.json</code>) and secure (<code>
                  appConfig-secured-default.json</code>) versions of the application specification. On secure
                clusters, you should use the secure version of the application specification.

                To configure Accumulo for Kerberos, the following properties should be changed in the
                <code>appConfig.json</code>
                file to specify a keytab and principal for Accumulo processes to use. You must generate the keytabs on
                all the hosts and provide their location to Accumulo in the keytab property. If the keytab is not
                headless, the recommended form for the principal is<code>accumulo/_HOST@&lt;realm&gt;</code>
                , with the realm for your system. The Slider Application Master also needs a keytab and a principal,
                which can be (but does not need to be) the same one used for the Accumulo processes.
              </p>
              <pre><code>"global": {
                "site.accumulo-site.general.kerberos.keytab": "",
                "site.accumulo-site.general.kerberos.principal": "",
                },
                "components": {
                "slider-appmaster": {
                "slider.am.keytab.local.path": "",
                "slider.keytab.principal.name": ""
                }
                }

                should be changed to:

                "global": {
                "site.accumulo-site.general.kerberos.keytab": &lt;keytab file abs path&gt;,
                "site.accumulo-site.general.kerberos.principal": &lt;principal name&gt;,
                },
                "components": {
                "slider-appmaster": {
                "slider.am.keytab.local.path": &lt;keytab file abs path&gt;,
                "slider.keytab.principal.name": &lt;principal name&gt;
                }
                }
              </code></pre>
              <aside class="custom-note">
                <div class="icon"><img src="Icons/Note.png" width="50"></div>
                <div class="simple-block">
                  <p>For more information on configuring Accumulo-on-Slider on secure clusters, including
                    information about keytab-associated properties and the available keytab distribution options, see
                    <a href="http://slider.incubator.apache.org/docs/security.html">Apache Slider
                      Security</a>.
                    
                  </p>
                </div>
              </aside>
            
          
          
            <h4 class="bold">Launching an Accumulo Application Instance</h4>
            
              <span class="title"><strong>1. Iniatialize Accumulo Passwords</strong></span>
              
                <p>Before starting an Accumulo cluster on YARN, you must initialize the passwords needed by
                  Accumulo. These passwords are specified in the credentials section of the
                  <code>appConfig.json</code>
                  file. By default they are the
                  <code>root.initial.password</code>
                  (the initial password for the root user, which can be changed later using the Accumulo API), the
                  <code>instance.secret</code>
                  (a shared secret needed by Accumulo server processes), and the
                  <code>trace.token.property.password</code>
                  (the password for the user that collects traces, which is the root user in the default configuration —
                  so it should match the<code>root.initial.password</code>).

                  These passwords can be created using the following commands:
                </p>
                <pre><code>/usr/hdp/current/hadoop-client/bin/hadoop credential create root.initial.password -provider
                  jceks://hdfs/user/&lt;user&gt;/accumulo-&lt;accumulo_cluster_name&gt;.jceks -v &lt;root password&gt;
                  /usr/hdp/current/hadoop-client/bin/hadoop credential create instance.secret -provider
                  jceks://hdfs/user/&lt;user&gt;/accumulo-&lt;accumulo_cluster_name&gt;.jceks -v &lt;instance secret&gt;
                  /usr/hdp/current/hadoop-client/bin/hadoop credential create trace.token.property.password -provider
                  jceks://hdfs/user/&lt;user&gt;/accumulo-&lt;accumulo_cluster_name&gt;.jceks -v &lt;root password again&gt;</code></pre>
                <p>Where
                  <code>&lt;user&gt;</code>
                  is the user who will create the Accumulo application instance and
                  <code>&lt;accumulo_cluster_name&gt;</code>
                  is the name of the Accumulo cluster that will be used in the Slider
                  <code>create</code>
                  command.

                  For example, using the user "hw_qa" and the Accumulo cluster name "accumulo1":
                </p>
                <pre><code>/usr/hdp/current/hadoop-client/bin/hadoop credential create root.initial.password -provider
                  jceks://hdfs/user/hw_qa/accumulo-accumulo1.jceks -v testRootPassword1
                  /usr/hdp/current/hadoop-client/bin/hadoop credential create instance.secret -provider
                  jceks://hdfs/user/hw_qa/accumulo-accumulo1.jceks -v instanceSecret1
                  /usr/hdp/current/hadoop-client/bin/hadoop credential create trace.token.property.password -provider
                  jceks://hdfs/user/hw_qa/accumulo-accumulo1.jceks -v testRootPassword1
                </code></pre>
              
            
            
              <span class="title"><strong>2. Launch the Accumulo Application Instance</strong></span>
              
                <p>Use the following command format to launch Accumulo:</p>
                <pre><code>su &lt;user&gt;
                  /usr/hdp/current/slider-client/bin/./slider create &lt;accumulo_cluster_name&gt; --template
                  appConfig.json --resources resources.json
                </code></pre>
                <p>Where
                  <code>&lt;user&gt;</code>
                  and
                  <code>&lt;accumulo_cluster_name&gt;</code>
                  are the user and cluster name that were used when the Accumulo passwords were created.

                  For example:
                </p>
                <pre><code>su hw_qa
                  /usr/hdp/current/slider-client/bin/./slider create accumulo1 --template
                  /usr/work/app-packages/accumulo/appConfig.json --resources
                  /usr/work/app-packages/accumulo/resources.json
                </code></pre>
                <aside class="custom-note">
                  <div class="icon"><img src="Icons/Note.png" width="50"></div>
                  <div class="simple-block">
                    <p>Uppercase characters are not allowed in the cluster name.</p>
                  </div>
                </aside>
              
              
                <p>You can use the Slider CLI
                  <code>status</code>
                  command to verify the application launch:
                </p>
                <pre><code>/usr/hdp/current/slider-client/bin/./slider status &lt;accumulo_cluster_name&gt;</code></pre>
                <p>You can also use the Slider CLI
                  <code>list</code>
                  command to verify the application launch:
                </p>
                <pre><code>/usr/hdp/current/slider-client/bin/./slider list &lt;accumulo_cluster_name&gt;</code></pre>
                <p>The Accumulo instance name will have the format<code>&lt;user name&gt;-&lt;accumulo_cluster_name&gt;</code>
                  .
                </p>
              
              
                <p>The successful launch of the Accumulo cluster can also be verified via the YARN Resource
                  Manager Web UI. This UI is usually accessible via a Web browser at port 8088 of the Resource Manager
                  Host:
                </p>
                <div class="figure">
                  
                    
                      
                      <img src="01-RawContent/Slider/Accumulo_on_Slider_RM_page.png" width="50">
                    
                  
                </div>
                <p>The specific information for the running Accumulo cluster is accessible via the
                  “ApplicationMaster” link that can be seen in the far right column of the row associated with the
                  running application (probably the top row):
                </p>
                <div class="figure">
                  
                    
                      
                      <img src="System%20Admin%20Guides/accumulo_slider_app_master.png" width="50">
                    
                  
                </div>
                <p>
                  <strong>Accesssing the Accumulo Monitor</strong>

                  On the Slider App Master page, one of the links should end with<code>
                  publisher/slider</code>, for example:

                  <code>http://node-1.example.com:52413/ws/v1/slider/publisher/slider</code>

                  Add "quicklinks" to that URL, for example:

                  <code>http://node-1.example.com:52413/ws/v1/slider/publisher/slider/quicklinks</code>

                  That page will list some .json code. The value of the "org.apache.slider.monitor" key is the URL of
                  the Accumulo monitor page. If you go to the quicklinks page right after the application instance is
                  created, it will not be ready right away, because the monitor has to come up and register itself. But
                  if you keep reloading the quicklinks URL, it will come up eventually (if the cluster is healthy).
                </p>
              
            
          
          
            <h4 class="bold">Client Connections to Accumulo and Retrieving Effective
              <code>accumulo-site.xml</code>
            </h4>
            
              <p>The most efficient method for obtaining a client for Accumulo would be to install the Accumulo
                <code>.rpm</code>
                (<code>yum install accumulo</code>) and the Accumulo configuration .rpm (<code>yum
                  install accumulo-conf-standalone</code>). Then copy all of the Accumulo default configuration
                files from the
                <code>/usr/hdp/&lt;version_number&gt;/etc/accumulo/conf.dist/templates/</code>
                directory into your desired Accumulo configuration directory (e.g.<code>
                  /etc/accumulo/conf</code>).

                The next step is to obtain some configuration files from the registry for your Slider Accumulo cluster
                to enable your client to connect to the instance of Accumulo you created. Once Slider Accumulo is
                running, you can use the following commands to retrieve the necessary configurations from the registry
                into the Accumulo
                <code>conf</code>
                directory you are setting up.
              </p>
              <pre><code>slider registry --getconf accumulo-site --name &lt;cluster name&gt; --format xml --dest &lt;accumulo_conf_dir&gt;/accumulo-site.xml
                slider registry --getconf client --name &lt;cluster name&gt; --format properties --dest &lt;accumulo_conf_dir&gt;/client.conf
                slider registry --getconf accumulo-env --name &lt;cluster name&gt; --format json --dest
                accumulo-env.json
                python -c "import json; file = open('accumulo-env.json'); content = json.load(file); file.close(); print
                content['content']" &gt; &lt;accumulo_conf_dir&gt;/accumulo-env.sh
              </code></pre>
              <p>You should now be able to connect to Accumulo with the following command:</p>
              <pre><code>accumulo shell -u root</code></pre>
              <p>If there are multiple Slider Accumulo clusters running, you can connect to any of them from the
                same client installation. You still must download the configuration files from the registry for one
                running cluster, but you only need to do that once, and can then connect to other clusters with the
                following command:
              </p>
              <pre><code>accumulo shell -zh &lt;zookeeper host(s)&gt; -zi &lt;instance name&gt; -u root</code></pre>
              <p>Remember that the Accumulo instance name will be
                <code>&lt;user name&gt;-&lt;slider cluster name&gt;</code>
                if you are using the default setting in the
                <code>appConfig.json</code>
                file (which is strongly recommended).
              </p>
            
          
          
            <h4 class="bold">Deployment Considerations</h4>
            
              <span class="title"><strong>Memory Considerations for Running One Component on a Node</strong></span>
              
                <p>You can adjust the amount of memory given to a component to achieve mutual exclusion of
                  components, depending upon the NodeManager configuration on each node. Typically all nodes have the
                  same value independent of the actual memory.

                  Assuming the memory capacity for each NodeManager is known (<code>
                    yarn.nodemanager.resource.memory-mb</code>), you can configure the component to ask for 51%
                  (basically more than half) of the maximum capacity. You also need to ensure that the maximum possible
                  memory allocation (<code>yarn.scheduler.maximum-allocation-mb</code>) allows that value.

                  For example, if
                  <code>yarn.nodemanager.resource.memory-mb</code>
                  =
                  <code>yarn.scheduler.maximum-allocation-mb</code>
                  = 2048
                  Set
                  <code>yarn.memory</code>
                  = 1280 for the
                  <code>ACCUMULO_MASTER</code>
                  and
                  <code>ACCUMULO_TSERVER</code>
                  properties in the
                  <code>resources.json</code>
                  file.

                  Then in the
                  <code>appConfig.json</code>
                  file, set the
                  <code>ACCUMULO_MASTER</code>
                  and
                  <code>ACCUMULO_TSERVER</code>
                  heap sizes (including the
                  <code>ACCUMULO_TSERVER</code>
                  off-heap memory properties, if native maps are enabled) to be 256 MB less than the memory requested
                  for the YARN containers to allow for the agent memory consumption -- the agent should not use more
                  than 100 MB, but you can assume that it consumes ~256 MB. So you can set the
                  <code>ACCUMULO_MASTER</code>
                  and
                  <code>ACCUMULO_TSERVER</code>
                  variables for memory limit to fit within 1024 MB.
                </p>
              
            
            
              <span class="title"><strong>Log Aggregation</strong></span>
              
                <p>This feature is backed by<a href="https://issues.apache.org/jira/browse/YARN-2468">
                  https://issues.apache.org/jira/browse/YARN-2468</a>.

                  Log aggregation is specified in the
                  <code>yarn-site.xml</code>
                  file. The
                  <code>yarn.log-aggregation-enable</code>
                  property enables log aggregation for running applications. If a monitoring interval is also set, it
                  will aggregate logs while an application is running, with the specified interval. The minimum interval
                  is 3600 seconds.
                </p>
                <pre><code>  &lt;property&gt;
                  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
                  &lt;value&gt;true&lt;/value&gt;
                  &lt;/property&gt;
                  &lt;property&gt;
                  &lt;name&gt;yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds&lt;/name&gt;
                  &lt;value&gt;3600&lt;/value&gt;
                  &lt;/property&gt;</code></pre>
                <p>Log aggregation is specified in the
                  <code>global</code>
                  section of<code>resources.json</code>:
                </p>
                <pre><code>"global": {
                  "yarn.log.include.patterns": "",
                  "yarn.log.exclude.patterns": ""
                  },
                </code></pre>
                <p>If
                  <code>yarn.log.include.patterns</code>
                  is empty, all container logs are included. You can specify the name(s) of log files (for example,
                  <code>agent.log</code>) that you do not want to aggregate using<code>
                    yarn.log.exclude.patterns</code>.

                  The aggregated logs are stored in the HDFS
                  <code>/app-logs/</code>
                  directory. The following command can be used to retrieve the logs:
                </p>
                <pre><code>yarn logs -applicationId &lt;app_id&gt;</code></pre>
              
            
            
              <span class="title"><strong>Reserving Nodes for Accumulo</strong></span>
              
                <p>You can use YARN node labels to reserve cluster nodes for applications and their components.
                  You could use node labels to reserve cluster nodes for Accumulo to ensure that
                  <code>ACCUMULO_MASTER</code>
                  and
                  <code>ACCUMULO_TSERVER</code>
                  provide a consistent performance level.

                  Node labels are specified with the
                  <code>yarn.label.expression</code>
                  property. If no label is specified, only non-labeled nodes are used when allocating containers for
                  component instances.

                  A brief summary is that you could label particular YARN nodes for Accumulo, say with labels
                  “accumulo1” and “accumulo1_master”, and create a separate queue for assigning containers to these
                  nodes. To use these labeled nodes, you would add
                  <code>yarn.label.expression</code>
                  parameters to the Accumulo components in your
                  <code>resources.json</code>
                  file (including the slider-appmaster), e.g.<code>"yarn.label.expression":
                    "accumulo1_master"</code>. When you run the
                  <code>slider create</code>
                  command for your Accumulo cluster, you would add the parameter<code>“--queue
                    &lt;queue_name&gt;”</code>.
                </p>
              
            
            
              <span class="title"><strong>Configuring Accumulo for SSL</strong></span>
              
                <p>Accumulo can be configured to use SSL (Secure Sockets Layer) for its internal RPC
                  communications, and its monitor web UI can also be configured to use SSL. The Slider Accumulo
                  application package is set up to use the same SSL certs for both, although the features can be enabled
                  independently.

                  The SSL certificates must be provided. To distribute the certificates in HDFS, upload them to the
                  directory specified in
                  <code>“site.global.ssl_cert_dir”</code>
                  (by default,<code>/user/&lt;user name&gt;/ssl</code>). There should be a
                  <code>truststore.jks</code>
                  file and a
                  <code>.jks</code>
                  file for each host named<code>&lt;hostname&gt;.jks</code>. You must add the passwords for
                  the certificates to the credential provider by adding them to the list in the “credentials” section of
                  the
                  <code>appConfig.json</code>
                  file as shown below. To turn on SSL, set the Accumulo SSL properties below to<code>
                    true</code>. To turn on https for the monitor UI, change the
                  <code>monitor_protocol</code>
                  to<code>https</code>.

                  The properties are as follows:
                </p>
                <pre><code>"global": {
                  "site.global.ssl_cert_dir": "ssl",
                  "site.global.monitor_protocol": "http",
                  "site.accumulo-site.instance.rpc.ssl.enabled": "false",
                  "site.accumulo-site.instance.rpc.ssl.clientAuth": "false",
                  },
                  "credentials": {
                  "jceks://hdfs/user/${USER}/accumulo-${CLUSTER_NAME}.jceks": ["root.initial.password",
                  "instance.secret", "trace.token.property.password"]
                  },
                </code></pre>
                <p>Change these to:</p>
                <pre><code>"global": {
                  "site.global.ssl_cert_dir": "ssl",
                  "site.global.monitor_protocol": "https",
                  "site.accumulo-site.instance.rpc.ssl.enabled": "true",
                  "site.accumulo-site.instance.rpc.ssl.clientAuth": "true",
                  },
                  "credentials": {
                  "jceks://hdfs/user/${USER}/accumulo-${CLUSTER_NAME}.jceks": ["root.initial.password",
                  "instance.secret", "trace.token.property.password", "rpc.javax.net.ssl.keyStorePassword",
                  "rpc.javax.net.ssl.trustStorePassword", "monitor.ssl.keyStorePassword",
                  "monitor.ssl.trustStorePassword"]
                  },
                </code></pre>
                <p>If you would like to distribute the certs yourself rather than through HDFS, simply set the
                  following properties to the locations of the
                  <code>.jks</code>
                  files in the local files ystem (the keystore file should have the same name on all hosts for this
                  configuration).
                </p>
                <pre><code>"global": {
                  "site.accumulo-site.rpc.javax.net.ssl.keyStore": "&lt;keystore file&gt;",
                  "site.accumulo-site.rpc.javax.net.ssl.trustStore": "&lt;truststore file&gt;",
                  },
                </code></pre>
                <p>If
                  <code>clientAuth</code>
                  is enabled, you must have a
                  <code>client.conf</code>
                  file in your client Accumulo conf directory, or a
                  <code>.accumulo/config</code>
                  file in your home directory. Your
                  <code>keystore.jks</code>
                  and
                  <code>truststore.jks</code>
                  SSL certs for the client can be placed in an ssl directory in the Accumulo
                  <code>conf</code>
                  directory (or their locations can be specified by also specifying the
                  <code>rpc.javax.net.ssl.keyStore</code>
                  and
                  <code>rpc.javax.net.ssl.trustStore</code>
                  properties). If the client user is the same user that created the Accumulo cluster, it can use the
                  same credential provider as the<code>app, jceks://hdfs/user/&lt;user name&gt;/accumulo-&lt;cluster
                    name&gt;.jceks</code>, but otherwise the client user will have to create their own credential
                  provider using the hadoop credential command. The user must set the credential provider in their
                  <code>client.conf</code>
                  file, and make sure the specified credential provider contains the
                  <code>rpc.javax.net.ssl.keyStorePassword</code>
                  and<code>rpc.javax.net.ssl.trustStorePassword</code>.

                  A
                  <code>client.conf</code>
                  file for the Accumulo instance can be retrieved with the following command:
                </p>
                <pre><code>slider registry --getconf client --name &lt;cluster name&gt; --format properties --dest &lt;path
                  to local client.conf&gt;</code></pre>
              
            
            
              <span class="title"><strong>Building Accumulo Native Libraries</strong></span>
              
                <p>Accumulo performs better when it uses a native in-memory map for newly written data. To build
                  the native libraries for your system, perform the following steps to add them to your application
                  package. Then set the
                  <code>“site.accumulo-site.tserver.memory.maps.native.enabled”</code>
                  property to
                  <code>true</code>
                  in your
                  <code>appConfig.json</code>
                  file, and be sure to adjust the
                  <code>ACCUMULO_TSERVER</code>
                  heapsize parameter so that it no longer includes the
                  <code>tserver.memory.maps.max</code>
                  memory.
                </p>
                <pre><code>unzip &lt;app package name&gt;.zip package/files/accumulo*gz
                  cd package/files/
                  gunzip accumulo-&lt;version&gt;-bin.tar.gz
                  tar xvf accumulo-&lt;version&gt;-bin.tar
                  accumulo-1.6.0/bin/build_native_library.sh
                  tar uvf accumulo-&lt;version&gt;-bin.tar accumulo-&lt;version&gt;
                  rm -rf accumulo-&lt;version&gt;
                  gzip accumulo-&lt;version&gt;-bin.tar
                  cd ../../
                  zip &lt;app package name&gt;.zip -r package
                  rm -rf package
                </code></pre>
              
            
          
        
      
      
        <h3 class="horton-blue bold" id="ref-20b1218d-a8e2-47b9-a380-f78e4c6cfe70">Running Multiple MapReduce Versions Using the YARN Distributed Cache</h3>
        
          <span class="title"><strong>Introduction</strong></span>
          
            <p>Beginning in HDP 2.2, multiple versions of the MapReduce framework can be deployed using the YARN
              Distributed Cache. By setting the appropriate configuration properties, you can run jobs using a different
              version of the MapReduce framework than the one currently installed on the cluster. Distributed cache
              ensures that the MapReduce job framework version is consistent throughout the entire job lifecycle. This
              enables you to maintain consistent results from MapReduce jobs during a rolling upgrade of the cluster.
              Without using Distributed Cache, a MapReduce job might start with one framework version, but finish with
              the new (upgrade) version, which could lead to unpredictable results.

              YARN Distributed Cache enables you to efficiently distribute large read-only files ( text files, archives,
              .jar files, etc.) for use by YARN applications. Applications use URLs (hdfs://) to specify the files to be
              cached, and the Distributed Cache framework copies the necessary files to the applicable nodes before any
              tasks for the job are executed. Its efficiency stems from the fact that the files are copied only once per
              job, and archives are extracted after they are copied to the applicable nodes. Note that Distributed Cache
              assumes that the files to be cached (and specified via hdfs:// URLs) are already present on the HDFS file
              system and are accessible by every node in the cluster.
            </p>
          
        
        
          <span class="title"><strong>Configuring MapReduce for the YARN Distributed Cache</strong></span>
          
            <ul class="number-list">
              
                <li>
                  <p>Copy the tarball that contains the version of MapReduce you would like to use into an HDFS
                    directory that applications can access.
                  </p>
                  <pre><code>$HADOOP_HOME/bin/hdfs dfs -put mapreduce.tar.gz /mapred/framework/</code></pre>
                </li>
                <li>
                  <p>In the
                    <code>mapred-site.xml</code>
                    file, set the value of the
                    <code>mapreduce.application.framework.path</code>
                    property URL to point to the archive file you just uploaded. The URL allows you to create an alias
                    for the archive if a URL fragment identifier is specified. In the following example,
                    <code>mr-framework</code>
                    is specified as the alias:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;name&gt;mapreduce.application.framework.path&lt;/name&gt;
                    &lt;value&gt;hdfs:/mapred/framework/mapreduce.tar.gz#mr-framework&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
                <li>
                  <p>In the
                    <code>mapred-site.xml</code>
                    file, the default value of the
                    <code>mapreduce.application.classpath</code>
                    uses the
                    <code>${hdp.version}</code>
                    environment variable to reference the currently installed version of HDP:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;
                    &lt;value&gt;$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                  <p>Change the value of the
                    <code>mapreduce.application.classpath</code>
                    property to reference the applicable version of the MapReduce framework .jar files. In this case we
                    need to replace
                    <code>${hdp.version}</code>
                    with the applicable HDP version, which in our example is<code>2.2.0.0-2041</code>. Note
                    that in the following example the
                    <code>mr-framework</code>
                    alias is used in the path references.
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;
                    &lt;value&gt;$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/2.2.0.0-2041/hadoop/lib/hadoop-lzo-0.6.0.2.2.0.0-2041.jar&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                  <p>With this configuration in place, MapReduce jobs will run on the version 2.2.0.0-2041
                    framework referenced in the
                    <code>mapred-site.xml</code>
                    file.

                    You can upload multiple versions of the MapReduce framework to HDFS and create a separate
                    <code>mapred-site.xml</code>
                    file to reference each version of the framework. Users can then run jobs against a specific version
                    by referencing the applicable
                    <code>mapred-site.xml</code>
                    file. The following example would run a MapReduce job on version 2.1 of the MapReduce framework:
                  </p>
                  <pre><code>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi -conf
                    etc/hdp-2.1.0.0/mapred-site.xml 10 10
                  </code></pre>
                  <p>You can use the ApplicationMaster log file to confirm that the job ran on the localized
                    version of MapReduce on the Distributed Cache. For example:
                  </p>
                  <pre><code>2014-06-10 08:19:30,199 INFO [main] org.mortbay.log: Extract jar: file:/&lt;nm-local-dirs&gt;/filecache/10/hadoop-2.3.0.tar.gz/hadoop-2.3.0/share/hadoop/yarn/hadoop-yarn-common-2.3.0.jar!/webapps/mapreduce
                    to /tmp/Jetty_0_0_0_0_42544_mapreduce____.pryk9q/webapp
                  </code></pre>
                </li>
              
            </ul>
          
        
        
          <span class="title"><strong>Limitations</strong></span>
          
            <p>Support for deploying the MapReduce framework via the YARN Distributed Cache currently does not
              address the job client code used to submit and query jobs. It also does not address the ShuffleHandler
              code that runs as an auxiliary service within each NodeManager. Therefore, the following limitations apply
              to MapReduce versions that can be successfully deployed via the Distributed Cache:
            </p>
            <ul class="bullet-list">
              
                <li>
                  <p>The MapReduce version must be compatible with the job client code used to submit and query
                    jobs. If it is incompatible, the job client must be upgraded separately on any node on which jobs
                    are submitted using the new MapReduce version.
                  </p>
                </li>
                <li>
                  <p>The MapReduce version must be compatible with the configuration files used by the job client
                    submitting the jobs. If it is incompatible with that configuration (that is, a new property must be
                    set, or an existing property value must be changed), the configuration must be updated before
                    submitting jobs.
                  </p>
                </li>
                <li>
                  <p>The MapReduce version must be compatible with the ShuffleHandler version running on the
                    cluster nodes. If it is incompatible, the new ShuffleHandler code must be deployed to all nodes in
                    the cluster, and the NodeManagers must be restarted to pick up the new ShuffleHandler code.
                  </p>
                </li>
              
            </ul>
          
        
        
          <span class="title"><strong>Troubleshooting Tips</strong></span>
          
            <ul class="bullet-list">
              
                <li>
                  <p>You can use the ApplicationMaster log file to check the version of MapReduce being used by a
                    running job. For example:
                  </p>
                  <pre><code>2014-11-20 08:19:30,199 INFO [main] org.mortbay.log: Extract jar: file:/&lt;nm-local-dirs&gt;/filecache/{...}/hadoop-2.6.0.tar.gz/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar!/webapps/mapreduce
                    to /tmp/Jetty_0_0_0_0_42544_mapreduce____.pryk9q/webapp
                  </code></pre>
                </li>
                <li>
                  <p>If shuffle encryption is enabled, MapReduce jobs may fail with the following exception:
                  </p>
                  <pre><code>2014-10-10 02:17:16,600 WARN [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Failed
                    to connect to junping-du-centos6.x-3.cs1cloud.internal:13562 with 1 map outputs
                    javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building
                    failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid
                    certification path to requested target
                    at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:174)
                    at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1731)
                    at com.sun.net.ssl.internal.ssl.Handshaker.fatalSE(Handshaker.java:241)
                    at com.sun.net.ssl.internal.ssl.Handshaker.fatalSE(Handshaker.java:235)
                    at com.sun.net.ssl.internal.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1206)
                    at com.sun.net.ssl.internal.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:136)
                    at com.sun.net.ssl.internal.ssl.Handshaker.processLoop(Handshaker.java:593)
                    at com.sun.net.ssl.internal.ssl.Handshaker.process_record(Handshaker.java:529)
                    at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:925)
                    at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1170)
                    at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1197)
                    at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1181)
                    at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:434)
                    at
                    sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.setNewClient(AbstractDelegateHttpsURLConnection.java:81)
                    at
                    sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.setNewClient(AbstractDelegateHttpsURLConnection.java:61)
                    at sun.net.www.protocol.http.HttpURLConnection.writeRequests(HttpURLConnection.java:584)
                    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1193)
                    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
                    at
                    sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:318)
                    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.verifyConnection(Fetcher.java:427)
                    ....
                  </code></pre>
                  <p>To fix this problem, create a sub-directory under
                    <code>$HADOOP_CONF</code>
                    (
                    <code>$HADOOP_HOME/etc/hadoop</code>
                    by default), and copy the
                    <code>ssl-client.xml</code>
                    file to that directory. Add this new directory path (/etc/hadoop/conf/secure) to the MapReduce
                    classpath specified in
                    <code>mapreduce.application.classpath</code>
                    in the
                    <code>mapred-site.xml</code>
                    file.
                  </p>
                </li>
              
            </ul>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-b5894119-5904-4c1a-a51d-35af1a1b9f7a">Timeline Server (Technical Preview)</h3>
        
          <p>This guide describes how to configure and run the Timeline Server, which enables you to collect
            generic and per-framework information about YARN applications.
          </p>
          <aside class="custom-note">
            <div class="icon"><img src="Icons/Note.png" width="50"></div>
            <div class="simple-block">
              <p>This feature is a technical preview and considered under development. Do not use this feature in
                your production systems. If you have questions regarding this feature, contact Support by logging a case
                on our Pivotal Support Portal at<a href="https://support.hortonworks.com/">
                  https://support.hortonworks.com</a>.
              </p>
            </div>
          </aside>
        
        
          <h4 class="bold">Introduction</h4>
          
            <p>The Timeline Server maintains historical state and provides metrics visibility for YARN
              applications, similar to the functionality the Job History Server provides for MapReduce.
            </p>
            <ul class="bullet-list">
              The Timeline Server provides the following information:
              
                <li>
                  <p>Generic Information about Completed Applications

                    Generic information includes application-level data such as queue name, user information,
                    information about application attempts, a list of Containers that were run under each application
                    attempt, and information about each Container. Generic data about completed applications can be
                    accessed using the web UI or via REST APIs.
                  </p>
                </li>
              
            </ul>
            <ul class="bullet-list">
              
                <li>
                  <p>Per-Framework Information for Running and Completed Applications

                    Per-framework information is specific to an application or framework. For example, the Hadoop
                    MapReduce framework can include pieces of information such as the number of map tasks, reduce tasks,
                    counters, etc. Application developers can publish this information to the Timeline Server via the
                    TimelineClient (from within a client), the ApplicationMaster, or the application's Containers. This
                    information can then be queried via REST APIs that enable rendering by
                    application/framework-specific UIs.
                  </p>
                </li>
              
            </ul>
            <p>
              The Timeline Server is a stand-alone server daemon that is deployed to a cluster node. It may or may not
              be co-located with the ResourceManager.
            </p>
          
        
        
          <h4 class="bold">Configuring the Timeline Server</h4>
          
            <span class="title"><strong>Required Properties</strong></span>
            
              <ul class="bullet-list">
                Only one property needs to be specified in the
                  <code>etc/hadoop/conf/yarn-site.xml</code>
                  file in order to enable the Timeline Server:
                
                
                  <li>
                    <p>
                      <strong>yarn.timeline-service.hostname</strong>
                      <code>

                      </code>The host name of the Timeline Server web application.

                      Example:
                    </p>
                    <pre><code>&lt;property&gt;
                      &lt;description&gt;The hostname of the timeline server web application.&lt;/description&gt;
                      &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt;
                      &lt;value&gt;0.0.0.0&lt;/value&gt;
                      &lt;/property&gt;</code></pre>
                  </li>
                
              </ul>
            
          
          
            <span class="title"><strong>Advanced Properties</strong></span>
            
              <p>In addition to the host name, administrators can also configure the ports of the RPC and the web
                interfaces, as well as the number of RPC handler threads.
              </p>
              <ul class="bullet-list">
                
                  <li>
                    <p>
                      <strong>yarn.timeline-service.address</strong>

                      The default address for the Timeline Server to start the RPC server.

                      Example:
                    </p>
                    <pre><code>&lt;property&gt;
                      &lt;description&gt;This is default address for the timeline server to start the RPC server.&lt;/description&gt;
                      &lt;name&gt;yarn.timeline-service.address&lt;/name&gt;
                      &lt;value&gt;${yarn.timeline-service.hostname}:10200&lt;/value&gt;
                      &lt;/property&gt;</code></pre>
                  </li>
                  <li>
                    <p>
                      <strong>yarn.timeline-service.webapp.address</strong>

                      The HTTP address of the Timeline Server web application.

                      Example:
                    </p>
                    <pre><code>&lt;property&gt;
                      &lt;description&gt;The http address of the timeline server web application.&lt;/description&gt;
                      &lt;name&gt;yarn.timeline-service.webapp.address&lt;/name&gt;
                      &lt;value&gt;${yarn.timeline-service.hostname}:8188&lt;/value&gt;
                      &lt;/property&gt;</code></pre>
                  </li>
                  <li>
                    <p>
                      <strong>yarn.timeline-service.webapp.https.address</strong>

                      The HTTPS address of the Timeline Server web application.

                      Example:
                    </p>
                    <pre><code>&lt;property&gt;
                      &lt;description&gt;The https adddress of the timeline server web application.&lt;/description&gt;
                      &lt;name&gt;yarn.timeline-service.webapp.https.address&lt;/name&gt;
                      &lt;value&gt;${yarn.timeline-service.hostname}:8190&lt;/value&gt;
                      &lt;/property&gt;</code></pre>
                  </li>
                  <li>
                    <p>
                      <strong>yarn.timeline-service.handler-thread-count</strong>

                      The handler thread count to serve the client RPC requests.

                      Example:
                    </p>
                    <pre><code>&lt;property&gt;
                      &lt;description&gt;Handler thread count to serve the client RPC requests.&lt;/description&gt;
                      &lt;name&gt;yarn.timeline-service.handler-thread-count&lt;/name&gt;
                      &lt;value&gt;10&lt;/value&gt;
                      &lt;/property&gt;</code></pre>
                  </li>
                
              </ul>
            
          
        
        
          <h4 class="bold">Enabling Generic Data Collection</h4>
          
            <ul class="bullet-list">
              
                <li>
                  <p>
                    <strong>yarn.resourcemanager.system-metrics-publisher.enabled</strong>

                    This property indicates to the ResourceManager, as well as to clients, whether or not the Generic
                    History Service (GHS) is enabled. If the GHS is enabled, the ResourceManager begins recording
                    historical data that the GHS can consume, and clients can redirect to the GHS when applications
                    finish running.

                    Example:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;description&gt;Enable or disable the GHS&lt;/description&gt;
                    &lt;name&gt;yarn.resourcemanager.system-metrics-publisher.enabled&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Configuring Per-Framework Data Collection</h4>
          
            <ul class="bullet-list">
              
                <li>
                  <p>
                    <strong>yarn.timeline-service.enabled</strong>

                    Indicates to clients whether or not the Timeline Server is enabled. If it is enabled, the
                    TimelineClient library used by end-users will post entities and events to the Timeline Server.

                    Example:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;description&gt;Enable or disable the Timeline Server.&lt;/description&gt;
                    &lt;name&gt;yarn.timeline-service.enabled&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Configuring the Timeline Server Store</h4>
          
            <ul class="bullet-list">
              
                <li>
                  <p>
                    <strong>yarn.timeline-service.store-class</strong>

                    The class name for the Timeline store.

                    Example:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;description&gt;Store class name for timeline store&lt;/description&gt;
                    &lt;name&gt;yarn.timeline-service.store-class&lt;/name&gt;
                    &lt;value&gt;org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
                <li>
                  <p>
                    <strong>yarn.timeline-service.leveldb-timeline-store.path</strong>

                    The store file path and name for the Timeline Server LevelDB store (if the LevelDB store is used).

                    Example:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;description&gt;Store file name for leveldb timeline store&lt;/description&gt;
                    &lt;name&gt;yarn.timeline-service.leveldb-timeline-store.path&lt;/name&gt;
                    &lt;value&gt;${yarn.log.dir}/timeline&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
                <li>
                  <p>
                    <strong>yarn.timeline-service.ttl-enable</strong>

                    Enable age-off of timeline store data.

                    Example:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;description&gt;Enable age off of timeline store data.&lt;/description&gt;
                    &lt;name&gt;yarn.timeline-service.ttl-enable&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
                <li>
                  <p>
                    <strong>yarn.timeline-service.ttl-ms</strong>

                    The Time-to-live for timeline store data (in milliseconds).

                    Example:
                  </p>
                  <pre><code>&lt;property&gt;
                    &lt;description&gt;Time to live for timeline store data in milliseconds.&lt;/description&gt;
                    &lt;name&gt;yarn.timeline-service.ttl-ms&lt;/name&gt;
                    &lt;value&gt;604800000&lt;/value&gt;
                    &lt;/property&gt;</code></pre>
                </li>
              
            </ul>
          
        
        
          <h4 class="bold">Configuring Timeline Server Security</h4>
          
            <p>
              <strong>Configuring Kerberos Authentication</strong>

              To configure Kerberos Authentication for the Timeline Server, add the following properties to the
              <code>yarn-site.xml</code>
              file.
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;
              &lt;value&gt;kerberos&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;yarn.timeline-service.http-authentication.kerberos.principal&lt;/name&gt;
              &lt;value&gt;HTTP/localhost@EXAMPLE.COM&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;yarn.timeline-service.http-authentication.kerberos.keytab&lt;/name&gt;
              &lt;value&gt;/etc/krb5.keytab&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>
              <strong>Configuring Timeline Server Authorization (ACLs)</strong>

              Timeline Server ACLs are configured in the same way as other YARN ACLs. To configure Timeline Server
              authorization with ACLs, add the following properties to the
              <code>yarn-site.xml</code>
              file.
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;yarn.acl.enable&lt;/name&gt;
              &lt;value&gt;true&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;yarn.admin.acl&lt;/name&gt;
              &lt;value&gt; &lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>
              <strong>Configuring Timeline Server SSL</strong>

              Timeline Server SSL is configured in the same way as other Hadoop components. To configure Timeline Server
              SSL, add the following properties to the
              <code>core-site.xml</code>
              file.
            </p>
            <pre><code>&lt;property&gt;
              &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;
              &lt;value&gt;false&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;hadoop.ssl.hostname.verifier&lt;/name&gt;
              &lt;value&gt;DEFAULT&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;
              &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;
              &lt;value&gt;ssl-server.xml&lt;/value&gt;
              &lt;/property&gt;

              &lt;property&gt;
              &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;
              &lt;value&gt;ssl-client.xml&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <aside class="custom-note">
              <div class="icon"><img src="Icons/Note.png" width="50"></div>
              <div class="simple-block">
                <p>You should also configure the YARN SSL settings described in "Enabling SSL for WebHDFS,
                  MapReduce Shuffle, and YARN" in the<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/HDP_Security_Guide_v22/index.html">
                    HDP Security Guide</a>.
                </p>
              </div>
            </aside>
          
        
        
          <h4 class="bold">Running the Timeline Server</h4>
          
            <p>To start the Timeline Server, run the following command:</p>
            <pre><code>yarn timelineserver</code></pre>
            <p>To start the Timeline Server as a daemon, run the following command:</p>
            <pre><code>sbin/yarn-daemon.sh start timelineserver</code></pre>
          
        
        
          <h4 class="bold">Accessing Generic Data from the Command- Line</h4>
          
            <p>You can use the following commands to access application generic history data from the
              command-line. Note that these same commands can be used to obtain corresponding information about running
              applications.
            </p>
            <pre><code>yarn application -status &lt;Application ID&gt;
              yarn applicationattempt -list &lt;Application ID&gt;
              yarn applicationattempt -status &lt;Application Attempt ID&gt;
              yarn container -list &lt;Application Attempt ID&gt;
              yarn container -status &lt;Container ID&gt;</code></pre>
          
        
        
          <h4 class="bold">Publishing Per-Framework Data in Applications</h4>
          
            <p>Developers can define the information they would like to record for their applications by
              composing
              <code>TimelineEntity</code>
              and
              <code>TimelineEvent</code>
              objects, and then putting the entities and events to the Timeline server via<code>
                TimelineClient</code>. For example:
            </p>
            <pre><code>// Create and start the Timeline client
              TimelineClient client = TimelineClient.createTimelineClient();
              client.init(conf);
              client.start();

              TimelineEntity entity = null;
              // Compose the entity
              try {
              TimelinePutResponse response = client.putEntities(entity);
              } catch (IOException e) {
              // Handle the exception
              } catch (YarnException e) {
              // Handle the exception
              }

              // Stop the Timeline client
              client.stop();
            </code></pre>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-93e24f69-7a50-4fb4-a7fb-0960d1c8c91b">Using the YARN REST APIs to Mangage Applications</h3>
        
          <p>This guide describes how to use the YARN REST APIs to submit, monitor, and kill applications.
          </p>
        
        
          <h4 class="bold">Get an Application ID</h4>
          
            <p>You can use the New Application API to get an application ID, which can then be used to submit an
              application. For example:
            </p>
            <pre><code>curl -v -X POST 'http://localhost:8088/ws/v1/cluster/apps/new-application'</code></pre>
            <p>The response returns the application ID, and also includes the maximum resource capabilities
              available on the cluster. For example:
            </p>
            <pre><code>{
              application-id: application_1409421698529_0012",
              "maximum-resource-capability":{"memory":"8192","vCores":"32"}
              }
            </code></pre>
          
        
        
          <h4 class="bold">Set Up an Application .json File</h4>
          
            <p>Before you submitting an application, you must set up a
              <code>.json</code>
              file with the parameters required by the application. This is analogous to creating your own
              ApplicationMaster. The application
              <code>.json</code>
              file contains all of the fields you are required to submit in order to launch the application.

              The following is an example of an application
              <code>.json</code>
              file:
            </p>
            <pre><code>{
              "application-id":"application_1404203615263_0001",
              "application-name":"test",
              "am-container-spec":
              {
              "local-resources":
              {
              "entry":
              [
              {
              "key":"AppMaster.jar",
              "value":
              {
              "resource":"hdfs://hdfs-namenode:9000/user/testuser/DistributedShell/demo-app/AppMaster.jar",
              "type":"FILE",
              "visibility":"APPLICATION",
              "size": "43004",
              "timestamp": "1405452071209"
              }
              }
              ]
              },
              "commands":
              {
              "command":"{{JAVA_HOME}}/bin/java -Xmx10m
              org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster --container_memory 10
              --container_vcores 1 --num_containers 1 --priority 0 1&gt;&lt;LOG_DIR&gt;/AppMaster.stdout 2&gt;&lt;LOG_DIR&gt;/AppMaster.stderr"
              },
              "environment":
              {
              "entry":
              [
              {
              "key": "DISTRIBUTEDSHELLSCRIPTTIMESTAMP",
              "value": "1405459400754"
              },
              {
              "key": "CLASSPATH",
              "value": "{{CLASSPATH}}&lt;CPS&gt;./*&lt;CPS&gt;{{HADOOP_CONF_DIR}}&lt;CPS&gt;{{HADOOP_COMMON_HOME}}/share/hadoop/common/*&lt;CPS&gt;{{HADOOP_COMMON_HOME}}/share/hadoop/common/lib/*&lt;CPS&gt;{{HADOOP_HDFS_HOME}}/share/hadoop/hdfs/*&lt;CPS&gt;{{HADOOP_HDFS_HOME}}/share/hadoop/hdfs/lib/*&lt;CPS&gt;{{HADOOP_YARN_HOME}}/share/hadoop/yarn/*&lt;CPS&gt;{{HADOOP_YARN_HOME}}/share/hadoop/yarn/lib/*&lt;CPS&gt;./log4j.properties"
              },
              {
              "key": "DISTRIBUTEDSHELLSCRIPTLEN",
              "value": "6"
              },
              {
              "key": "DISTRIBUTEDSHELLSCRIPTLOCATION",
              "value": "hdfs://hdfs-namenode:9000/user/testuser/demo-app/shellCommands"
              }
              ]
              }
              },
              "unmanaged-AM":"false",
              "max-app-attempts":"2",
              "resource":
              {
              "memory":"1024",
              "vCores":"1"
              },
              "application-type":"YARN",
              "keep-containers-across-application-attempts":"false"
              }
            </code></pre>
          
        
        
          <h4 class="bold">Submit an Application</h4>
          
            <p>You can use the Submit Application API to submit applications. For example:</p>
            <pre><code>curl -v -X POST -d @example-submit-app.json -H "Content-type: application/json"
              'http://localhost:8088/ws/v1/cluster/apps'
            </code></pre>
            <p>After you submit an application the response includes the following field:</p>
            <pre><code>HTTP/1.1 202 Accepted</code></pre>
            <p>The response also includes the Location field, which you can use to get the status of the
              application (app ID). The following is an example of a returned Location code:
            </p>
            <pre><code>Location: http://localhost:8088/ws/v1/cluster/apps/application_1409421698529_0012</code></pre>
          
        
        
          <h4 class="bold">Monitor an Application</h4>
          
            <p>You can use the Application State API to query the application state. To return only the state of
              a running application, use the following command format:
            </p>
            <pre><code>curl 'http://localhost:8088/ws/v1/cluster/apps/application_1409421698529_0012/state'</code></pre>
            <p>You can also use the value of the Location field (returned in the application submission response)
              to check the application status. For example:
            </p>
            <pre><code>curl -v 'http://localhost:8088/ws/v1/cluster/apps/application_1409421698529_0012'</code></pre>
            <p>You can use the following command format to check the logs:</p>
            <pre><code>yarn logs -appOwner 'dr.who' -applicationId application_1409421698529_0012 | less</code></pre>
          
        
        
          <h4 class="bold">Kill an Application</h4>
          
            <p>You can also use the Application State API to kill an application by using a
              <code>PUT</code>
              operation to set the application state to<code>KILLED</code>. For example:
            </p>
            <pre><code>curl -v -X PUT -d '{"state": "KILLED"}'
              'http://localhost:8088/ws/v1/cluster/apps/application_1409421698529_0012'
            </code></pre>
          
        
        
          <h4 class="bold">Access the Apache YARN REST API Specification</h4>
          
            <p>For more information, see the Apache
              <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Writeable_APIs">
                YARN REST APIs
              </a>
              documentation.
            </p>
          
        
      
      
        <h3 class="horton-blue bold" id="ref-5cd339dc-a21a-4dab-b978-86e0c31e700d">Work-Preserving Restart</h3>
        
          <p>This guide describes how to configure YARN to preserve the work of running applications in the event
            of a ResourceManager or NodeManager restart. Work-preserving ResourceManager and NodeManager restart ensures
            that node restart or fail-over is completely transparent to end-users, with minimal impact to running
            applications.
          </p>
        
        
          <h4 class="bold">Configuring the ResourceManager for Work-Preserving Restart</h4>
          
            <p>Work-preserving ResourceManager restart ensures that applications continuously function during a
              ResourceManager restart with minimal impact to end-users. The overall concept is that the ResourceManager
              preserves application queue state in a pluggable state store, and reloads that state on restart. While the
              ResourceManager is down, ApplicationMasters and NodeManagers continuously poll the ResourceManager until
              it restarts. When the ResourceManager comes back online, the ApplicationMasters and NodeManagers
              re-register with the newly started ResourceManger. When the ResourceManager restarts, it also recovers
              container information by absorbing the container statuses sent from all NodeManagers. Thus, no work will
              be lost due to a ResourceManager crash-reboot event

              To configure work-preserving restart for the ResourceManager, set the following properties in the
              <code>yarn-site.xml</code>
              file.

              <strong>Property:</strong>
              <code>yarn.resourcemanager.recovery.enabled</code>
              <strong>Value:</strong>
              <code>true
              </code>
              <strong>Description:</strong>
              Enables ResourceManager restart. The default value is<code>false</code>. If this configuration
              property is set to<code>true</code>, running applications will resume when the ResourceManager
              is restarted.

              <strong>Example:</strong>
            </p>
            <pre><code>  &lt;property&gt;
              &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;
              &lt;value&gt;true&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>
              <strong>Property:</strong>
              <code>yarn.resourcemanager.store.class</code>
              <strong>Value:</strong>
              <code>&lt;specified_state_store&gt;</code>

              <strong>Description:</strong>
              Specifies the state-store used to store application and application-attempt state and other credential
              information to enable restart. The available state-store implementations are:

              <code>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</code>
              – a state-store implementation persisting state to a file system such as HDFS. This is the default value.
              <code>org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore</code>
              - a LevelDB-based state-store implementation.
              <code>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</code>
              – a ZooKeeper-based state-store implementation.

              <strong>Example:</strong>
            </p>
            <pre><code>  &lt;property&gt;
              &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;
              &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore&lt;/value&gt;
              &lt;/property&gt;</code></pre>
          
          
            <span class="title"><strong>FileSystemRMStateStore Configuration</strong></span>
            
              <p>The following properties apply only if
                <code>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</code>
                has been specified as the state-store in the
                <code>yarn.resourcemanager.store.class</code>
                property.
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.fs.state-store.uri</code>
                <strong>Value:</strong>
                <code>&lt;hadoop.tmp.dir&gt;/yarn/system/rmstore</code>

                <strong>Description:</strong>
                The URI pointing to the location of the file system path where the RM state will be stored (e.g.
                <code>hdfs://localhost:9000/rmstore</code>). The default value is<code>&lt;hadoop.tmp.dir&gt;/yarn/system/rmstore</code>
                .

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.fs.state-store.uri&lt;/name&gt;
                &lt;value&gt;hdfs://localhost:9000/rmstore&lt;/value&gt;
                &lt;/property
              </code></pre>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.fs.state-store.retry-policy-spec</code>
                <strong>Value:</strong>
                <code>2000, 500</code>

                <strong>Description:</strong>
                The Hadoop FileSystem client retry policy specification. Hadoop FileSystem client retry is always
                enabled. This is pecified in pairs of sleep-time and number-of-retries i.e. (t0, n0), (t1, n1), ..., the
                first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on
                average, and so on. The default value is (<code>2000, 500</code>).

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.fs.state-store.retry-policy-spec&lt;/name&gt;
                &lt;value&gt;2000, 500&lt;/value&gt;
                &lt;/property
              </code></pre>
            
          
          
            <span class="title"><strong>LeveldbRMStateStore Configuration</strong></span>
            
              <p>The following properties apply only if
                <code>org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore</code>
                has been specified as the state-store in the
                <code>yarn.resourcemanager.store.class</code>
                property.
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.leveldb-state-store.path</code>
                <strong>Value:</strong>
                <code>&lt;hadoop.tmp.dir&gt;/yarn/system/rmstore</code>

                <strong>Description:</strong>
                The local path where the RM state will be stored.

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.leveldb-state-store.path&lt;/name&gt;
                &lt;value&gt;&lt;hadoop.tmp.dir&gt;/yarn/system/rmstore&lt;/value&gt;
                &lt;/property
              </code></pre>
            
          
          
            <span class="title"><strong>ZKRMStateStore Configuration</strong></span>
            
              <p>The following properties apply only if
                <code>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</code>
                has been specified as the state-store in the
                <code>yarn.resourcemanager.store.class</code>
                property.
              </p>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.zk-address</code>
                <strong>Value:</strong>
                <code>&lt;host&gt;:&lt;port&gt;</code>

                <strong>Description:</strong>
                A comma-separated list of
                <code>&lt;host&gt;:&lt;port&gt;</code>
                pairs, each corresponding to a server in a ZooKeeper cluster where the ResourceManager state will be
                stored.

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
                &lt;value&gt;127.0.0.1:2181&lt;/value&gt;
                &lt;/property
              </code></pre>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.zk-state-store.parent-path</code>
                <strong>Value:</strong>
                <code>/rmstore</code>

                <strong>Description:</strong>
                The full path of the root znode where RM state will be stored. The default value is<code>
                /rmstore</code>.

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.zk-state-store.parent-path&lt;/name&gt;
                &lt;value&gt;/rmstore&lt;/value&gt;
                &lt;/property
              </code></pre>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.zk-num-retries</code>
                <strong>Value:</strong>
                <code>500</code>

                <strong>Description:</strong>
                The number of times the ZooKeeper-client running inside the ZKRMStateStore tries to connect to ZooKeeper
                in case of connection timeouts. The default value is<code>500</code>.

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.zk-num-retries&lt;/name&gt;
                &lt;value&gt;500&lt;/value&gt;
                &lt;/property
              </code></pre>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.zk-retry-interval-ms</code>
                <strong>Value:</strong>
                <code>2000</code>

                <strong>Description:</strong>
                The interval in milliseconds between retries when connecting to a ZooKeeper server. The default value is
                2 seconds.

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.zk-retry-interval-ms&lt;/name&gt;
                &lt;value&gt;2000&lt;/value&gt;
                &lt;/property
              </code></pre>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.zk-timeout-ms</code>
                <strong>Value:</strong>
                <code>10000</code>

                <strong>Description:</strong>
                The ZooKeeper session timeout in milliseconds. This configuration is used by the ZooKeeper server to
                determine when the session expires. Session expiration happens when the server does not hear from the
                client (i.e. no heartbeat) within the session timeout period specified by this property. The default
                value is 10 seconds.

                <strong>Example:</strong>
              </p>
              <pre><code>  &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.zk-timeout-ms&lt;/name&gt;
                &lt;value&gt;10000&lt;/value&gt;
                &lt;/property
              </code></pre>
              <p>
                <strong>Property:</strong>
                <code>yarn.resourcemanager.zk-acl</code>
                <strong>Value:</strong>
                <code>world:anyone:rwcda</code>

                <strong>Description:</strong>
                The ACLs to be used for setting permissions on ZooKeeper znodes. The default value is
                <code>world:anyone:rwcda.

                </code>
                <strong>Example</strong>
              </p>
              <pre><code>&lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.zk-acl&lt;/name&gt;
                &lt;value&gt;world:anyone:rwcda&lt;/value&gt;
                &lt;/property&gt;</code></pre>
            
          
        
        
          <h4 class="bold">Configuring NodeManagers for Work-Preserving Restart</h4>
          
            <p>NodeManager work-preserving enables a NodeManager to be restarted without losing the active
              containers running on the node. At a high level, the NodeManager stores any necessary state to a local
              state store as it processes container management requests. When the NodeManager restarts, it recovers by
              first loading the state for various subsystems, and then lets those subsystems perform recovery using the
              loaded state.

              To configure work-preserving restart for NodeManagers, set the following properties in the
              <code>yarn-site.xml</code>
              file on all NodeManagers in the cluster.

              <strong>Property:</strong>
              <code>yarn.nodemanager.recovery.enabled</code>
              <strong>Value:</strong>
              <code>true</code>

              <strong>Description:</strong>
              Enables the NodeManager to recover after a restart.

              <strong>Example:</strong>
            </p>
            <pre><code>  &lt;property
              &lt;name&gt;yarn.nodemanager.recovery.enabled&lt;/name&gt;
              &lt;value&gt;true&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>
              <strong>Property:</strong>
              <code>yarn.nodemanager.recovery.dir</code>
              <strong>Value:</strong>
              <code>&lt;yarn_log_dir_prefix&gt;/nodemanager/recovery-state</code>

              <strong>Description:</strong>
              The local file system directory in which the NodeManager will store state information when recovery is
              enabled.

              <strong>Example:</strong>
            </p>
            <pre><code>  &lt;property&gt;
              &lt;name&gt;yarn.nodemanager.recovery.dir&lt;/name&gt;
              &lt;value&gt;&lt;yarn_log_dir_prefix&gt;/nodemanager/recovery-state&lt;/value&gt;
              &lt;/property&gt;</code></pre>
            <p>
              You should also confirm that the
              <code>yarn.nodemanager.address</code>
              port is set to a non-zero value, e.g.<code>45454</code>:
            </p>
            <pre><code>  &lt;property&gt;
              &lt;name&gt;yarn.nodemanager.address&lt;/name&gt;
              &lt;value&gt;0.0.0.0:45454&lt;/value&gt;
              &lt;/property&gt;</code></pre>
          
        
      
    
  </div></main>
</div>
<div class="container"><div class="footnotes">
<h2 class="horton-blue border-bottom">Footnotes</h2>
<ol></ol>
</div></div>
<div class="container"><footer><h3 class="horton-blue border-bottom">About Pivotal Data Platform</h3>
      Copyright
      
        <p>This work by
          <a href="http://hortonworks.com">Pivotal, Inc.</a>
          is licensed under a<a href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
            Attribution-ShareAlike 3.0 Unported License</a>.
        </p>
      
      
    <div class="copyright"><p>
    
      <p>The Pivotal Data Platform, powered by Apache Hadoop, is a massively scalable and 100% open source
        platform for storing, processing and analyzing large volumes of data. It is designed to deal with data from many
        sources and formats in a very quick, easy and cost-effective manner. The Pivotal Data Platform consists of
        the essential set of Apache Hadoop projects including MapReduce, Hadoop Distributed File System (HDFS),
        HCatalog, Pig, Hive, HBase, Zookeeper and Ambari. Pivotal is the major contributor of code and patches to
        many of these projects. These projects have been integrated and tested as part of the Pivotal Data Platform
        release process and installation and configuration tools have also been included.
      </p>
      <p>
        Unlike other providers of platforms built using Apache Hadoop, Pivotal contributes 100% of our code back to
        the Apache Software Foundation. The Pivotal
        Data Platform is Apache-licensed and completely open source. We sell only expert technical support,
        <a href="http://hortonworks.com/hadoop-training/">training</a>
        and partner enablement services.
        <strong>All of our technology is, and will remain, free and open source.</strong>
      </p>
      <p>
        For more information on Pivotal technology, Please visit the
        <a href="http://hortonworks.com/products/hdp/">Pivotal Data Platform</a>
        page. For more information on Pivotal services, please visit either the
        <a href="http://hortonworks.com/hadoop-support/">Support</a>
        or
        <a href="http://hortonworks.com/hadoop-training">Training</a>
        page. Feel free to
        <a href="http://hortonworks.com/about-us/contact-us/">Contact Us</a>
        directly to discuss your specific needs.
      </p>
    
    <span>© Copyright © 2012, 2014 Pivotal, Inc. Some rights reserved.</span>
  </p></div></footer></div>
</div></div></body>
</html>
